{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Alloceee","url":"https://alloceee.github.io","root":"/"},"pages":[{"title":"","date":"2021-07-21T07:09:57.761Z","updated":"2021-07-21T07:09:52.056Z","comments":true,"path":"contact.html","permalink":"https://alloceee.github.io/contact.html","excerpt":"","text":""},{"title":"","date":"2021-07-28T03:36:59.438Z","updated":"2021-07-28T03:36:59.438Z","comments":true,"path":"404.html","permalink":"https://alloceee.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2021-07-28T03:32:03.331Z","updated":"2021-07-28T03:32:03.331Z","comments":true,"path":"about/index.html","permalink":"https://alloceee.github.io/about/index.html","excerpt":"","text":""},{"title":"首页","date":"2021-07-21T07:07:37.111Z","updated":"2021-07-21T07:07:37.111Z","comments":true,"path":"archives/index.html","permalink":"https://alloceee.github.io/archives/index.html","excerpt":"","text":""},{"title":"my-friends","date":"2021-07-28T03:35:55.301Z","updated":"2021-07-28T03:35:55.301Z","comments":true,"path":"friends/index.html","permalink":"https://alloceee.github.io/friends/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2021-07-28T03:33:13.532Z","updated":"2021-07-28T03:33:13.532Z","comments":true,"path":"categories/index.html","permalink":"https://alloceee.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2021-07-28T03:34:26.738Z","updated":"2021-07-28T03:34:26.738Z","comments":true,"path":"mylist/index.html","permalink":"https://alloceee.github.io/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-07-28T03:33:40.751Z","updated":"2021-07-28T03:33:40.751Z","comments":true,"path":"tags/index.html","permalink":"https://alloceee.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"搜索引擎/【300期】Elasticsearch 是如何做到快速检索的","date":"2021-08-03T09:49:49.140Z","updated":"2021-07-14T03:46:13.049Z","comments":true,"path":"2021/08/03/搜索引擎/【300期】Elasticsearch 是如何做到快速检索的/","link":"","permalink":"https://alloceee.github.io/2021/08/03/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/%E3%80%90300%E6%9C%9F%E3%80%91Elasticsearch%20%E6%98%AF%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E5%BF%AB%E9%80%9F%E6%A3%80%E7%B4%A2%E7%9A%84/","excerpt":"","text":"【300期】Elasticsearch 是如何做到快速检索的一、前言本文大致包括一下内容： 关于搜索 传统关系型数据库和ES的差别 搜索引擎原理 细究倒排索引 倒排索引具体是个什么样子的（posting list -&gt;term dic-&gt;term index） 关于postings list的一些巧技（FOR、Roaring Bitmaps） 如何快速做联合查询 二、关于搜索先设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗， 用 传统关系型数据库和 ES 实现会有什么差别？ 如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询 1select name from poems where content like &quot;%前%&quot;; 这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。 不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD”这样的关键词时，通常还希望看到”A”,”AB”,”CD”,“ABC”的搜索结果。 于是乎就有了专业的搜索引擎，比如我们今天的主角 – ES。 搜索引擎原理搜索引擎的搜索原理简单概括的话可以分为这么几步， 内容爬取，停顿词过滤，比如一些无用的像“的”，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 这里我们就引出了一个概念，也就是我们今天要剖析的重点 - 倒排索引。也是ES的核心知识点。 ES可以说是对Lucene的一个封装，里面关于倒排索引的时间就是通过lucene这个jar包提供的API实现的，所以下面讲的关于倒排索引的内容实际上都是lucene里面的内容。 三、倒排索引先看下建立倒排索引之后，我们上述的查询需求会变成什么样子 这样我们一输入“前”，借助倒排索引就可以直接定位到符合查询条件的古诗。 当然这只是一个很大白话的形式来描述倒排索引的简要工作原理。在 ES 中，这个倒排索引是具体是个什么样的，怎么存储的等等，这些才是倒排索引的精华内容。推荐：250期面试题 1. 几个概念在进入下文之前，先描述几个前置概念。 term关键词这个东西是我自己的讲法，在 ES 中，关键词被称为 term。 postings list还是用上面的例子，&#123;静夜思, 望庐山瀑布&#125;是 “前” 这个 term 所对应列表。在 ES 中，这些被描述为所有包含特定 term 文档的 id 的集合。由于整型数字 integer 可以被高效压缩的特质，integer 是最适合放在 postings list 作为文档的唯一标识的，ES 会对这些存入的文档进行处理，转化成一个唯一的整型 id。 再说下这个 id 的范围，在存储数据的时候，在每一个 shard 里444444面，ES 会将数据存入不同的 segment，这是一个比 shard 更小的分片单位，这些 segment 会定期合并。在每一个 segment 里面都会保存最多 2^31 个文档，每个文档被分配一个唯一的 id，从0到(2^31)-1。 相关的名词都是 ES 官方文档给的描述，后面参考材料中都可以找到出处。 2. 索引内部结构上面所描述的倒排索引，仅仅是一个很粗糙的模型。真的要在实际生产中使用，当然还差的很远。 在实际生产场景中，比如 ES 最常用的日志分析，日志内容进行分词之后，可以得到多少的 term？ 那么如何快速的在海量 term 中查询到对应的 term 呢？遍历一遍显然是不现实的。 term dictionary于是乎就有了 term dictionary，ES 为了能快速查找到 term，将所有的 term 排了一个序，二分法查找。是不是感觉有点眼熟，这不就是 MySQL 的索引方式的，直接用 B+树建立索引词典指向被索引的数据。 term index但是问题又来了，你觉得 Term Dictionary 应该放在哪里？肯定是放在内存里面吧？磁盘 io 那么慢。就像 MySQL 索引就是存在内存里面了。 但是如果把整个 term dictionary 放在内存里面会有什么后果呢？ 内存爆了… 别忘了，ES 默认可是会对全部 text 字段进行索引，必然会消耗巨大的内存，为此 ES 针对索引进行了深度的优化。在保证执行效率的同时，尽量缩减内存空间的占用。 于是乎就有了 term index。 Term index从数据结构上分类算是一个“Trie树”，也就是我们常说的字典树。这是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。 这棵树不会包含所有的term，它包含的是term的一些前缀（这也是字典树的使用场景，公共前缀）。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。就像右边这个图所表示的。（类似英文字典，首先定位S开头的一个单词，或者定位到Sh开头的一个单词，然后再往后顺序查询）。 lucene在这里还做了两点优化，一是term dictionary在磁盘上面是分block保存的，一个block内部利用公共前缀压缩，比如都是Ab开头的单词就可以把Ab省去。二是term index在内存中是以FST（finite state transducers）的数据结构保存的。 FST有两个优点： 空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间 查询速度快。O(len(str))的查询时间复杂度 FST 的理论比较复杂，本文不细讲 延伸阅读：https://www.shenyanchao.cn/blog/2018/12/04/lucene-fst/ OK，现在我们能得到 lucene 倒排索引大致是个什么样子的了。 四、关于postings list的一些技巧在实际使用中，postings list还需要解决几个痛点， postings list如果不进行压缩，会非常占用磁盘空间 联合查询下，如何快速求交并集（intersections and unions） 对于如何压缩，可能会有人觉得没有必要，”posting list 不是已经只存储文档 id 了吗？还需要压缩？”，但是如果在 posting list 有百万个 doc id 的情况，压缩就显得很有必要了。（比如按照朝代查询古诗？），至于为啥需要求交并集，ES 是专门用来搜索的，肯定会有很多联合查询的需求吧 （AND、OR）。 按照上面的思路，我们先将如何压缩。 1. 压缩Frame of Reference在 lucene 中，要求 postings lists 都要是有序的整形数组。这样就带来了一个很好的好处，可以通过 增量编码（delta-encode）这种方式进行压缩。 比如现在有 id 列表 [73, 300, 302, 332, 343, 372]，转化成每一个 id 相对于前一个 id 的增量值（第一个 id 的前一个 id 默认是 0，增量就是它自己）列表是[73, 227, 2, 30, 11, 29]。在这个新的列表里面，所有的 id 都是小于 255 的，所以每个 id 只需要一个字节存储。 实际上 ES 会做的更加精细， 它会把所有的文档分成很多个 block，每个 block 正好包含 256 个文档，然后单独对每个文档进行增量编码，计算出存储这个 block 里面所有文档最多需要多少位来保存每个 id，并且把这个位数作为头信息（header）放在每个 block 的前面。这个技术叫 Frame of Reference。 上图也是来自于 ES 官方博客中的一个示例（假设每个 block 只有 3 个文件而不是 256）。 FOR 的步骤可以总结为： 进过最后的位压缩之后，整型数组的类型从固定大小 (8,16,32,64 位)4 种类型,扩展到了[1-64] 位共 64 种类型。 通过以上的方式可以极大的节省 posting list 的空间消耗，提高查询性能。不过 ES 为了提高 filter 过滤器查询的性能，还做了更多的工作，那就是缓存。 Roaring Bitmaps（for filter cache）在ES中，可以用filters来优化查询，filter查询只处理文档是否匹配与否，不涉及文档评分操作，查询的结果可以被缓存。 对于filter查询，es提供了filter cache这种特殊的缓存，filter cache用来存储filters得到的结果集。缓存filters不需要太多的内存，它只保留一种信息，即哪些文档与filter相匹配。同时它可以由其他的查询服用，极大地提升了查询的性能。 我们上面提到的 Frame Of Reference 压缩算法对于 postings list 来说效果很好，但对于需要存储在内存中的 filter cache 等不太合适。 filter cache 会存储那些经常使用的数据，针对 filter 的缓存就是为了加速处理效率，对压缩算法要求更高。 对于这类postings list，ES采用不一样的压缩方式。 首先我们知道 postings list 是 Integer 数组，具有压缩空间。 假设有这么一个数组，我们第一个压缩的思路是什么？用位的方式来表示，每个文档对应其中的一位，也就是我们常说的位图，bitmap。 它经常被作为索引用在数据库、查询索引和搜索引擎中，并且位操作（如and求交集、or求并集）之间可以并行，效率更好。 但是，位图有个很明显的缺点，不管业务中实际的元素基数有多少，它占用的内存空间都恒定不变。也就是说不适用于稀疏存储。业内对于稀疏位图也有很多成熟的压缩方案，lucene 采用的就是roaring bitmaps。推荐：250期面试题 我这里用简单的方式描述一下这个压缩过程是怎么样， 将 doc id 拆成高 16 位，低 16 位。对高位进行聚合 (以高位做 key，value 为有相同高位的所有低位数组)，根据低位的数据量 (不同高位聚合出的低位数组长度不相同)，使用不同的 container(数据结构) 存储。 len&lt;4096 ArrayContainer 直接存值 len&gt;=4096 BitmapContainer 使用 bitmap 存储 分界线的来源：value 的最大总数是为2^16=65536. 假设以 bitmap 方式存储需要 65536bit=8kb,而直接存值的方式，一个值 2 byte，4K 个总共需要2byte*4K=8kb。所以当 value 总量 &lt;4k 时,使用直接存值的方式更节省空间。 空间压缩主要体现在: 高位聚合（假设数据中有100w个高位相同的值，原先需要100w_2byte，现在只要1_2byte） 低位压缩 缺点就是于位操作的速度相对于原生的bitmap会有影响。 这就是trade-off呀，平衡的艺术。 2.联合查询讲完了压缩，我们再来讲讲联合查询。 先讲简单的，如果查询有 filter cache，那就是直接拿 filter cache 来做计算，也就是说位图来做 AND 或者 OR 的计算。 如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历磁盘上的 postings list。 以上是三个 posting list。我们现在需要把它们用 AND 的关系合并，得出 posting list 的交集。首先选择最短的 posting list，逐个在另外两个 posting list 中查找看是否存在，最后得到交集的结果。遍历的过程可以跳过一些元素，比如我们遍历到绿色的 13 的时候，就可以跳过蓝色的 3 了，因为 3 比 13 要小。 用skip list还会带来一个好处，还记得前面说的吗，postings list 在磁盘里面是采用 FOR 的编码方式存储的 会把所有的文档分成很多个 block，每个 block 正好包含 256 个文档，然后单独对每个文档进行增量编码，计算出存储这个 block 里面所有文档最多需要多少位来保存每个 id，并且把这个位数作为头信息（header）放在每个 block 的前面。 因为这个 FOR 的编码是有解压缩成本的。利用 skip list，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的 block 的过程，从而节省了 cpu。推荐：250期面试题 五、总结 为了能够快速定位到目标文档，ES使用倒排索引技术来优化搜索速度，虽然空间消耗比较大，但是搜索性能提高十分显著。 为了能够在数量巨大的terms中快速定位到某一个term，同时节约对内存的使用和减少磁盘io的读取，lucene使用”term index -&gt; term dictionary -&gt; posting list”的倒排索引结构，通过FST压缩放入内存，进一步提高搜索效率。 为了减少posting list的磁盘消耗，lucene使用了FOR(Frame of Reference)技术压缩，带来的压缩效果十分明显。 ES的filter语句采用了Roaring Bitmap技术来缓存搜索结果，保证高频filter查询速度的同时降低存储空间消耗。 在联合查询时，在有filter cache的情况下，会直接利用位图的原生特性快速求交并集得到联合查询结果，否则使用skip list对多个posting list求交并集，跳过遍历成本并且节省部分数据的解压缩cpu成本。 Elaticsearch的索引思路 将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数（同时也利用磁盘顺序读特性），结合各种压缩算法，用及其苛刻的态度使用内存。 所以，对于使用Elasticsearch进行索引时需要注意： 不需要索引的字段，一定要明确定义出来，因为默示是自动建索引的 同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的 选择有规律的ID很重要，随机性太大的ID（比如Java的UUID）不利于查询 最后说一下，技术选型永远伴随着业务场景的考量，每种数据库都有自己要解决的问题（或者说擅长的领域），对应的就有自己的数据结构，而不同的使用场景和数据结构，需要用不同的索引，才能起到最大化加快查询的目的。 这篇文章讲的虽是 Lucene 如何实现倒排索引，如何精打细算每一块内存、磁盘空间、如何用诡谲的位运算加快处理速度，但往高处思考，再类比一下 MySQL，你就会发现，虽然都是索引，但是实现起来，截然不同。笼统的来说，b-tree 索引是为写入优化的索引结构。当我们不需要支持快速的更新的时候，可以用预先排序等方式换取更小的存储空间，更快的检索速度等好处，其代价就是更新慢，就像 ES。 希望本篇文章能给你带来一些收获~ 参考文档 https://www.elastic.co/cn/blog/frame-of-reference-and-roaring-bitmaps https://www.elastic.co/cn/blog/found-elasticsearch-from-the-bottom-up http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html https://www.infoq.cn/article/database-timestamp-02 https://zhuanlan.zhihu.com/p/137574234","categories":[],"tags":[]},{"title":"数据挖掘","slug":"SystemArchitect/第3章：数据库系统/数据挖掘","date":"2021-08-03T05:14:42.881Z","updated":"2021-08-03T05:24:37.268Z","comments":true,"path":"2021/08/03/SystemArchitect/第3章：数据库系统/数据挖掘/","link":"","permalink":"https://alloceee.github.io/2021/08/03/SystemArchitect/%E7%AC%AC3%E7%AB%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"","text":"数据挖掘是从数据库的大量数据中揭示出隐含的、先前未知的并有潜在价值的信息的非平凡过程，数据挖掘的任务有关联分析、聚类分析、分类分析、异常分析、特意群组分析和演变分析等等。 并非所有的信息发现任务都被称为数据挖掘。例如，使用数据库管理系统查找个别的记录，或通过英特网的搜索引擎查找特定的web页面，则是信息检索领域的任务。虽然这些任务是重要的，可能设计使用复杂的算法和数据结构，但是他们主要是依赖传统的计算机科学技术和数据的明显特征来创建索引结构，从而有效地组织和检索信息。","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"数据库系统","slug":"系统架构师/数据库系统","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"author":"Alloceee"},{"title":"ES 不香吗，为啥还要 ClickHouse？","slug":"搜索引擎/ES 不香吗，为啥还要 ClickHouse？","date":"2021-08-03T03:50:35.924Z","updated":"2021-08-03T12:21:34.487Z","comments":true,"path":"2021/08/03/搜索引擎/ES 不香吗，为啥还要 ClickHouse？/","link":"","permalink":"https://alloceee.github.io/2021/08/03/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/ES%20%E4%B8%8D%E9%A6%99%E5%90%97%EF%BC%8C%E4%B8%BA%E5%95%A5%E8%BF%98%E8%A6%81%20ClickHouse%EF%BC%9F/","excerpt":"Elasticsearch 是一个实时的分布式搜索分析引擎，它的底层是构建在 Lucene 之上的。简单来说是通过扩展 Lucene 的搜索能力，使其具有分布式的功能。 ES通常会和其他两个开源组件Logstash（日志采集）和Kibana（仪表盘）一起提供端到端的日志/搜索分析的功能，常常被简称为ELK。","text":"Elasticsearch 是一个实时的分布式搜索分析引擎，它的底层是构建在 Lucene 之上的。简单来说是通过扩展 Lucene 的搜索能力，使其具有分布式的功能。 ES通常会和其他两个开源组件Logstash（日志采集）和Kibana（仪表盘）一起提供端到端的日志/搜索分析的功能，常常被简称为ELK。 关于OLAP利器Clickhouse是俄罗斯搜索巨头Yandex开发的面向列式存储的关系型数据库。ClickHouse是过去两年中OLAP领域中最热门的，并与2016年开源。 ES是最为流行的大数据日志和搜索解决方案，但是近几年来，它的江湖地位受到了一些挑战，许多公司已经开始把自己的日志解决方案从ES迁移到了Clickhouse，这里就包括：携程，快手等公司。 架构和设计的对比ES的底层是Lucene，主要是要解决搜索的问题。搜索是大数据领域要解决的一个常见的问题，就是在海量的数据量要如何按照条件找到需要的数据。搜索的核心技术是倒排索引和布隆过滤器。 ES通过分布式技术，利用分片与副本机制，直接解决了集群下搜索性能与高可用的问题。 ElasticSearch 是为分布式设计的，有很好的扩展性，在一个典型的分布式配置中，每一个节点（node）可以配制成不同的角色。","categories":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://alloceee.github.io/categories/ElasticSearch/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://alloceee.github.io/tags/ElasticSearch/"}],"author":"Alloceee"},{"title":"第4章 计算机网络","slug":"SystemArchitect/第4章：计算机网络","date":"2021-08-01T04:26:47.530Z","updated":"2021-08-02T03:37:29.269Z","comments":true,"path":"2021/08/01/SystemArchitect/第4章：计算机网络/","link":"","permalink":"https://alloceee.github.io/2021/08/01/SystemArchitect/%E7%AC%AC4%E7%AB%A0%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","excerpt":"","text":"网络架构与协议网络工程网络规划网络设计网络逻辑结构设计网络逻辑结构设计是体现网络设计核心思想的关键阶段，在这一阶段根据需求规范和通信规范，选择一种比较适宜的网络逻辑结构，并基于该逻辑结构实施后续的资源分配规划、安全规划等内容。 在逻辑网络设计阶段，需要描述满足用户需求的网络行为及性能，详细说明数据是如何在网络上传输的，此阶段不涉及网络元素的具体物理位置。网络设计者利用需求分析和现有网络体系分析的结果来设计逻辑网络结构。如果现有的软件、硬件不能满足新网络的需求，现有系统就必须升级。如果现有系统能继续运行使用，可以将它们集成到新设计中来。如果不集成旧系统，网络设计小组可以找一个新系统，对它进行测试，确定是否符合用户的需求。 此阶段最后应该得到一份逻辑网络设计文档。 网络物理结构设计物理网络设计是对逻辑网络设计的物理实现，通过对设备的具体物理分布、运行环境等的确定，确保网络的物理连接符合逻辑连接的要求。在这一阶段，网络设计者需要确定具体的软硬件、连接设备、布线和服务。 分层设计为了能够更好地分析与设计复杂的大型互连网络，在计算机网络设计中，主要采用分层（分级）设计模型，它类似于软件工程中的结构化设计。通过一些通用规则来设计网络，就可以简化设计、优化带宽的分配和规划。在分层设计中，引入了三个关键层的概念，分别是核心层、汇聚层和接入层。 通常将网络中直接面向用户连接或访问网络的部分称为接入层，将位于接入层和核心层之间的部分称为分布层或汇聚层。接入层的目的是允许终端用户连接到网络，因此，接入层交换机具有低成本和高端口密度特性。 汇聚层是核心层和接入层的分界面，完成网络访问策略控制、数据包处理、过滤、寻址，以及其他数据处理的任务。汇聚层交换机是多台接入层交换机的汇聚点，它必须能够处理来自接入层设备的所有通信量，并提供到核心层的上行链路，因此，汇聚层交换机与接入层交换机比较，需要更高的性能，更少的接口和更高的交换速率。 网络主干部分称为核心层，核心层的主要目的在于通过高速转发通信，提供优化、可靠的骨干传输结构，因此，核心层交换机应拥有更高的可靠性，性能和吞吐量。核心层为网络提供了骨干组件或高速交换组件，在纯粹的分层设计中，核心层只完成数据交换的特殊任务。需要根据网络需求的地理距离、信息流量和数据负载的轻重来选择核心层技术，常用的技术包括 ATM、100Base-Fx 和千兆以太网等。在主干网中，考虑到高可用性的需求，通常会使用双星（树）结构，即采用两台同样的交换机，与汇聚层交换机分别连接，并使用链路聚合技术实现双机互联。 层次化网络设计应该遵循一些简单的原则，这些原则可以保证设计出来的网络更加具有层层次的特性： 在设计时，设计者应该尽量控制层次化的程度。一般情况下，由核心层、汇聚层、接入层三个层次就足够了，过多的层次会导致整体网络性能的下降，并且会提高网络的延迟，但是方便网络故障排查和文档编写。 在接入层应当保持对网络结构的严格控制，接入层的用户总是为了获取更大的外部网络访问带宽，而随意申请其他的渠道访问外部网路是不允许的。 为了保证网络的层次性，不能在设计中随意加入额外链接，额外连接是指打破层次性，在不相邻层次间的连接，这些连接会导致网络中的各种问题，例如缺乏汇聚层的访问控制和数据包过滤等。 在进行设计时，应当首先设计接入层，根据流量负载、流量和行为的分析，对上层进行更精细的容量规划，再依次完成各上层的设计。 除去接入层的其他层次，应尽量采用模块化方式，每个层次由多个模块或者设备集合构成，每个模块间的边界应非常清晰。 DNS查询过程分为两种查询方式：递归查询和迭代查询 递归查询的查询方式为：当用户发出查询请求时，本地服务器要进行递归查询。这种查询方式要求服务器彻底地进行名字，并返回最后的结果——IP地址或错误信息。如果查询请求在本地服务器中不能完成，那么服务器就根据它的配置向域名树中的上级服务器进行查询，在最坏的情况下可能要查询到根服务器。每次查询返回的结果如果是其他名字服务器的IP地址，则本地服务器要把查询请求发送给这些服务器，故进一步的查询。 迭代查询的查询方式为：服务器与服务器之间的查询采用迭代的方式进行，发出查询请求的服务器得到的响应可能不是目标的IP地址，而是其他服务器的引用（名字和地址），那么本地服务器就要访问被引用的服务器，做进一步的查询。如此反复多次，每次都要接近目标的授权服务器，直至到最后的结果——目标的IP地址或错误信息。 根域名服务器为众多请求提供域名，若采用递归方式会大大影响性能。","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"计算机网络","slug":"系统架构师/计算机网络","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"author":"Alloceee"},{"title":"为什么MySQL使用B+树","slug":"数据库/MySQL/other/为什么MySQL使用B+树","date":"2021-07-27T09:10:38.662Z","updated":"2021-07-30T09:43:24.547Z","comments":true,"path":"2021/07/27/数据库/MySQL/other/为什么MySQL使用B+树/","link":"","permalink":"https://alloceee.github.io/2021/07/27/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/%E4%B8%BA%E4%BB%80%E4%B9%88MySQL%E4%BD%BF%E7%94%A8B+%E6%A0%91/","excerpt":"在具体分析 InnoDB 使用 B+ 树背后的原因之前，我们需要为 B+ 树找几个『假想敌』，因为如果我们只有一个选择，那么选择 B+ 树也并不值得讨论，找到的两个假想敌就是 B 树和哈希，我们就以这两种数据结构为例，分析比较 B+ 树的优点。","text":"在具体分析 InnoDB 使用 B+ 树背后的原因之前，我们需要为 B+ 树找几个『假想敌』，因为如果我们只有一个选择，那么选择 B+ 树也并不值得讨论，找到的两个假想敌就是 B 树和哈希，我们就以这两种数据结构为例，分析比较 B+ 树的优点。 概述首先需要澄清的一点是，MySQL跟B+树没有直接的关系，真正与B+树有关系的是MySQL的默认存储引擎InnoDB，MySQL中存储引擎的主要作用是负责数据的存储和提取，除了InnoDB之外，MySQL中也支持MyISAM作为表的底层存储引擎。 我们在使用 SQL 语句创建表时就可以为当前表指定使用的存储引擎，你能在 MySQL 的文档 Alternative Storage Engines 中找到它支持的全部存储引擎，例如：MyISAM、CSV、MEMORY 等，然而默认情况下，使用如下所示的 SQL 语句来创建表就会得到 InnoDB 存储引擎支撑的表： 1234CREATE TABLE t1 ( a INT, b CHAR (20), PRIMARY KEY (a)) ENGINE=InnoDB; 想要详细了解 MySQL 默认存储引擎的读者，可以通过之前的文章 『浅入浅出』MySQL 和 InnoDB 了解包括 InnoDB 存储方式、索引和锁等内容，我们在这里主要不会介绍 InnoDB 相关的过多内容。 我们今天最终将要分析的问题其实还是，为什么 MySQL 默认的存储引擎 InnoDB 会使用 MySQL 来存储数据，相信对MySQL稍微有些了解的人都知道，无论是表中的数据（主键索引）还是辅助索引最终都会使用B+树来存储数据，其中前者在表中会以&lt;id,row&gt;的方式存储，而后者会以&lt;index,id&gt;的方式进行存储，这其实也比较好理解： 在主键索引中，id是主键，我们能够通过id找到该行的全部列； 在辅助索引中，索引中的几个列构成了键，我们能够通过索引中的列找到id，如果有需要的话，可以再通过id找到当前数据行的全部内容； 对于InnoDB来说，所有的数据都是以键值对的方式存储的，主键索引和辅助索引在存储数据时都会将id和index作为键，将所有列和id作为键对应的值。 在具体分析 InnoDB 使用 B+ 树背后的原因之前，我们需要为 B+ 树找几个『假想敌』，因为如果我们只有一个选择，那么选择 B+ 树也并不值得讨论，找到的两个假想敌就是 B 树和哈希，相信这也是很多人会在面试中真实遇到的问题，我们就以这两种数据结构为例，分析比较 B+ 树的优点。 设计为什么MySQL的InnoDB存储引擎会选择B+树作为底层数据结构，而不选择B树或者哈希？在这一节中，我们将通过以下的两个方面介绍InnoDB这样选择的原因。 InnoDB需要支持的场景和功能需要在特定查询上拥有较强的性能； CPU将磁盘上的数据加载到内存中需要花费大量的时间，这使得B+树成为了非常好的选择。 数据的持久化以及持久化数据的查询其实是一种常见的需求，而数据的持久化就需要我们与磁盘、内存和CPU打交道；MySQL作为OLTP的数据库不仅需要具备事务的处理能力，而且要保证数据的持久化而且能够有一定的实时数据查询能力，这些需求共同决定了B+树的选择，接下来我们会详细分析上述两个原因背后的逻辑。 读写性能OLTP代表Online Transaction Processing，与OLTP相比的还有OLAP：Online Analytical Processing，从这两个名字中我们就可以看出，前者指的是传统的关系型数据库，主要用于处理基本的、日常的事务处理，而后者主要在数据仓库中使用，用于支持一些复杂的分析和决策。 作为支撑CLTP数据的存储引擎，我们经常会使用InnoDB完成以下的一些工作： 通过 INSERT、UPDATE 和 DELETE 语句对表中的数据进行增加、修改和删除； 通过 UPDATE 和 DELETE 语句对符合条件的数据进行批量的删除； 通过 SELECT 语句和主键查询某条记录的全部列； 通过 SELECT 语句在表中查询符合某些条件的记录并根据某些字段排序； 通过 SELECT 语句查询表中数据的行数； 通过唯一索引保证表中某个字段或者某几个字段的唯一性； 如果我们使用B+树作为底层的数据结构，那么所有只会访问或者修改一条数据的SQL的时间复杂度都是O($log_n$)，也就是树的高度，但是使用哈希却有可能达到O(1)的时间复杂度，看起来是不是特别的美好，但是当我们使用如下所示的SQL时，哈希的表现就不会这么好了： 1234SELECT * FROM posts WHERE author = &#x27;draven&#x27; ORDER BY created_at DESCSELECT * FROM posts WHERE comments_count &gt; 10UPDATE posts SET github = &#x27;github.com/draveness&#x27; WHERE author = &#x27;draven&#x27;DELETE FROM posts WHERE author = &#x27;draven&#x27; 如果我们使用哈希作为底层的数据结构，遇到上述的场景时，使用哈希构成的主键索引或者辅助索引可能就没有办法快速处理了，它对于处理范围查询或者排序性能会非常差，只能进行全表扫描并依次判断是否满足条件。全表扫描对于数据库来说是一个非常糟糕的结果，这其实也就意味着我们使用的数据结构对于这些查询没有其他任何效果，最终的性能可能都不如从日志中顺序进行匹配。 使用B+树其实能够保证按照键的顺序进行存储，也就是相邻的所有数据其实都是按照自然顺序排列的，使用哈希却无法达到这样的效果，因为哈希函数的目的就是让数据尽可能被分散到不同的桶中进行存储，所以在遇到可能存在相同键author = ‘draven’ 或者排序以及范围查询comments_count &gt; 10时，由哈希作为底层数据结构的表可能就会面对数据库查询的噩梦 —— 全表扫描。 B树和B+树在数据结构上其实有一些类似，它们都可以按照某些顺序对索引中的内容进行遍历，对于排序和范围查询等操作，B树和B+树相比于哈希会带来更好的性能，当然如果索引建立不够友好或者SQL查询非常复杂，依然会导致全表扫描。 数据加载既然使用哈希无法应对我们常见的SQL中排序和范围查询等操作，而B树和B+树都可以相对高效地执行这些查询，那么为什么我们不选择B树呢？这个原因其实非常简单——计算机在读写文件时会以页为单位将数据加载到内存中。页的大小可能会根据操作系统的不同而发生变化，不过在大多数的操作系统中，页的大小都是4KB，你可以通过如下的命令获取操作系统上的页大小： 12$ getconf PAGE_SIZE4096 当我们需要在数据库中查询数据时，CPU 会发现当前数据位于磁盘而不是内存中，这时就会触发 I/O 操作将数据加载到内存中进行访问，数据的加载都是以页的维度进行加载的，然而将数据从磁盘读取到内存中所需要的成本是非常大的，普通磁盘（非 SSD）加载数据需要经过队列、寻道、旋转以及传输的这些过程，大概要花费 10ms 左右的时间。 我们在估算 MySQL 的查询时就可以使用 10ms 这个数量级对随机 I/O 占用的时间进行估算，这里想要说的是随机 I/O 对于 MySQL 的查询性能影响会非常大，而顺序读取磁盘中的数据时速度可以达到 40MB/s，这两者的性能差距有几个数量级，由此我们也应该尽量减少随机 I/O 的次数，这样才能提高性能。 B树与B+树的最大区别就是，B树可以在非叶结点中存储数据，但是B+树的所有数据其实都存储在叶子结点中，当一个表底层的数据结构是B树时，假设我们需要访问所有『大于 4，并且小于 9 的数据』： 如果不考虑任何优化，在上面的简单B树中我们需要进行4次磁盘的随机I/O才能找到所有满足条件的数据行： 加载根节点所在的页，发现根节点的第一个元素是 6，大于 4； 通过根节点的指针加载左子节点所在的页，遍历页面中的数据，找到 5； 重新加载根节点所在的页，发现根节点不包含第二个元素； 通过根节点的指针加载右子节点所在的页，遍历页面中的数据，找到 7 和 8； 当然我们可以通过各种方式来对上述的过程进行优化，不过 B 树能做的优化 B+ 树基本都可以，所以我们不需要考虑优化 B 树而带来的收益，直接来看看什么样的优化 B+ 树可以做，而 B 树不行。 由于所有的节点都可能包含目标数据，我们总是要从根节点向下遍历子树查找满足条件的数据行，这个特点带来了大量的随机 I/O，也是 B 树最大的性能问题。 B+ 树中就不存在这个问题了，因为所有的数据行都存储在叶节点中，而这些叶节点可以通过『指针』依次按顺序连接，当我们在如下所示的 B+ 树遍历数据时可以直接在多个子节点之间进行跳转，这样能够节省大量的磁盘 I/O 时间，也不需要在不同层级的节点之间对数据进行拼接和排序；通过一个 B+ 树最左侧的叶子节点，我们可以像链表一样遍历整个树中的全部数据，我们也可以引入双向链表保证倒序遍历时的性能。 有些读者可能会认为使用 B+ 树这种数据结构会增加树的高度从而增加整体的耗时，然而高度为 3 的 B+ 树就能够存储千万级别的数据，实践中 B+ 树的高度最多也就 4 或者 5，所以这并不是影响性能的根本问题。 总结任何不考虑应用场景的设计都不是最好的设计，当我们明确的定义了使用 MySQL 时的常见查询需求并理解场景之后，再对不同的数据结构进行选择就成了理所当然的事情，当然 B+ 树可能无法对所有 OLTP 场景下的查询都有着较好的性能，但是它能够解决大多数的问题。 我们在这里重新回顾一下 MySQL 默认的存储引擎选择 B+ 树而不是哈希或者 B 树的原因： 哈希虽然能够提供 O(1) 的单数据行操作性能，但是对于范围查询和排序却无法很好地支持，最终导致全表扫描； B 树能够在非叶节点中存储数据，但是这也导致在查询连续数据时可能会带来更多的随机 I/O，而 B+ 树的所有叶节点可以通过指针相互连接，能够减少顺序遍历时产生的额外随机 I/O； 如果想要追求各方面的极致性能也不是没有可能，只是会带来更高的复杂度，我们可以为一张表同时建 B+ 树和哈希构成的存储结构，这样不同类型的查询就可以选择相对更快的数据结构，但是会导致更新和删除时需要操作多份数据。 从今天的角度来看，B+ 树可能不是 InnoDB 的最优选择，但是它一定是能够满足当时设计场景的需要，从 B+ 树作为数据库底层的存储结构到今天已经过了几十年的时间，我们不得不说优秀的工程设计确实有足够的生命力。而我们作为工程师，在选择数据库时也应该非常清楚地知道不同数据库适合的场景，因为软件工程中没有银弹。 到最后，我们还是来看一些比较开放的相关问题，有兴趣的读者可以仔细思考一下下面的问题： 常用于分析的 OLAP 数据库一般会使用什么样的数据结构存储数据？为什么？ Redis 是如何对数据进行持久化存储的？常见的数据结构都有什么？ 如果对文章中的内容有疑问或者想要了解更多软件工程上一些设计决策背后的原因，可以在博客下面留言，作者会及时回复本文相关的疑问并选择其中合适的主题作为后续的内容。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://alloceee.github.io/tags/MySQL/"}],"author":"Alloceee"},{"title":"SQL如何使用Concat连接两列数据","slug":"数据库/MySQL/other/concat组合多个字段","date":"2021-07-26T16:00:00.000Z","updated":"2021-08-01T03:35:29.856Z","comments":true,"path":"2021/07/27/数据库/MySQL/other/concat组合多个字段/","link":"","permalink":"https://alloceee.github.io/2021/07/27/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/concat%E7%BB%84%E5%90%88%E5%A4%9A%E4%B8%AA%E5%AD%97%E6%AE%B5/","excerpt":"使用concat直接组合多个字段，连接字符串和嵌套使用","text":"使用concat直接组合多个字段，连接字符串和嵌套使用 方法/步骤concat直接组合多个字段1select concat(name,uid) as name from user concat不仅可以使用字段名，也可以使用字符串1select concat(name,&#x27;~&#x27;,uid) as name from user concat嵌套使用1select concat(name,concat(uid,sex)) as name from user","categories":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://alloceee.github.io/tags/MySQL/"}],"author":"Alloceee"},{"title":"第5章 系统性能评价","slug":"SystemArchitect/第5章：系统性能评价","date":"2021-07-25T16:00:00.000Z","updated":"2021-07-28T08:26:44.106Z","comments":true,"path":"2021/07/26/SystemArchitect/第5章：系统性能评价/","link":"","permalink":"https://alloceee.github.io/2021/07/26/SystemArchitect/%E7%AC%AC5%E7%AB%A0%EF%BC%9A%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7/","excerpt":"性能设计主要包含两方面的内容：一是作为未来计算机发展的参考和规划；另一个则是对现有系统进行性能上的调整已达到最优化。","text":"性能设计主要包含两方面的内容：一是作为未来计算机发展的参考和规划；另一个则是对现有系统进行性能上的调整已达到最优化。 5.1 性能指标描述当前流行系统主要涉及的性能指标 5.1.1 对计算机评价的主要性能指标1.时钟频率（主频）主频是计算机的主要性能指标之一，在很大程度上决定了计算机的运算速度。CPU的工作节拍是由主时钟来控制的，主时钟不断产生固定频率的时钟脉冲，这个主时钟的评率即是CPU的主频。主频越高，意味着CPU的工作节拍就越快，运算速度也就越快。现在已经发展为多核心CPU，除了看时钟频率还得看单个CPU中的内核数。 2.高速缓存高速缓存可以提高CPU的运行效率。目前一般采用两级高速缓存技术，有些使用三层。高速缓冲存储器均由静态RAM（Random Access Memory，随机存取存储器）组成，结构较复杂，在 CPU 管芯面积不能太大的情况下，L1 级高速缓存的容量不可能做得太大。采用回写（WriteBack）结构的高速缓存。它对读和写操作均有可提供缓存。而采用写通（Write-through）结构的高速缓存，仅对读操作有效。L2 及 L3 高速缓存容量也会影响 CPU的性能，原则是越大越好。 性能计算描述当前使用到的主要性能指标的计算方法 性能设计描述如何对现有系统进行性能上的调整优化，并介绍几个已经成熟的设计规则和解决方案 性能评估描述如何对当前取得的性能指标进行评价和改进","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}]},{"title":"第10章：设计模式","slug":"SystemArchitect/第10章：设计模式","date":"2021-07-21T06:33:00.000Z","updated":"2021-07-27T06:48:02.887Z","comments":true,"path":"2021/07/21/SystemArchitect/第10章：设计模式/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC10%E7%AB%A0%EF%BC%9A%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"1.设计模式概述1.1设计模式的概念GoF设计模式：对被用来在特定场景下解决一般设计问题的类和互相通信的对象的描述。通俗地说，可以把设计模式理解为对某一类问题的通用解决方案。 1.2设计模式的组成一般的，在描述一个设计模式时，至少需要包含四个方面： 模式名称（Pattern name） 问题（Problem） 解决方案（Solution） 效果（Consequence） 这四个方面就是设计模式的四要素。 1.3GoF设计模式（模式介绍）创建型1.Factory Method模式（工厂方法）Factory Method 模式提供了一种延迟创建类的方法，使用这个方法可以在运行期由子类决定创建哪一个类的实例。 2.Abstract Factory模式（抽象工厂）Abstract Factory又称为抽象工厂模式，该模式主要为解决复杂系统中对象创建的问题。抽象工厂模式提供了一个一致的对象创建接口来创建一系列具有相似基类或相似接口的对象。 3.Buider模式（建造器）Builder模式与Abstract Factory模式非常类似，但Builder模式是逐步地构造出一个复杂对象，并在最后返回对象的实例。Builder模式可以把复杂对象的创建与表示分离，使得同样的创建过程可以创建不同的表示。 4.Prototype模式（原型）Prototype模式可以根据原型实例制定创建的对象的种类，并通过深复制这个原型来创建新的对象。Prototype模式有着同Abstract Factory模式和Builder模式相同的效果，不过当需要实例化的类是在运行期才被指定的而且要避免创建一个与产品曾是平行的工厂类层次时，可以使用Prototype模式。使用Prototype模式可以在运行时增加或减少原型，比Abstract Factory和Builder模式更加灵活。 5.Singleton模式（单例）使用Singleton模式可以保证一个类仅有一个实例，从而可以提供一个单一的全局访问点。 结构型6.Adapter模式（适配器）Adapter模式可以解决系统间接口不相容的问题。通过Adapter可以把类的接口转化为客户程序所希望的接口，从而提高复用性。 7.Bridge模式（桥接）Bridge模式基于类的最小设计的原则，通过使用封装、聚合及继承等行为让不同的类承担不同的职责。它的主要特点是把抽象（Abstraction）与行为实现（Implementation）分离开来，从而可以保证各部分的独立性以及应对他们的功能拓展。 8.Composite模式（组合）Composite模式提供了一种以树形结构组合对象的方法，使用Composite可以使单个对象和组合后的对象具有一致性以提高软件的复用性。 9.Decorator模式（装饰者）Decorator模式可以动态地为对象的某一个方法增加更多的功能。在更多时候，使用Decorator模式可以不必继承出新的子类从而维护简介的类继承结构。 10.Facade模式（门面）Facade模式为一组类提供了一致的访问接口。使用Facade可以封装内部具有不同接口的类，使其对外提供统一的访问方法。Facade模式在J2EE系统开发中发展为Session Facade模式。 11.Flyweight模式（享元）Flyweight模式可以共享大量的细粒度对象，从而节省创建对象所需要分配的空间，不过在时间上的开销会变大。 12.Proxy模式（代理）顾名思义，Proxy模式为对象提供了一种访问代理，通过对象Proxy可以控制客户程序的访问。例如：访问权限的控制、访问地址的控制、访问方式的控制等，甚至可以通过Proxy将开销较大的访问化整为零，提高访问效率。 行为型13.Interpreter模式（解释器）定义了一个解释器，来解释遵循给定语言和文法的句子。 14.Template Method模式（模板）定义一个操作的模板，其中的一些步骤会在子类中实现，以适应不同的情况。 15.Chain of Responsibility模式（责任链）Chain of Responsibility模式把可以响应请求的对象组织成一条链，并在这条对象链上传递对象，从而保证多个对象都有机会处理请求而且可以避免请求方和响应方的耦合。 16.Command模式（命令）将请求封装为对象，从而增强请求的能力，如参数化、排队、记录日志等。 17.Iterator模式（迭代器）Iterator模式提供了顺序访问一个对象集合的各元素的方法，使用Iterator可以避免暴露集合中对象的耦合关系。 18.Mediator模式（中介者）Mediator模式可以减少系统中对象间的耦合性。Mediator模式使用中介对象封装其他对象，从而使这些被封装的对象间的关系就成了松散耦合。 19.Memento模式（备忘录）Memento模式提供了一种捕获对象状态的方法，且不会破坏对象的封装，并且可以在对象外部保存对象的状态，并在需要的时候恢复对象状态。 20.Observer模式（观察者）Observer模式提供了将对象的状态广播到一组观察者的方式，从而可以让每个观察者随时都可以得到对象更新的通知。 21.State模式（状态）State模式允许一个对象在其内部状态改变的时候改变他的行为。 22.Strategy（策略）使用Strategy模式可以让对象中算法的变化独立于客户。 23.Visitor模式（访问者）表示对某对象结构中各元素的操作，使用Visitor模式可以在不改变各元素类的前提下定义作用于这些元素的新操作。 1.5设计模式与软件架构1.6设计模式分类 创建型 结构型 行为型 应用范围 应用于类 Factory Method Adapter Interpreter Template Method 应用于对象 Abstract Factory Builder Prototype Singleton Adapter Bridege Composite Decorator Facade Flyweight Proxy Chain of Responsiblity Command Interator Mediator Memento Observer State Strategy Visitor 1.7设计模式的六大原则开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行扩展的时候，不能去修改原有的代码，实现一个热拔插的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级，想要达到这样的效果，我们需要使用接口和抽象类。 1.8模式具体分析5.Signleton模式7.Bridge模式桥接模式的角色和职责： 1.Client调用端 这是Bridge模式的调用者。 2.抽象类（Abstraction） 抽象类接口（接口这货抽象类）维护对行为实现（implementation）的引用，他的角色就是桥接类。 3.Refined Abstraction 这是Abstraction的子类。 4.Implementor 行为实现类接口（Abstraction接口定义了基于Implementor接口的更高层次的操作） 5.ConcreteImplmentor Implementor的子类。 桥接模式的UML图如下： 总结： 1.桥接模式的优点 （1）实现了抽象和实现部分的分离 桥接模式分离了抽象部分和实现部分，从而极大地提供了系统的灵活性，让抽象部分和实现部分独立开来，分别定义接口，这有助于系统进行分层设计，从而产生更好的结构化系统。对于系统的高层部分，只需要知道抽象部分和实现部分的接口就可以了。 （2）更好的可扩展性 由于桥接模式把抽象部分和实现部分分离了，从而分别定义接口，这就使得抽象部分和实现部分可以分别独立扩展，而不会相互影响，大大的提供了系统的可扩展性。 （3）可动态的切换实现 由于桥接模式实现了抽象和实现的分离，所以在实现桥接模式时，就可以实现动态的选择和使用具体的实现。 （4）实现细节对客户端透明，可以对用户隐藏实现细节。 2.桥接模式的缺点 （1）桥接模式的引入增加了系统的理解和设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计和编程。 （2）桥接模式要求正确识别出系统中两个独立变化的维度，因此其使用范围有一定的局限性。 3.桥接模式的使用场景 （1）如果一个系统需要在构建的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使他们在抽象层建立一个关联关系。 （2）抽象化角色和实现化角色可以继承的方式独立扩展而互不影响，在程序运行时可以动态将一个抽象化子类的对象和一个实现化子类的对象进行组合，即系统需要对抽象化角色和实现化角色进行动态耦合。","categories":[],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}]},{"title":"第9章：软件架构设计","slug":"SystemArchitect/第9章：软件架构设计","date":"2021-07-21T06:32:00.000Z","updated":"2021-07-28T09:26:39.630Z","comments":true,"path":"2021/07/21/SystemArchitect/第9章：软件架构设计/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC9%E7%AB%A0%EF%BC%9A%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/","excerpt":"软件架构设计的一个核心问题是否使用重复的软件架构模式，即能否达到架构级别的软件重用。也就是说，能否在不同的软件系统中，使用同一架构。","text":"软件架构设计的一个核心问题是否使用重复的软件架构模式，即能否达到架构级别的软件重用。也就是说，能否在不同的软件系统中，使用同一架构。 3.软件架构风格软件架构设计的一个核心问题是否使用重复的软件架构模式，即能否达到架构级别的软件重用。也就是说，能否在不同的软件系统中，使用同一架构。 软件架构风格是描述某一特定应用领域中系统组织方式的惯用模式（idiomatic paradigm），定义了用于描述系统的术语表和一组指导构建系统的规则。 3.1软件架构风格分类（1）设计词汇表是什么？ （2）构建和连接件的类型是什么？ （3）可容许的结构模式是什么？ （4）基本的计算模型是什么？ （5）风格的基本不变性是什么？ （6）其使用的常见例子是什么？ （7）使用此风格的优缺点是什么？ （8）其常见的特例是什么？ 3.2数据流风格数据流风格的软件架构是一种最常见，结构最为简单的软件架构。这样的架构下，所有的数据按照流的形式在执行过程中前进，不存在结构的反复和重构，就像工厂中的汽车流水线一样，数据就像汽车零部件一样的流水线的各个节点上被加工，最终输出所需要的结果（一步完整的汽车）。在流动过程中，数据经过序列间的数据处理组件进行加工，然后将处理结果向后传送，最后进行输出。 批处理序列批处理风格的每一步处理都是独立的，并且每一步是顺序执行的。只有当前一步处理完，后一步处理才能开始。数据传送在步与步之间作为一个整体。（组件为一系列固定顺序的计算单元，组件间只通过数据传递交互。每个处理步骤是一个独立的程序，每一步必须在前一步结束后才能开始，数据必须是完整的，以整体的方式传递）。 批处理的典型应用： （1）经典数据处理； （2）程序开发； （3）Windows 下的 BAT 程序就是这种应用的典型实例 管道-过滤器在管道/过滤器风格的软件架构中，每个构件都有一组输入和输出，构件读输入的数据流，经过内部处理，然后产生输出数据流。这个过程通常通过对输入流的变换及增量计算来完成，所以在输入被完全消费之前，输出便产生了。因此，这里的构件被称为过滤器，这种风格的连接件就像是数据流传输的管道，将一个过滤器的输出传到另一过滤器的输入。 此风格特别重要的过滤器必须是独立的实体，它不能与其他的过滤器共享数据，而且一个过滤器不知道它上游和下游的标识。一个管道/过滤器网络输出的正确性并不依赖于过滤器进行增量计算过程的顺序。 典型应用： （1）以UNIX Shell编写的程序。UNIX 既提供一种符号，以连接各组成部分（UNIX 的进程），又提供某种进程运行时机制以实现管道。 （2）传统的编译器。传统的编译器一直被认为是一种管道系统，在该系统中，一个阶段（包括词法分析、语法分析、语义分析和代码生成）的输出是另一个阶段的输入。 特点3.3调用/返回风格调用返回风格顾名思义，就是指在系统中采用了调用与返回机制。利用调用-返回实际上是一种分而治之的策略，其主要思想是将一个复杂的大系统分解为一些子系统，以便降低复杂度，并且增加可修改性。程序从其执行起点开始执行该构建的代码，程序执行结束，将控制返回给程序调用构件。 主程序/子程序主程序/子程序风格是结构化开发时期的经典架构风格。这种风格一般采用单线程控制，把问题划分为若干处理步骤，构件即为主程序和子程序。子程序通常可合成为模块。过程调用作为交互机制，即充当连接件。调用关系具有层次性，其语义逻辑表现为子程序的正确性，取决于它调用的子程序的正确性。 面向对象风格抽象数据类型概念对软件系统有着重要作用，目前软件界已普遍使用面向对象系统。这种风格建立在数据抽象和面向对象的基础上，数据的表示方法和它们的相应操作封装在一个抽象数据类型或对象中。这种风格的构件是对象，或者说是抽象数据类型的实例。对象是一种被称作管理者的构件，因为它负责保持资源的完整性。对象是通过函数和过程的调用来交互的。 特点：（1）对象负责维护其表示的完整性； 层次结构5.面向服务的架构SOA是一种在计算环境中设计、开发、部署和管理离散逻辑单元（服务）模型的方法。","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}],"author":"Alloceee"},{"title":"第1章：计算机组成与体系结构","slug":"SystemArchitect/第1章：计算机组成与体系结构","date":"2021-07-20T16:00:00.000Z","updated":"2021-07-28T03:39:22.787Z","comments":true,"path":"2021/07/21/SystemArchitect/第1章：计算机组成与体系结构/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC1%E7%AB%A0%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/","excerpt":"","text":"1.计算机系统1.3 复杂指令集系统与精简指令集系统 指令系统类型 指令 寻址方式 实现方式 其他 CISC（复杂） 数量多，使用频率差别大，可变长格式 支持多种 微程序控制技术 研制周期长 RISC（精简） 数量少，使用频率接近，定长格式，大部分为单周期指令，操作寄存器，只有Load/Store 支持方式少 增加了通用寄存器；硬布线逻辑控制为主；适合采用流水线 优化编译，有效支持高级语言 2.存储器系统2.3 Cache存储器Cache的性能是计算机系统性能的重要方面。命中率是cache的一个重要指标，但不是最重要的指标。cache设计的主要目标是在成本允许的情况下达到较高的命中率，使存储系统具有最短的平均访问时间。cache的命中率和cache容量的关系是：cache容量越大，则命中率越高，随着容量的增加，其失效率接近0%（命中率接近100%）。但是，增加cache容量意味着增加cache的成本和增加cache的命中时间。 3.流水线流水线技术把一个任务分解为若干顺序执行的子任务，不同的子任务由不同的执行机构复杂执行，而这些机构可以同时并行工作。在任一时刻，任一任务只占用其中一个执行机构，这个就可以实现多个任务的重叠执行，以提高工作效率。 3.1 流水线周期流水线应用过程中，会将需要处理的工作分为 N 个阶段，最耗时的那一段所消耗的时间为流水线周期。 3.2 计算流水线执行时间以流水线的执行时间可通俗的表达为： 流水线执行时间=第 1 条指令的执行时间+（n-1）*流水线周期 n 代表需要处理的任务数量。 而实际上，真正做流水线处理时，考虑到处理的复杂性，会将指令的每个执行阶段的时 间都统一为流水线周期，即 1 条指令的执行时间为：4ms+4ms+4ms=12ms。 所以：实际流水线执行时间=4ms+4ms+4ms+(100-1)*4=408ms。 考试时 80%以上的概率采用理论公式计算，所以考试时需要以理论公式计算，若计算的结果无正确选项才考虑采用实际公式计算。 3.3 流水线的吞吐率流水线的吞吐率（Though Put rate，TP）是指在单位时间内流水线所完成的任务数量或输出的结果数量。有些文献也称为平均吞吐率、实际吞吐率。计算流水线吞吐率的最基本的公式如下：$$TP = \\frac{n}{T_k}$$其中n为任务数，$T_k$是处理完成n个任务所用的时间。 流水线的最大吞吐率为：$$TP_{max} = \\lim_{n\\to\\infty}\\frac{n}{(k+n-1)\\Delta{t}} = \\frac{1}{\\Delta{t}}$$ 3.4 流水线的加速比加速比：不使用流水线的执行时间/使用流水线的执行时间 如果不使用流水线，即顺序执行所用的时间为 $T_0$ ，使用流水线的执行时间为$T_k$，则计算流水线加速比的基本公式如下：$$S = \\frac{T_0}{T_k}$$如果流水线各个流水段的执行时间都相等（设为Dt），则一条k段流水线完成n个连续任务所需要的时间为(k+n-1)Dt。如果不使用流水线，即顺序执行这 n 个任务，则所需要的时间为 nkDt。因此，各个流水段执行时间均相等的一条 k 段流水线完成 n 个连续任务 时的实际加速比为：$$S = \\frac{nk\\Delta{t}}{(k+n-1)\\Delta{t}} = \\frac{nk}{k+n-1}$$这种情况下的最大加速比为：$$S_{max} = \\lim_{n\\to\\infty}\\frac{nk}{k+n-1} = k$$效率：即流水线设备的利用率，指流水线中的设备实际使用时间与整个运行时间的比值","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}]},{"title":"第2章：操作系统","slug":"SystemArchitect/第2章：操作系统","date":"2021-07-20T16:00:00.000Z","updated":"2021-08-01T04:23:28.829Z","comments":true,"path":"2021/07/21/SystemArchitect/第2章：操作系统/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC2%E7%AB%A0%EF%BC%9A%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"操作系统的类型与结构操作系统是计算机系统中最基本的系统软件，它既是管理计算年级系统的软、硬件资源，又控制程序的执行。 从资源管理的角度看，操作系统主要是对处理器、存储器、文件、设备和作业进行管理。 操作系统的定义操作系统（Operating System，OS）是计算机系统中的核心系统软件，负责管理和控制计算机系统中的硬件和软件资源，合理地组织计算机工作流程和有效地利用资源，在计算机与用户之间起接口的作用。操作系统为用户提供的接口表现形式一般为：命令、菜单、窗口之类的，而操作系统为应用程序提供的接口为 API。 操作系统分类按照操作系统的功能划分，操作系统的基本类型有批处理操作系统、分时操作系统、实时操作系统、网络操作系统、分布式操作系统、嵌入式操作系统、微内核操作系统等。 实时操作系统（RTOS）任务是RTOS中最重要的操作对象，每个任务在RTOS的调度下由CPU分时执行。任务的调度目前主要有时间分片式、轮流查询式和优先抢占式三种，不同的RTOS可能支持其中一种或几种，其中优先抢占式对实时性的支持最好。 在非实时系统中，调度的主要目的是缩短系统平均响应时间，提高系统资源的利用率，或优化某一项指标；而实时系统中调度的目的则是要尽可能地保证每个任务满足他们的时间约束，及时对外部请求做出响应。 操作系统基本原理操作系统的主要功能是进行处理机与进程管理、存储管理、设备管理、文件管理和作业管理的工作。 进程管理进程是处理机管理中最基本的、最重要的概念。进程是系统并发执行的体现。为了动态地看待操作系统，则以进程作为独立运行的基本单位，以进程作为分配资源的基本单位，从进程的角度来研究操作系统。因此，处理机管理也被称为进程管理。处理机管理的功能就是组织和协调用户对处理机的争夺使用，把处理机分配给进程，对进程进行管理和控制，最大限度也发挥处理机的作用。 1.进程的概念用静态的观点看，操作系统是一组程序和表格的集合。用动态的观点看，操作系统是进程的动态和并发执行的。 在多道程序系统中，程序的运行环境发生了很大的变化。主要体现在： （1）资源共享。为了提高资源的利用率，计算机系统中的资源不再由一道程序专用，而是由多道程序共同使用。 （2）程序的并发执行或并行执行。逻辑上讲允许多道不同用户的程序并行运行；允许一个用户程序内部完成不同操作的程序段之间并行运行；允许操作系统内部不同的程序之间并行运行。物理上讲：内存储器中保存多个程序，I/O 设备被多个程序交替地共享使用；在多处理机系统的情形下，表现为多个程序在各自的处理机上运行，执行时间是重叠的。单处理机系统时，程序的执行表现为多道程序交替地在处理机上相互空插运行。 实际上，在多道程序系统中，程序的并行执行和资源共享之间是相辅相成的。一方面，只有允许程序并行执行，才可能存在资源共享的问题；另一方面，只有有效地实现资源共享，才可能使得程序并行执行。 3.关于挂起状态引入挂起状态的原因有： （1）对换的需要。 4．进程互斥与同步进程互斥 定义为：一组并发进程中一个或多个程序段，因共享某一共有资源而导致必须以一个不允许交叉执行的单位执行。也就是说互斥是要保证临界资源在某一时刻只被一个进程访问。 进程同步定义为：把异步环境下的一组并发进程因直接制约而互相发送消息而进行互相合作、互相等待，使得各进程按一定的速度执行的过程称为进程同步。也就是说进程之间是异步执行的，同步即是使各进程按一定的制约顺序和速度执行。 简单一点来说，互斥是资源的竞争关系，而同步是进程间的协作关系 由于资源共享与进程合作，并发执行的任务（进程）之间可能产生相互制约关系，这些制约关系可分为两类：竞争与协作。 并发进程之间的竞争关系为互斥，并发进程之间的协作关系体现为同步。 同步是因合作进程之间协调彼此的工作而控制自己的执行速度，即因相互合作，相互等待而产生的制约关系。 互斥是进程之间竞争临界资源而禁止两个以上的进程同时进入临界区所发生的制约关系。 一个任务要等待另一个任务发来消息，或建立某个条件后再向前执行，体现的制约关系是任务的同步。","categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"操作系统","slug":"系统架构师/操作系统","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"操作系统","slug":"操作系统","permalink":"https://alloceee.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"第3章：数据库系统","slug":"SystemArchitect/第3章：数据库系统","date":"2021-07-20T16:00:00.000Z","updated":"2021-08-03T05:14:10.696Z","comments":true,"path":"2021/07/21/SystemArchitect/第3章：数据库系统/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC3%E7%AB%A0%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"数据库模式与范式数据的规范化综合1NF、2NF和3NF、BCNF的内涵可概括如下： （1）非主属性完全函数依赖于码（2NF的要求） （2）非主属性不传递依赖于任何一个候选码（3NF的要求） （3）主属性对不含它的码完全函数依赖（BCNF的要求） （4）没有属性完全函数依赖于一组非主属性（BCNF的要求） 数据库设计逻辑结构设计1.基本E-R模型向关系模型转换2.数据模型优化（1）改善数据库的性能。 减少连接运算 减少关系大小和数据量（分表） 分表常用有水平分割与垂直分割。水平分割为分系建立关系，垂直分割为将常用数据与非常用数据分开。 （2）节约存储空间 缩小每个属性占用的空间 通常可以有两种方法：即用编码和用缩写符号表示属性，但这两种方法的缺点是失去了属性值含义的直观性。 采用假属性 主要任务逻辑结构设计阶段的主要任务是确定数据模型、将ER图转换成指定数据模型、确定完整性约束、确定用户视图。 超类实体：由多个实体中共有的属性组成 派生属性：由其他属性计算获得，用于存储计算结果值。","categories":[],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}]},{"title":"第6章：开发方法","slug":"SystemArchitect/第6章：开发方法","date":"2021-07-20T16:00:00.000Z","updated":"2021-07-21T06:29:31.768Z","comments":true,"path":"2021/07/21/SystemArchitect/第6章：开发方法/","link":"","permalink":"https://alloceee.github.io/2021/07/21/SystemArchitect/%E7%AC%AC6%E7%AB%A0%EF%BC%9A%E5%BC%80%E5%8F%91%E6%96%B9%E6%B3%95/","excerpt":"","text":"3.统一过程（Unified Process,UP）RUP软件开发生命周期是一个二维的软件开发模型，其中有9个核心工作流，分别为： 业务建模、需求 分析设计 实施 测试 部署 配置与变更管理 项目管理 环境 RUP把软件生命生存周期划分为多个循环，每个循环产生产品的一个新的版本，每个循环依次由4个连续的阶段组成，每个阶段完成确定的任务。这四个阶段分别为： 初始阶段：定义最终产品视图与业务模型，并确定系统范围。 细化阶段：设计及确定系统的体系结构，制定工作计划及资源要求。 构造阶段：构造产品并继续演进需求、体系构造、计划直至产品提交。 移交阶段：把产品提交给用户使用。 每个阶段都有一个或多人连续的迭代组成。迭代并不是重复得做相同的事，而是针对不同用例的细化和实现。每一个迭代都是一个完整的开发过程，它需要项目经理根据当前迭代所处的阶段以及上次迭代的结果，适当地对工作流中的行为进行裁剪。在每个阶段结束前都有一个里程碑评估该阶段的工作。如果未能通过该里程碑的评估，则决策者应该做出决定，是取消该项目还是继续该阶段的工作。 与其他软件开发过程相比，RUP具有自己的特点，即RUP是用例驱动的、以体系结构为中心的、迭代和增量的软件开发过程。","categories":[],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"}]},{"title":"","slug":"计算机网络/TCP协议面试10连问","date":"2021-07-20T05:09:35.090Z","updated":"2021-04-13T05:16:30.616Z","comments":true,"path":"2021/07/20/计算机网络/TCP协议面试10连问/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP%E5%8D%8F%E8%AE%AE%E9%9D%A2%E8%AF%9510%E8%BF%9E%E9%97%AE/","excerpt":"","text":"TCP和UDP的区别 面向连接 可靠性 基于字节流 001. 能不能说一说 TCP 和 UDP 的区别？首先概括一下基本的区别: TCP是一个面向连接的、可靠的、基于字节流的传输层协议。 而UDP是一个面向无连接的传输层协议。(就这么简单，其它TCP的特性也就没有了)。 具体来分析，和 UDP 相比，TCP 有三大核心特性: 面向连接。所谓的连接，指的是客户端和服务器的连接，在双方互相通信之前，TCP 需要三次握手建立连接，而 UDP 没有相应建立连接的过程。 可靠性。TCP 花了非常多的功夫保证连接的可靠，这个可靠性体现在哪些方面呢？一个是有状态，另一个是可控制。 TCP 会精准记录哪些数据发送了，哪些数据被对方接收了，哪些没有被接收到，而且保证数据包按序到达，不允许半点差错。这是有状态。 当意识到丢包了或者网络环境不佳，TCP 会根据具体情况调整自己的行为，控制自己的发送速度或者重发。这是可控制。 相应的，UDP 就是无状态, 不可控的。 面向字节流。UDP 的数据传输是基于数据报的，这是因为仅仅只是继承了 IP 层的特性，而 TCP 为了维护状态，将一个个 IP 包变成了字节流。 002: 说说 TCP 三次握手的过程？为什么是三次而不是两次、四次？确认双方的两样能力: 发送的能力和接收的能力。于是便会有下面的三次握手的过程: 从最开始双方都处于CLOSED状态。然后服务端开始监听某个端口，进入了LISTEN状态。 然后客户端主动发起连接，发送 SYN , 自己变成了SYN-SENT状态。 服务端接收到，返回SYN和ACK(对应客户端发来的SYN)，自己变成了SYN-REVD。 之后客户端再发送ACK给服务端，自己变成了ESTABLISHED状态；服务端收到ACK之后，也变成了ESTABLISHED状态。 另外需要提醒你注意的是，从图中可以看出，SYN 是需要消耗一个序列号的，下次发送对应的 ACK 序列号要加1，为什么呢？只需要记住一个规则: 凡是需要对端确认的，一定消耗TCP报文的序列号。 SYN 需要对端的确认， 而 ACK 并不需要，因此 SYN 消耗一个序列号而 ACK 不需要。 为什么不是两次？根本原因: 无法确认客户端的接收能力。 分析如下: 如果是两次，你现在发了 SYN 报文想握手，但是这个包滞留在了当前的网络中迟迟没有到达，TCP 以为这是丢了包，于是重传，两次握手建立好了连接。 看似没有问题，但是连接关闭后，如果这个滞留在网路中的包到达了服务端呢？这时候由于是两次握手，服务端只要接收到然后发送相应的数据包，就默认建立连接，但是现在客户端已经断开了。 看到问题的吧，这就带来了连接资源的浪费。 为什么不是四次？三次握手的目的是确认双方发送和接收的能力，那四次握手可以嘛？ 当然可以，100 次都可以。但为了解决问题，三次就足够了，再多用处就不大了。 三次握手过程中可以携带数据么？第三次握手的时候，可以携带。前两次握手不能携带数据。 如果前两次握手能够携带数据，那么一旦有人想攻击服务器，那么他只需要在第一次握手中的 SYN 报文中放大量数据，那么服务器势必会消耗更多的时间和内存空间去处理这些数据，增大了服务器被攻击的风险。 第三次握手的时候，客户端已经处于ESTABLISHED状态，并且已经能够确认服务器的接收、发送能力正常，这个时候相对安全了，可以携带数据。 同时打开会怎样？如果双方同时发 SYN报文，状态变化会是怎样的呢？ 这是一个可能会发生的情况。 状态变迁如下: 在发送方给接收方发SYN报文的同时，接收方也给发送方发SYN报文，两个人刚上了! 发完SYN，两者的状态都变为SYN-SENT。 在各自收到对方的SYN后，两者状态都变为SYN-REVD。 接着会回复对应的ACK + SYN，这个报文在对方接收之后，两者状态一起变为ESTABLISHED。 这就是同时打开情况下的状态变迁。 四次挥手断开连接一般挥手过程及状态变化 刚开始双方处于ESTABLISHED状态。 客户端要断开了，向服务器发送 FIN 报文，在 TCP 报文中的位置如下图: 发送后客户端变成了FIN-WAIT-1状态。注意, 这时候客户端同时也变成了half-close(半关闭)状态，即无法向服务端发送报文，只能接收。 服务端接收后向客户端确认，变成了CLOSED-WAIT状态。 客户端接收到了服务端的确认，变成了FIN-WAIT2状态。 随后，服务端向客户端发送FIN，自己进入LAST-ACK状态， 客户端收到服务端发来的FIN后，自己变成了TIME-WAIT状态，然后发送 ACK 给服务端。 注意了，这个时候，客户端需要等待足够长的时间，具体来说，是 2 个 MSL(Maximum Segment Lifetime，报文最大生存时间), 在这段时间内如果客户端没有收到服务端的重发请求，那么表示 ACK 成功到达，挥手结束，否则客户端重发 ACK。 等待2MSL的意义如果不等待会怎样？ 如果不等待，客户端直接跑路，当服务端还有很多数据包要给客户端发，且还在路上的时候，若客户端的端口此时刚好被新的应用占用，那么就接收到了无用数据包，造成数据包混乱。所以，最保险的做法是等服务器发来的数据包都死翘翘再启动新的应用。 那，照这样说一个 MSL 不就不够了吗，为什么要等待 2 MSL? 1 个 MSL 确保四次挥手中主动关闭方最后的 ACK 报文最终能达到对端 1 个 MSL 确保对端没有收到 ACK 重传的 FIN 报文可以到达 这就是等待 2MSL 的意义。 为什么是四次挥手而不是三次？因为服务端在接收到FIN, 往往不会立即返回FIN, 必须等到服务端所有的报文都发送完毕了，才能发FIN。因此先发一个ACK表示已经收到客户端的FIN，延迟一段时间才发FIN。这就造成了四次挥手。 如果是三次挥手会有什么问题？ 等于说服务端将ACK和FIN的发送合并为一次挥手，这个时候长时间的延迟可能会导致客户端误以为FIN没有到达客户端，从而让客户端不断的重发FIN。 同时发起挥手会怎样？（同时关闭会怎样？）如果客户端和服务端同时发送 FIN ，状态会如何变化？如图所示: SYN Flood攻击原理三次握手前，服务端的状态从CLOSED变为LISTEN, 同时在内部创建了两个队列：半连接队列和全连接队列，即SYN队列和ACCEPT队列。 半连接队列当客户端发送SYN到服务端，服务端收到以后回复ACK和SYN，状态由LISTEN变为SYN_RCVD，此时这个连接就被推入了SYN队列，也就是半连接队列。 全连接队列当客户端返回ACK, 服务端接收后，三次握手完成。这个时候连接等待被具体的应用取走，在被取走之前，它会被推入另外一个 TCP 维护的队列，也就是**全连接队列(Accept Queue)**。 SYN Flood 攻击原理SYN Flood 属于典型的 DoS/DDoS 攻击。其攻击的原理很简单，就是用客户端在短时间内伪造大量不存在的 IP 地址，并向服务端疯狂发送SYN。对于服务端而言，会产生两个危险的后果: 处理大量的SYN包并返回对应ACK, 势必有大量连接处于SYN_RCVD状态，从而占满整个半连接队列，无法处理正常的请求。 由于是不存在的 IP，服务端长时间收不到客户端的ACK，会导致服务端不断重发数据，直到耗尽服务端的资源。 如何应对 SYN Flood 攻击？ 增加 SYN 连接，也就是增加半连接队列的容量。 减少 SYN + ACK 重试次数，避免大量的超时重发。 利用 SYN Cookie 技术，在服务端接收到SYN后不立即分配连接资源，而是根据这个SYN计算出一个Cookie，连同第二次握手回复给客户端，在客户端回复ACK的时候带上这个Cookie值，服务端验证 Cookie 合法之后才分配连接资源。 剖析TCP报文首部字段（介绍一下 TCP 报文头部的字段）报文头部结构如下(单位为字节): 源端口、目标端口如何标识唯一标识一个连接？答案是 TCP 连接的四元组——源 IP、源端口、目标 IP 和目标端口。 那 TCP 报文怎么没有源 IP 和目标 IP 呢？这是因为在 IP 层就已经处理了 IP 。TCP 只需要记录两者的端口即可。 序列号即Sequence number, 指的是本报文段第一个字节的序列号。 从图中可以看出，序列号是一个长为 4 个字节，也就是 32 位的无符号整数，表示范围为 0 ~ 2^32 - 1。如果到达最大值了后就循环到0。 序列号在 TCP 通信的过程中有两个作用: 在 SYN 报文中交换彼此的初始序列号。 保证数据包按正确的顺序组装。 即Initial Sequence Number（初始序列号）,在三次握手的过程当中，双方会用过SYN报文来交换彼此的 ISN。 ISN 并不是一个固定的值，而是每 4 ms 加一，溢出则回到 0，这个算法使得猜测 ISN 变得很困难。那为什么要这么做？ 如果 ISN 被攻击者预测到，要知道源 IP 和源端口号都是很容易伪造的，当攻击者猜测 ISN 之后，直接伪造一个 RST 后，就可以强制连接关闭的，这是非常危险的。 而动态增长的 ISN 大大提高了猜测 ISN 的难度。 确认号即ACK(Acknowledgment number)。用来告知对方下一个期望接收的序列号，小于ACK的所有字节已经全部收到。 标记位常见的标记位有SYN,ACK,FIN,RST,PSH。 SYN 和 ACK 已经在上文说过，后三个解释如下: FIN：即 Finish，表示发送方准备断开连接。 RST：即 Reset，用来强制断开连接。 PSH：即 Push, 告知对方这些数据包收到后应该马上交给上层的应用，不能缓存。 窗口大小占用两个字节，也就是 16 位，但实际上是不够用的。因此 TCP 引入了窗口缩放的选项，作为窗口缩放的比例因子，这个比例因子的范围在 0 ~ 14，比例因子可以将窗口的值扩大为原来的 2 ^ n 次方。 校验和占用两个字节，防止传输过程中数据包有损坏，如果遇到校验和有差错的报文，TCP 直接丢弃之，等待重传。 可选项可选项的格式如下: 常用的可选项有以下几个: TimeStamp: TCP 时间戳，后面详细介绍。 MSS: 指的是 TCP 允许的从对方接收的最大报文段。 SACK: 选择确认选项。 Window Scale：窗口缩放选项。 TCP快速打开（TFP）原理第一节讲了 TCP 三次握手，可能有人会说，每次都三次握手好麻烦呀！能不能优化一点？ 可以啊。今天来说说这个优化后的 TCP 握手流程，也就是 TCP 快速打开(TCP Fast Open, 即TFO)的原理。 优化的过程是这样的，还记得我们说 SYN Flood 攻击时提到的 SYN Cookie 吗？这个 Cookie 可不是浏览器的Cookie, 用它同样可以实现 TFO。 TFO 流程首轮三次握手首先客户端发送SYN给服务端，服务端接收到。 注意哦！现在服务端不是立刻回复 SYN + ACK，而是通过计算得到一个SYN Cookie, 将这个Cookie放到 TCP 报文的 Fast Open选项中，然后才给客户端返回。 客户端拿到这个 Cookie 的值缓存下来。后面正常完成三次握手。 首轮三次握手就是这样的流程。而后面的三次握手就不一样啦！ 后面的三次握手在后面的三次握手中，客户端会将之前缓存的 Cookie、SYN 和HTTP请求(是的，你没看错)发送给服务端，服务端验证了 Cookie 的合法性，如果不合法直接丢弃；如果是合法的，那么就正常返回SYN + ACK。 重点来了，现在服务端能向客户端发 HTTP 响应了！这是最显著的改变，三次握手还没建立，仅仅验证了 Cookie 的合法性，就可以返回 HTTP 响应了。 当然，客户端的ACK还得正常传过来，不然怎么叫三次握手嘛。 流程如下: 注意: 客户端最后握手的 ACK 不一定要等到服务端的 HTTP 响应到达才发送，两个过程没有任何关系。 TFO 的优势TFO 的优势并不在与首轮三次握手，而在于后面的握手，在拿到客户端的 Cookie 并验证通过以后，可以直接返回 HTTP 响应，充分利用了1 个RTT(Round-Trip Time，往返时延)的时间提前进行数据传输，积累起来还是一个比较大的优势。 TCP时间戳的作用timestamp是 TCP 报文首部的一个可选项，一共占 10 个字节，格式如下: 1kind(1 字节) + length(1 字节) + info(8 个字节) 其中 kind = 8， length = 10， info 有两部分构成: timestamp和timestamp echo，各占 4 个字节。 那么这些字段都是干嘛的呢？它们用来解决那些问题？ 接下来我们就来一一梳理，TCP 的时间戳主要解决两大问题: 计算往返时延 RTT(Round-Trip Time) 防止序列号的回绕问题 计算往返时延RTT在没有时间戳的时候，计算 RTT 会遇到的问题如下图所示: 如果以第一次发包为开始时间的话，就会出现左图的问题，RTT 明显偏大，开始时间应该采用第二次的； 如果以第二次发包为开始时间的话，就会导致右图的问题，RTT 明显偏小，开始时间应该采用第一次发包的。 实际上无论开始时间以第一次发包还是第二次发包为准，都是不准确的。 那这个时候引入时间戳就很好的解决了这个问题。 比如现在 a 向 b 发送一个报文 s1，b 向 a 回复一个含 ACK 的报文 s2 那么： step 1: a 向 b 发送的时候，timestamp 中存放的内容就是 a 主机发送时的内核时刻 ta1。 step 2: b 向 a 回复 s2 报文的时候，timestamp 中存放的是 b 主机的时刻 tb, timestamp echo字段为从 s1 报文中解析出来的 ta1。 step 3: a 收到 b 的 s2 报文之后，此时 a 主机的内核时刻是 ta2, 而在 s2 报文中的 timestamp echo 选项中可以得到 ta1, 也就是 s2 对应的报文最初的发送时刻。然后直接采用 ta2 - ta1 就得到了 RTT 的值。 防止序列号回绕问题现在我们来模拟一下这个问题。 序列号的范围其实是在0 ~ 2 ^ 32 - 1, 为了方便演示，我们缩小一下这个区间，假设范围是 0 ~ 4，那么到达 4 的时候会回到 0。 第几次发包 发送字节 对应序列号 状态 1 0 ~ 1 0 ~ 1 成功接收 2 1 ~ 2 1 ~ 2 滞留在网络中 3 2 ~ 3 2 ~ 3 成功接收 4 3 ~ 4 3 ~ 4 成功接收 5 4 ~ 5 0 ~ 1 成功接收，序列号从0开始 6 5 ~ 6 1 ~ 2 ？？？ 假设在第 6 次的时候，之前还滞留在网路中的包回来了，那么就有两个序列号为1 ~ 2的数据包了，怎么区分谁是谁呢？这个时候就产生了序列号回绕的问题。 那么用 timestamp 就能很好地解决这个问题，因为每次发包的时候都是将发包机器当时的内核时间记录在报文中，那么两次发包序列号即使相同，时间戳也不可能相同，这样就能够区分开两个数据包了。 TCP超时重传算法TCP 具有超时重传机制，即间隔一段时间没有等到数据包的回复时，重传这个数据包。 那么这个重传间隔是如何来计算的呢？ 今天我们就来讨论一下这个问题。 这个重传间隔也叫做超时重传时间(Retransmission TimeOut, 简称RTO)，它的计算跟上一节提到的 RTT 密切相关。这里我们将介绍两种主要的方法，一个是经典方法，一个是标准方法。 经典方法引入了一个新的概念——SRTT(Smoothed round trip time，即平滑往返时间)，没产生一次新的 RTT. 就根据一定的算法对 SRTT 进行更新，具体而言，计算方式如下(SRTT 初始值为0): 1SRTT = (α * SRTT) + ((1 - α) * RTT) 其中，α 是平滑因子，建议值是0.8，范围是0.8 ~ 0.9。 拿到 SRTT，我们就可以计算 RTO 的值了: 1RTO = min(ubound, max(lbound, β * SRTT)) β 是加权因子，一般为1.3 ~ 2.0， lbound 是下界，ubound 是上界。 其实这个算法过程还是很简单的，但是也存在一定的局限，就是在 RTT 稳定的地方表现还可以，而在 RTT 变化较大的地方就不行了，因为平滑因子 α 的范围是0.8 ~ 0.9, RTT 对于 RTO 的影响太小。 标准方法(Jacobson/Karels算法)为了解决经典方法对于 RTT 变化不敏感的问题，后面又引出了标准方法，也叫Jacobson / Karels 算法。 一共有三步。 第一步: 计算SRTT，公式如下: 1SRTT = (1 - α) * SRTT + α * RTT 注意这个时候的 α跟经典方法中的α取值不一样了，建议值是1/8，也就是0.125。 第二步: 计算RTTVAR(round-trip time variation)这个中间变量。 1RTTVAR = (1 - β) * RTTVAR + β * (|RTT - SRTT|) β 建议值为 0.25。这个值是这个算法中出彩的地方，也就是说，它记录了最新的 RTT 与当前 SRTT 之间的差值，给我们在后续感知到 RTT 的变化提供了抓手。 第三步: 计算最终的RTO: 1RTO = µ * SRTT + ∂ * RTTVAR µ建议值取1, ∂建议值取4。 这个公式在 SRTT 的基础上加上了最新 RTT 与它的偏移，从而很好的感知了 RTT 的变化，这种算法下，RTO 与 RTT 变化的差值关系更加密切。 TCP流量控制对于发送端和接收端而言，TCP 需要把发送的数据放到发送缓存区, 将接收的数据放到接收缓存区。 而流量控制索要做的事情，就是在通过接收缓存区的大小，控制发送端的发送。如果对方的接收缓存区满了，就不能再继续发送了。 要具体理解流量控制，首先需要了解滑动窗口的概念。 TCP滑动窗口概念TCP 滑动窗口分为两种: 发送窗口和接收窗口。 发送窗口发送端的滑动窗口结构如下: 其中包含四大部分: 已发送且已确认 已发送但未确认 未发送但可以发送 未发送也不可以发送 其中有一些重要的概念，我标注在图中: 发送窗口就是图中被框住的范围。SND 即send, WND 即window, UNA 即unacknowledged, 表示未被确认，NXT 即next, 表示下一个发送的位置。 接收窗口接收端的窗口结构如下: REV 即 receive，NXT 表示下一个接收的位置，WND 表示接收窗口大小。 流量控制过程这里我们不用太复杂的例子，以一个最简单的来回来模拟一下流量控制的过程，方便大家理解。 首先双方三次握手，初始化各自的窗口大小，均为 200 个字节。 假如当前发送端给接收端发送 100 个字节，那么此时对于发送端而言，SND.NXT 当然要右移 100 个字节，也就是说当前的可用窗口减少了 100 个字节，这很好理解。 现在这 100 个到达了接收端，被放到接收端的缓冲队列中。不过此时由于大量负载的原因，接收端处理不了这么多字节，只能处理 40 个字节，剩下的 60 个字节被留在了缓冲队列中。 注意了，此时接收端的情况是处理能力不够用啦，你发送端给我少发点，所以此时接收端的接收窗口应该缩小，具体来说，缩小 60 个字节，由 200 个字节变成了 140 字节，因为缓冲队列还有 60 个字节没被应用拿走。 因此，接收端会在 ACK 的报文首部带上缩小后的滑动窗口 140 字节，发送端对应地调整发送窗口的大小为 140 个字节。 此时对于发送端而言，已经发送且确认的部分增加 40 字节，也就是 SND.UNA 右移 40 个字节，同时发送窗口缩小为 140 个字节。 这也就是流量控制的过程。尽管回合再多，整个控制的过程和原理是一样的。 TCP阻塞控制上一节所说的流量控制发生在发送端跟接收端之间，并没有考虑到整个网络环境的影响，如果说当前网络特别差，特别容易丢包，那么发送端就应该注意一些了。而这，也正是拥塞控制需要处理的问题。 对于拥塞控制来说，TCP 每条连接都需要维护两个核心状态: 拥塞窗口（Congestion Window，cwnd） 慢启动阈值（Slow Start Threshold，ssthresh） 涉及到的算法有这几个: 慢启动 拥塞避免 快速重传和快速恢复 接下来，我们就来一一拆解这些状态和算法。首先，从拥塞窗口说起。 拥塞窗口拥塞窗口（Congestion Window，cwnd）是指目前自己还能传输的数据量大小。 那么之前介绍了接收窗口的概念，两者有什么区别呢？ 接收窗口(rwnd)是接收端给的限制 拥塞窗口(cwnd)是发送端的限制 限制谁呢？ 限制的是发送窗口的大小。 有了这两个窗口，如何来计算发送窗口？ 1发送窗口大小 = min(rwnd, cwnd) 取两者的较小值。而拥塞控制，就是来控制cwnd的变化。 慢启动刚开始进入传输数据的时候，你是不知道现在的网路到底是稳定还是拥堵的，如果做的太激进，发包太急，那么疯狂丢包，造成雪崩式的网络灾难。 因此，拥塞控制首先就是要采用一种保守的算法来慢慢地适应整个网路，这种算法叫慢启动。运作过程如下: 首先，三次握手，双方宣告自己的接收窗口大小 双方初始化自己的拥塞窗口(cwnd)大小 在开始传输的一段时间，发送端每收到一个 ACK，拥塞窗口大小加 1，也就是说，每经过一个 RTT，cwnd 翻倍。如果说初始窗口为 10，那么第一轮 10 个报文传完且发送端收到 ACK 后，cwnd 变为 20，第二轮变为 40，第三轮变为 80，依次类推。 难道就这么无止境地翻倍下去？当然不可能。它的阈值叫做慢启动阈值，当 cwnd 到达这个阈值之后，好比踩了下刹车，别涨了那么快了，老铁，先 hold 住！ 在到达阈值后，如何来控制 cwnd 的大小呢？ 这就是拥塞避免做的事情了。 拥塞避免原来每收到一个 ACK，cwnd 加1，现在到达阈值了，cwnd 只能加这么一点: 1 / cwnd。那你仔细算算，一轮 RTT 下来，收到 cwnd 个 ACK, 那最后拥塞窗口的大小 cwnd 总共才增加 1。 也就是说，以前一个 RTT 下来，cwnd翻倍，现在cwnd只是增加 1 而已。 当然，慢启动和拥塞避免是一起作用的，是一体的。 快速重传和快速恢复快速重传在 TCP 传输的过程中，如果发生了丢包，即接收端发现数据段不是按序到达的时候，接收端的处理是重复发送之前的 ACK。 比如第 5 个包丢了，即使第 6、7 个包到达的接收端，接收端也一律返回第 4 个包的 ACK。当发送端收到 3 个重复的 ACK 时，意识到丢包了，于是马上进行重传，不用等到一个 RTO 的时间到了才重传。 这就是快速重传，它解决的是是否需要重传的问题。 选择性重传那你可能会问了，既然要重传，那么只重传第 5 个包还是第5、6、7 个包都重传呢？ 当然第 6、7 个都已经到达了，TCP 的设计者也不傻，已经传过去干嘛还要传？干脆记录一下哪些包到了，哪些没到，针对性地重传。 在收到发送端的报文后，接收端回复一个 ACK 报文，那么在这个报文首部的可选项中，就可以加上SACK这个属性，通过left edge和right edge告知发送端已经收到了哪些区间的数据报。因此，即使第 5 个包丢包了，当收到第 6、7 个包之后，接收端依然会告诉发送端，这两个包到了。剩下第 5 个包没到，就重传这个包。这个过程也叫做选择性重传(SACK，Selective Acknowledgment)，它解决的是如何重传的问题。 快速恢复当然，发送端收到三次重复 ACK 之后，发现丢包，觉得现在的网络已经有些拥塞了，自己会进入快速恢复阶段。 在这个阶段，发送端如下改变： 拥塞阈值降低为 cwnd 的一半 cwnd 的大小变为拥塞阈值 cwnd 线性增加 以上就是 TCP 拥塞控制的经典算法: 慢启动、拥塞避免、快速重传和快速恢复。 能不能说说 Nagle 算法和延迟确认？Nagle 算法试想一个场景，发送端不停地给接收端发很小的包，一次只发 1 个字节，那么发 1 千个字节需要发 1000 次。这种频繁的发送是存在问题的，不光是传输的时延消耗，发送和确认本身也是需要耗时的，频繁的发送接收带来了巨大的时延。 而避免小包的频繁发送，这就是 Nagle 算法要做的事情。 具体来说，Nagle 算法的规则如下: 当第一次发送数据时不用等待，就算是 1byte 的小包也立即发送 后面发送满足下面条件之一就可以发了: 数据包大小达到最大段大小(Max Segment Size, 即 MSS) 之前所有包的 ACK 都已接收到 延迟确认试想这样一个场景，当我收到了发送端的一个包，然后在极短的时间内又接收到了第二个包，那我是一个个地回复，还是稍微等一下，把两个包的 ACK 合并后一起回复呢？ 延迟确认(delayed ack)所做的事情，就是后者，稍稍延迟，然后合并 ACK，最后才回复给发送端。TCP 要求这个延迟的时延必须小于500ms，一般操作系统实现都不会超过200ms。 不过需要主要的是，有一些场景是不能延迟确认的，收到了就要马上回复: 接收到了大于一个 frame 的报文，且需要调整窗口大小 TCP 处于 quickack 模式（通过tcp_in_quickack_mode设置） 发现了乱序包 两者一起使用会怎样？前者意味着延迟发，后者意味着延迟接收，会造成更大的延迟，产生性能问题。 如何理解 TCP 的 keep-alive？大家都听说过 http 的keep-alive, 不过 TCP 层面也是有keep-alive机制，而且跟应用层不太一样。 试想一个场景，当有一方因为网络故障或者宕机导致连接失效，由于 TCP 并不是一个轮询的协议，在下一个数据包到达之前，对端对连接失效的情况是一无所知的。 这个时候就出现了 keep-alive, 它的作用就是探测对端的连接有没有失效。 在 Linux 下，可以这样查看相关的配置: 1sudo sysctl -a | grep keepalive// 每隔 7200 s 检测一次net.ipv4.tcp_keepalive_time = 7200// 一次最多重传 9 个包net.ipv4.tcp_keepalive_probes = 9// 每个包的间隔重传间隔 75 snet.ipv4.tcp_keepalive_intvl = 75 不过，现状是大部分的应用并没有默认开启 TCP 的keep-alive选项，为什么？ 站在应用的角度: 7200s 也就是两个小时检测一次，时间太长 时间再短一些，也难以体现其设计的初衷, 即检测长时间的死连接 因此是一个比较尴尬的设计。 基于丢包的阻塞控制点产生的问题Google的BBR阻塞控制算法","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件","date":"2021-07-20T05:09:35.077Z","updated":"2020-05-11T07:00:28.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-基础","date":"2021-07-20T05:09:35.072Z","updated":"2020-05-11T07:01:06.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-基础/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-%E5%9F%BA%E7%A1%80/","excerpt":"","text":"介绍一下消息队列MQ的使用场景1.通过异步处理提高系统性能在不适用消息队列 服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送到消息队列之后立即返回，再由消息队列的消费者进程从消息队列中获得数据，异步写入数据库，由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 消息队列具有很好的削峰功能–即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀，促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。 2.降低系统耦合性使用消息队列的好处 消息中间件使用场景介绍消息中间件(ActiveMQ、RabbitMQ、RocketMQ、Kafka)简介及对比消息中间件的编年史 其他问题 引入消息队列之后如何确保高可用性 如何保证消息不被重复消费呢 如何保证消息的高可靠性传输 我该怎么保证从消息队列里拿到的数据按顺序执行 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该如何处理？有几百万消息持续积压几小时，说说该如何解决？ 如果让你来开发一个消息队列中间件，你会怎么设计架构","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-RocketMQ","date":"2021-07-20T05:09:35.070Z","updated":"2020-05-11T07:00:32.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-RocketMQ/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-RocketMQ/","excerpt":"","text":"RocketMQ","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-RabbitMQ","date":"2021-07-20T05:09:35.068Z","updated":"2020-05-11T07:01:54.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-RabbitMQ/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-RabbitMQ/","excerpt":"","text":"RabbitMQRabbitMq AMQP规范和RabbitMQ基本概念 要素 生产者、消费者、消息 信道 交换器、队列、绑定、路由键 消息的确认 交换器类型 Direct Fanout Topic 虚拟主机 RabbitMQ在Windows下安装和运行 原生Java客户端使用 消息发布时的权衡 失败通知 发布者确认 事务 备用交换器 消息消费时的权衡 消息的获得方式 QoS预取模式 可靠性和性能的权衡 消息的拒绝 消息的拒绝方式 死信交换器 控制队列 临时队列 永久队列 队列级别消息过期 消息的属性 属性列表 消息的持久化 与Spring集成 Xml配置方式 SpringBoot 实战：应用解耦 安装配置 下载安装和日常管理 web监控平台 集群化与镜像队列 简介MQ全称为Message Queue，消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过读写入队列的消息（针对应用程序的数据）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信。队列的使用除去了接收和发送应用程序同时执行的请求。其中较为成熟的MQ产品有IBM WEBSPHERE MQ等等。 RabbitMQ是一个在AMQP基础上完成的，可复用的企业消息系统。他遵循Mozilla Public License开源协议。 AMQP，即Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计，基于此协议的客户端和消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。Erlang中的实现由RabbitMQ等。 安装先下载erlang，再安装rabbitmq erlang下载地址 rabbitmq下载地址 本机查看管理页面 http://127.0.0.1:15672/ 端口号：5672 用户角色1、超级管理员(administrator)可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。2、监控者(monitoring)可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)3、策略制定者(policymaker)可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。4、普通管理者(management)仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。5、其他无法登陆管理控制台，通常就是普通的生产者和消费者。 1.背景RabbitMQ是一个由erlang开发的AMQP(Advanved Message Queue)的开源实现。 2.应用场景2.1异步处理场景说明：用户注册后，需要发注册邮件和注册短信,传统的做法有两种1.串行的方式;2.并行的方式(1)串行方式:将注册信息写入数据库后,发送注册邮件,再发送注册短信,以上三个任务全部完成后才返回给客户端。 这有一个问题是,邮件,短信并不是必须的,它只是一个通知,而这种做法让客户端等待没有必要等待的东西. (2)并行方式:将注册信息写入数据库后,发送邮件的同时,发送短信,以上三个任务完成后,返回给客户端,并行的方式能提高处理的时间。 假设三个业务节点分别使用50ms,串行方式使用时间150ms,并行使用时间100ms。虽然并性已经提高的处理时间,但是,前面说过,邮件和短信对我正常的使用网站没有任何影响，客户端没有必要等着其发送完成才显示注册成功,英爱是写入数据库后就返回.(3)消息队列引入消息队列后，把发送邮件,短信不是必须的业务逻辑异步处理 由此可以看出,引入消息队列后，用户的响应时间就等于写入数据库的时间+写入消息队列的时间(可以忽略不计),引入消息队列后处理后,响应时间是串行的3倍,是并行的2倍。 2.2 应用解耦场景：双11是购物狂节,用户下单后,订单系统需要通知库存系统,传统的做法就是订单系统调用库存系统的接口. 这种做法有一个缺点: 当库存系统出现故障时,订单就会失败。(这样马云将少赚好多好多钱^ ^)订单系统和库存系统高耦合.引入消息队列 订单系统:用户下单后,订单系统完成持久化处理,将消息写入消息队列,返回用户订单下单成功。 库存系统:订阅下单的消息,获取下单消息,进行库操作。就算库存系统出现故障,消息队列也能保证消息的可靠投递,不会导致消息丢失(马云这下高兴了).流量削峰流量削峰一般在秒杀活动中应用广泛场景:秒杀活动，一般会因为流量过大，导致应用挂掉,为了解决这个问题，一般在应用前端加入消息队列。作用:1.可以控制活动人数，超过此一定阀值的订单直接丢弃(我为什么秒杀一次都没有成功过呢^^)2.可以缓解短时间的高流量压垮应用(应用程序按自己的最大处理能力获取订单) 1.用户的请求,服务器收到之后,首先写入消息队列,加入消息队列长度超过最大值,则直接抛弃用户请求或跳转到错误页面.2.秒杀业务根据消息队列中的请求信息，再做后续处理. 3.系统架构 几个概念说明:Broker:它提供一种传输服务,它的角色就是维护一条从生产者到消费者的路线，保证数据能按照指定的方式进行传输,Exchange：消息交换机,它指定消息按什么规则,路由到哪个队列。Queue:消息的载体,每个消息都会被投到一个或多个队列。Binding:绑定，它的作用就是把exchange和queue按照路由规则绑定起来.Routing Key:路由关键字,exchange根据这个关键字进行消息投递。vhost:虚拟主机,一个broker里可以有多个vhost，用作不同用户的权限分离。Producer:消息生产者,就是投递消息的程序.Consumer:消息消费者,就是接受消息的程序.Channel:消息通道,在客户端的每个连接里,可建立多个channel. 4.任务分发机制4.1Round-robin dispathching循环分发RabbbitMQ的分发机制非常适合扩展,而且它是专门为并发程序设计的,如果现在load加重,那么只需要创建更多的Consumer来进行任务处理。 4.2Message acknowledgment消息确认为了保证数据不被丢失,RabbitMQ支持消息确认机制,为了保证数据能被正确处理而不仅仅是被Consumer收到,那么我们不能采用no-ack，而应该是在处理完数据之后发送ack.在处理完数据之后发送ack,就是告诉RabbitMQ数据已经被接收,处理完成,RabbitMQ可以安全的删除它了.如果Consumer退出了但是没有发送ack,那么RabbitMQ就会把这个Message发送到下一个Consumer，这样就保证在Consumer异常退出情况下数据也不会丢失.RabbitMQ它没有用到超时机制.RabbitMQ仅仅通过Consumer的连接中断来确认该Message并没有正确处理，也就是说RabbitMQ给了Consumer足够长的时间做数据处理。如果忘记ack,那么当Consumer退出时,Mesage会重新分发,然后RabbitMQ会占用越来越多的内存. 5.Message durability消息持久化要持久化队列queue的持久化需要在声明时指定durable=True;这里要注意,队列的名字一定要是Broker中不存在的,不然不能改变此队列的任何属性.队列和交换机有一个创建时候指定的标志durable,durable的唯一含义就是具有这个标志的队列和交换机会在重启之后重新建立,它不表示说在队列中的消息会在重启后恢复消息持久化包括3部分 exchange持久化,在声明时指定durable =&gt; true hannel.ExchangeDeclare(ExchangeName, “direct”, durable: true, autoDelete: false, arguments: null);//声明消息队列，且为可持久化的12.queue持久化,在声明时指定durable =&gt; true channel.QueueDeclare(QueueName, durable: true, exclusive: false, autoDelete: false, arguments: null);//声明消息队列，且为可持久化的13.消息持久化,在投递时指定delivery_mode =&gt; 2(1是非持久化). channel.basicPublish(“”, queueName, MessageProperties.PERSISTENT_TEXT_PLAIN, msg.getBytes());1如果exchange和queue都是持久化的,那么它们之间的binding也是持久化的,如果exchange和queue两者之间有一个持久化，一个非持久化,则不允许建立绑定.注意：一旦创建了队列和交换机,就不能修改其标志了,例如,创建了一个non-durable的队列,然后想把它改变成durable的,唯一的办法就是删除这个队列然后重现创建。 6.Fair dispath 公平分发你可能也注意到了,分发机制不是那么优雅,默认状态下,RabbitMQ将第n个Message分发给第n个Consumer。n是取余后的,它不管Consumer是否还有unacked Message，只是按照这个默认的机制进行分发.那么如果有个Consumer工作比较重,那么就会导致有的Consumer基本没事可做,有的Consumer却毫无休息的机会,那么,Rabbit是如何处理这种问题呢? 通过basic.qos方法设置prefetch_count=1，这样RabbitMQ就会使得每个Consumer在同一个时间点最多处理一个Message，换句话说,在接收到该Consumer的ack前,它不会将新的Message分发给它 channel.basic_qos(prefetch_count=1)1注意，这种方法可能会导致queue满。当然，这种情况下你可能需要添加更多的Consumer，或者创建更多的virtualHost来细化你的设计。 7.分发到多个Consumer7.1Exchange先来温习以下交换机路由的几种类型:Direct Exchange:直接匹配,通过Exchange名称+RountingKey来发送与接收消息.Fanout Exchange:广播订阅,向所有的消费者发布消息,但是只有消费者将队列绑定到该路由器才能收到消息,忽略Routing Key.Topic Exchange：主题匹配订阅,这里的主题指的是RoutingKey,RoutingKey可以采用通配符,如:*或#，RoutingKey命名采用.来分隔多个词,只有消息这将队列绑定到该路由器且指定RoutingKey符合匹配规则时才能收到消息;Headers Exchange:消息头订阅,消息发布前,为消息定义一个或多个键值对的消息头,然后消费者接收消息同时需要定义类似的键值对请求头:(如:x-mactch=all或者x_match=any)，只有请求头与消息头匹配,才能接收消息,忽略RoutingKey.默认的exchange:如果用空字符串去声明一个exchange，那么系统就会使用”amq.direct”这个exchange，我们创建一个queue时,默认的都会有一个和新建queue同名的routingKey绑定到这个默认的exchange上去 channel.BasicPublish(“”, “TaskQueue”, properties, bytes);1因为在第一个参数选择了默认的exchange，而我们申明的队列叫TaskQueue，所以默认的，它在新建一个也叫TaskQueue的routingKey，并绑定在默认的exchange上，导致了我们可以在第二个参数routingKey中写TaskQueue，这样它就会找到定义的同名的queue，并把消息放进去。如果有两个接收程序都是用了同一个的queue和相同的routingKey去绑定direct exchange的话，分发的行为是负载均衡的，也就是说第一个是程序1收到，第二个是程序2收到，以此类推。如果有两个接收程序用了各自的queue，但使用相同的routingKey去绑定direct exchange的话，分发的行为是复制的，也就是说每个程序都会收到这个消息的副本。行为相当于fanout类型的exchange。下面详细来说: 7.2 Bindings 绑定绑定其实就是关联了exchange和queue，或者这么说:queue对exchange的内容感兴趣,exchange要把它的Message deliver到queue。 7.3Direct exchangeDriect exchange的路由算法非常简单:通过bindingkey的完全匹配，可以用下图来说明. Exchange和两个队列绑定在一起,Q1的bindingkey是orange，Q2的binding key是black和green.当Producer publish key是orange时,exchange会把它放到Q1上,如果是black或green就会到Q2上,其余的Message被丢弃. 7.4 Multiple bindings多个queue绑定同一个key也是可以的,对于下图的例子,Q1和Q2都绑定了black,对于routing key是black的Message，会被deliver到Q1和Q2，其余的Message都会被丢弃. 7.5 Topic exchange对于Message的routing_key是有限制的，不能使任意的。格式是以点号“.”分割的字符表。比如：”stock.usd.nyse”, “nyse.vmw”, “quick.orange.rabbit”。你可以放任意的key在routing_key中，当然最长不能超过255 bytes。对于routing_key，有两个特殊字符 *(星号)代表任意一个单词#(hash)0个或多个单词 Producer发送消息时需要设置routing_key，routing_key包含三个单词和连个点号o,第一个key描述了celerity(灵巧),第二个是color(色彩),第三个是物种:在这里我们创建了两个绑定： Q1 的binding key 是”.orange.“； Q2 是 “..rabbit” 和 “lazy.#”： Q1感兴趣所有orange颜色的动物Q2感兴趣所有rabbits和所有的lazy的.例子:rounting_key 为 “quick.orange.rabbit”将会发送到Q1和Q2中rounting_key 为”lazy.orange.rabbit.hujj.ddd”会被投递到Q2中,#匹配0个或多个单词。8.消息序列化RabbitMQ使用ProtoBuf序列化消息,它可作为RabbitMQ的Message的数据格式进行传输,由于是结构化的数据,这样就极大的方便了Consumer的数据高效处理,当然也可以使用XML，与XML相比,ProtoBuf有以下优势:1.简单2.size小了3-10倍3.速度快了20-100倍4.易于编程6.减少了语义的歧义.，ProtoBuf具有速度和空间的优势，使得它现在应用非常广泛 SpringBoot整合https://blog.csdn.net/qq_38455201/article/details/80308771","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-MetaQ","date":"2021-07-20T05:09:35.067Z","updated":"2020-05-11T07:00:30.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-MetaQ/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-MetaQ/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-Kafka","date":"2021-07-20T05:09:35.064Z","updated":"2020-05-11T07:00:48.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-Kafka/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-Kafka/","excerpt":"","text":"KafkaKafka Kafka入门 Kafka中的基本概念 为什么选择Kafka Kafka的安装和配置参数 Kafka的生产者和消费者 消息的发送和接收 生产者和消费者的配置 消费者群组和再均衡 消费者中的提交和偏移量 序列化和反序列化 深入理解Kafka 控制器和复制 请求处理流程 物理存储原理 保证Kafka的可靠数据传递 数据管道和流式处理入门 数据管道基本概念 流式处理基本概念和设计模式","categories":[],"tags":[]},{"title":"","slug":"消息中间件/消息中间件-ActiveMQ","date":"2021-07-20T05:09:35.063Z","updated":"2020-05-11T07:00:50.000Z","comments":true,"path":"2021/07/20/消息中间件/消息中间件-ActiveMQ/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-ActiveMQ/","excerpt":"","text":"ActiveMq JMS规范 什么是JMS（Java Messaging Service）规范？ 包含要素 消息类型 P2P模型 Topic(PUB\\SUB)模型 ActiveMQ使用 安装和部署 原生ActiveMQ的API编程 与Spring集成 Xml配置方式 SpringBoot Request-Response模式 实战：用户注册的异步处理 ActiveMQ高级特性和用法 嵌入式MQ 消息存储的持久化机制 消息持久订阅 消息的可靠性 通配符式分层订阅 死信队列DLQ(Dead Letter Queue) 镜像队列 虚拟主题 组合Destinations 实战：限时订单 企业级高可用集群部署方案 Shared File System DB Replicated LevelDB Store Broker-Cluster","categories":[],"tags":[]},{"title":"","slug":"数据库/Redis/Redis计数器解决并发问题","date":"2021-07-20T05:09:35.036Z","updated":"2020-08-06T03:11:18.000Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis计数器解决并发问题/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis%E8%AE%A1%E6%95%B0%E5%99%A8%E8%A7%A3%E5%86%B3%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/","excerpt":"","text":"Redis解决并发问题用setnx替代set命令初始化计数器，这确保了一旦A初始化计数器成功，B就不会再去初始化计数器。 12345678910111213141516171819202122/** * 获取用户当前的排序 * @return int */ protected function getCurrentOrder() &#123; $redis = new R(&quot;game&quot;); $order = $redis-&gt;get(self::$hd_ename); $number = point_model::getPointLogByTips(self::$hd_ename, &#x27;luckNumber&#x27;); if(!$order)&#123; if($number)&#123; $number = $number[count($number) - 1][&#x27;point&#x27;]; &#125;else&#123; $number = 0; &#125; $redis-&gt;setex(self::$hd_ename,120,$number); &#125;else&#123; $number = $redis-&gt;incr(self::$hd_ename); &#125; return $number; &#125; 我们都或多或少遇到过并发问题。家人因为看电视抢遥控器，这就是一种并发；两个孩子争着玩同一个玩具，这也是并发。在每一次“双11”购物节狂欢的背后，都有一群程序员在严阵以待，这不是一位数的并发，而是成千上万级别的并发。 说了什么是并发，接下来将向大家演示如何用Redis处理一个典型的并发问题。我们选择最常见的商品抢购场景，假定我们有100件商品，参与抢购的用户有成千上万，如何确保我们的商品不被多抢了？ 聪明的你应该想到了，可以用计数器来控制，每卖出一件商品，计数器加1，当计数器到达100时，我们的商品就卖完了。程序的工作流程如下图。 看起来很完美，但需要通过高并发场景的检验。我们假定有A、B两个进程同时在运行这段程序。 问题1初始化set计数器：A、B都发现计数器尚未初始化，在A执行“计数器加1”后，B去set计数器，此时计数器的值比正确值少1。（为什么时间差那么大？这在高并发场景中是完全可能存在的） 问题2计数器加1：A、B都读到计数器的值为99，不满足&gt;=100，两者都抢到了商品，但最终卖掉了101件，显然超卖了。 上面的流程存在两个问题，我们需要对程序流程做一点改进，新的流程如下图。 改进1用setnx替代set命令初始化计数器，这确保了一旦A初始化计数器成功，B就不会再去初始化计数器。 改进2先对计数器加1，再判断计数器是否&gt;100，如果是，说明超卖了。这确保了即使A、B同时读到计数器的值为99，都去对计数器加1，两者至少有一个得到的结果&gt;100，不会超卖。 从以上的内容我们学习到，如何用Redis处理一个常见的并发场景，这背后还有更多的技术细节值得我们深入了解，期待在下一篇文章中与大家共同学习。","categories":[],"tags":[]},{"title":"Redis的skiplist","slug":"数据库/Redis/Redis-skiplist","date":"2021-07-20T05:09:35.033Z","updated":"2021-07-27T01:42:38.389Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis-skiplist/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis-skiplist/","excerpt":"","text":"跳表的实现从排行榜切入懂行的老哥一看这个小标题，就知道我要以排行榜作为切入点，去讲 Redis 的 zset 了。 是的，经典面试题，请实现一个排行榜，大部分情况下就是在考验你知不知道 Redis 的 zset 结构，和其对应的操作。 当然了，排行榜我们也可以基于其他的解决方案。比如 mysql。 我曾经就基于 mysql 做过排行榜，一条 sql 就能搞定。但是仅限于数据量比较少，性能要求不高的场景（我当时只有 11 支队伍做排行榜，一分钟刷新一次排行榜）。 对于这种经典的面试八股文，网上一找一大把，所以本文就不去做相关解析了。 说好的只是一个切入点。 如果你不知道具体怎么实现，或者根本就不知道这题在问啥，那一定记得看完本文后要去看看相关的文章。最好自己实操一下。 相信我，八股文，得背，这题会考。 zset的内部编码众所周知，Redis 对外提供了五种基本数据类型。但是每一种基本类型的内部编码却是另外一番风景： 其中 list 数据结构，在 Redis 3.2 版本中还提供了 quicklist 的内部编码。不是本文重点，我提一嘴就行，有兴趣的朋友自己去了解一下。 本文主要探讨的是上图中的 zset 数据结构。 zset 的内部编码有两种：ziplist 和 skiplist。 其实你也别觉得这个东西有多神奇。因为对于这种对外一套，对内又是一套的“双标党”其实你已经很熟悉了。 它就是 JDK 的一个集合类，来朋友们，大胆的喊出它的名字：HashMap。 HashMap 除了基础的数组结构之外，还有另外两个数据结构：一个链表，一个红黑树。 这样一联想是不是就觉得也不过如此，心里至少有个底了。 当链表长度大于 8 且数组长度大于 64 的时候， HashMap 中的链表会转红黑数。 对于 zset 也是一样的，一定会有条件触发其内部编码 ziplist 和 skiplist 之间的变化？ 这个问题的答案就藏在 redis.conf 文件中，其中有两个配置： 上图中配置的含义是，当有序集合的元素个数小于 zset-max-ziplist-entries 配置的值，且每个元素的值的长度都小于 zset-max-ziplist-value 配置的值时，zset 的内部编码是 ziplist。 反之则用 skiplist。 理论铺垫上了，接下我给大家演示一波。 首先，我们给 memberscore 这个有序集合的 key 设置两个值，然后看看其内部编码： 此时有序集合的元素个数是 2，可以看到，内部编码采用的是 ziplist 的结构。 为了大家方便理解这个储存，我给大家画个图： 然后我们需要触发内部编码从 ziplist 到 skiplist 的变化。 先验证 zset-max-ziplist-value 配置，往 memberscore 元素中塞入一个长度大于 64字节（zset-max-ziplist-value默认配置）的值： 这个时候 key 为 memberscore 的有序集合中有 3 个元素了，其中有一个元素的值特别长，超过了 64 字节。 此时的内部编码采用的是 skiplist。 接下来，我们往 zset 中多塞点值，验证一下元素个数大于 zset-max-ziplist-entries 的情况。 我们搞个新的 key，取值为 whytestkey。 首先，往 whytestkey 中塞两个元素，这是其内部编码还是 ziplist： 那么问题来了，从配置来看 zset-max-ziplist-entries 128。 这个 128 是等于呢还是大于呢？ 没关系，我也不知道，试一下就行了。 现在已经有两个元素了，再追加 126 个元素，看看： 通过实验我们发现，当 whytestkey 中的元素个数是 128 的时候，其内部编码还是 ziplist。 那么触发其从 ziplist 转变为 skiplist 的条件是 元素个数大于 128，我们再加入一个试一试： 果然，内部编码从 ziplist 转变为了 skiplist。 理论验证完毕，zset 确实是有两幅面孔。 本文主要探讨 skiplist 这个内部编码。 它就是标题说的：基于运气的数据结构。 什么是 skiplist？这个结构是一个叫做 William Pugh 的哥们在 1990 年发布的一篇叫做《Skip Lists: A Probabilistic Alternative to Balanced Trees》的论文中提出的。 1论文地址：ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf 摘要里面说：跳表是一种可以用来代替平衡树的数据结构，跳表使用概率平衡而不是严格的平衡，因此，与平衡树相比，跳表中插入和删除的算法要简单得多，并且速度要快得多。 论文里面，在对跳表算法进行详细描述的地方他是这样说的： 首先火男大佬说，对于一个有序的链表来说，如果我们需要查找某个元素，必须对链表进行遍历。比如他给的示意图的 a 部分。 我单独截取一下： 这个时候，大家还能跟上，对吧。链表查找，逐个遍历是基本操作。 那么，如果这个链表是有序的，我们可以搞一个指针，这个指针指向的是该节点的下下个节点。 意思就是往上抽离一部分节点。 怎么抽离呢，每隔一个节点，就抽一个出来，和上面的 a 示意图比起来，变化就是这样的： 抽离出来有什么好处呢？ 假设我们要查询的节点是 25 。 当就是普通有序链表的时候，我们从头节点开始遍历，需要遍历的路径是： head -&gt; 3 -&gt; 6 -&gt; 7 -&gt; 9 -&gt; 12 -&gt; 17 -&gt; 19 -&gt; 21 -&gt; 25 需要 9 次查询才能找到 25 。 但是当结构稍微一变，变成了 b 示意图的样子之后，查询路径就是： 第二层的 head -&gt; 6 -&gt; 9 -&gt; 17 -&gt; 21 -&gt; 25。 5 次查询就找到了 25 。 这个情况下我们找到指定的元素，不会超过 (n/2)+1 个节点： 那么这个时候有个小问题就来了：怎么从 21 直接到 25 的呢？ 看论文中的图片，稍微有一点不容易明白。 所以，我给大家重新画个示意图： 看到了吗？“多了”一个向下的指针。其实也不是多了，只是论文里面没有明示而已。 所以，查询 25 的路径是这样的，空心箭头指示的方向： 在 21 到 26 节点之间，往下了一层，逻辑也很简单。 21 节点有一个右指针指向 26，先判断右指针的值大于查询的值了。 于是下指针就起到作用了，往下一层，再继续进行右指针的判断。 其实每个节点的判断逻辑都是这样，只是前面的判断结果是进行走右指针。 按照这个往上抽节点的思想，假设我们抽到第四层，也就是论文中的这个示意图： 我们查询 25 的时候，只需要经过 2 次。 第一步就直接跳过了 21 之前的所有元素。 怎么样，爽不爽？ 但是，它是有缺陷的。 火男的论文里面是这样说的： This data structure could be used for fast searching, but insertion and deletion would be impractical. 查询确实飞快。但是对于插入和删除 would be impractical。 impractical 是什么意思？ 你看，又学一个四级单词。 对于插入和删除几乎是难以实现的。 你想啊，上面那个最底层的有序链表，我一开始就拿出来给你了。 然后我就说基于这个有序链表每隔一个节点抽离到上一层去，再构建一个链表。那么这样上下层节点比例应该是 2:1。巴拉巴拉的….. 但是实际情况应该是我们最开始的时候连这个有序链表都没有，需要自己去创建的。 就假设要在现有的这个跳表结构中插入一个节点，毋庸置疑，肯定是要插入到最底层的有序链表中的。 但是你破坏了上下层 2:1 的比例了呀？ 怎么办，一层层的调整呗。 可以，但是请你考虑一下编码实现起来的难度和对应的时间复杂度？ 要这样搞，直接就是一波劝退。 这就受不了了？ 我还没说删除的事呢。 那怎么办？ 看看论文里面怎么说到： 首先我们关注一下第一段划红线的地方。 火男写到：50% 的节点在第一层，25% 的节点在第二层， 12.5% 的节点在第三层。 你以为他在给你说什么？ 他要表达的意思除了每一层节点的个数之外，还说明了层级： 没有第 0 层，至少论文里面没有说有第 0 层。 如果你非要说最下面那个有全部节点的有序链表叫做第 0 层，我觉得也可以。但是，我觉得叫它基础链表更加合适一点。 然后我再看第二段划线的地方。 火男提到了一个关键词：randomly，意思是随机。 说出来你可能不信，但是跳表是用随机的方式解决上面提出的插入（删除）之后调整结构的问题。 怎么随机呢？抛硬币。 是的，没有骗你，真的是“抛硬币”。 跳表中的“硬币”当跳表中插入一个元素的时候，火男表示我们上下层之间可以不严格遵循 1:2 的节点关系。 如果插入的这个元素需要建立索引，那么把索引建立在第几层，都是由抛硬币决定的。 或者说：由抛硬币的概率决定的。 我问你，一个硬币抛出去之后，是正面的概率有多大？ 是不是 50%？ 如果我们把这个概率记为 p，那么 50%，即 p=1/2。 上面我们提到的概率，到底是怎么用的呢？ 火男的论文中有一小节是这样的写的： 随机选择一个层级。他说我们假设概率 p=1/2，然后叫我们看图 5。 图 5 是这样的： 非常关键的一张图啊。 短短几行代码，描述的是如何选择层级的随机算法。 首先定义初始层级为 1（lvl := 1）。 然后有一行注释：random() that returns a random value in [0…1) random() 返回一个 [0…1) 之间的随机值。 接下来一个 while…do 循环。 循环条件两个。 第一个：random() &lt; p。由于 p = 1/2，那么该条件成立的概率也是 1/2。 如果每随机一次，满足 random() &lt; p，那么层级加一。 那假设你运气爆棚，接连一百次随机出来的数都是小于 p 的怎么办？岂不是层级也到 100 层了？ 第二个条件 lvl &lt; MaxLevel，就是防止这种情况的。可以保证算出来的层级不会超过指定的 MaxLevel。 这样看来，虽然每次都是基于概率决定在那个层级，但是总体趋势是趋近于 1/2 的。 带来的好处是，每次插入都是独立的，只需要调整插入前后节点的指针即可。 一次插入就是一次查询加更新的操作，比如下面的这个示意图： 对于这个概率，其实火男在论文专门写了一个小标题，还给出了一个图表： 最终得出的结论是，火男建议 p 值取 1/4。如果你主要关心的是执行时间的变化，那么 p 就取值 1/2。 说一下我的理解。首先跳表这个是一个典型的空间换时间的例子。 一个有序的二维数组，查找指定元素，理论上是二分查找最快。而跳表就是在基础的链表上不断的抽节点（或者叫索引），形成新的链表。 所以，当 p=1/2 的时候，就近似于二分查找，查询速度快，但是层数比较高，占的空间就大。 当 p=1/4 的时候，元素升级层数的概率就低，总体层高就低，虽然查询速度慢一点，但是占的空间就小一点。 根据《Redis深度历险》一书里面的描述，在 Redis 中 p 的取值就是 1/4，MaxLevel 的取值是 64（视版本而定，有的Redis版本是32）。 论文里面还花了大量的篇幅去推理时间复杂度，有兴趣的可以去看着论文一起推理一下： 跳表在Java中的应用跳表，虽然是一个接触比较少的数据结构。 其实在 java 中也有对应的实现。 先问个问题：Map 家族中大多都是无序的，那么请问你知道有什么 Map 是有序的呢？ TreeMap，LinkedHashMap 是有序的，对吧。 但是他们不是线程安全的。 那么既是线程安全的，又是有序的 Map 是什么？ 那就是它，一个存在感也是低的不行的 ConcurrentSkipListMap。 你看它这个名字多吊，又有 list 又有 Map。 看一个测试用例： 12345678910111213141516public class MainTest &#123; public static void main(String[] args) &#123; ConcurrentSkipListMap&lt;Integer, String&gt; skipListMap = new ConcurrentSkipListMap&lt;&gt;(); skipListMap.put(3,&quot;3&quot;); skipListMap.put(6,&quot;6&quot;); skipListMap.put(7,&quot;7&quot;); skipListMap.put(9,&quot;9&quot;); skipListMap.put(12,&quot;12&quot;); skipListMap.put(17,&quot;17&quot;); skipListMap.put(19,&quot;19&quot;); skipListMap.put(21,&quot;21&quot;); skipListMap.put(25,&quot;25&quot;); skipListMap.put(26,&quot;26&quot;); System.out.println(&quot;skipListMap = &quot; + skipListMap); &#125;&#125; 输出结果是这样的，确实是有序的： 稍微的剖析一下。首先看看它的三个关键结构。 第一个是 index： index 里面包含了一个节点 node、一个右指针（right）、一个下指针（down）。 第二个是 HeadIndex: 它是继承自 index 的，只是多了一个 level 属性，记录是位于第几层的索引。 第三个是 node： 这个 node 没啥说的，一看就是个链表。 这三者之间的关系就是示意图这样的： 我们就用前面的示例代码，先 debug 一下，把上面的示意图，用真实的值填充上。 debug 跑起来之后，可以看到当前是有两层索引的，需要注意的是这里的两层是不包含最底层的基础的有序链表的： 我们先看看第二层的链表是怎样的，也就是看第二层头节点的 right 属性： 所以第二层的链表是这样的： 第二层的 HeadIndex 节点除了我们刚刚分析的 right 属性外，还有一个 down，指向的是下一层，也就是第一层的 HeadIndex： 可以看到第一层的 HeadIndex 的 down 属性是 null。但是它的 right 属性是有值的，但 right 属性里面的 down 属性也是 null： 可以画出第一层的链表结构是这样的： 同时我们可以看到其 node 属性里面其实是整个有序链表（其实每一层的 HeadIndex 里面都有，right 节点里面也有）： 所以，整个跳表结构是这样的，需要注意的是最底层的有序链表和第一层之间是虚线连接的，它们之间是不存在 down 属性的： 但是当你拿着同样的程序，自己去调试的时候，你会发现，你的跳表不长这样啊？ 当然不一样了，一样了才是撞了鬼了。 别忘了，索引的层级是随机产生的。 ConcurrentSkipListMap 是怎样随机的呢？ 带大家看看 put 部分的源码。 标号为 ① 的地方代码很多，但是核心思想是把指定元素维护进最底层的有序链表中。就不进行解读了，所以我把这块代码折叠起来了。 标号为 ② 的地方是 (rnd &amp; 0x80000001) == 0。 这个 rnd 是上一行代码随机出来的值。 而 0x80000001 对应的二进制是这样的： 一头一尾都是1，其他位都是 0。 那么只有 rnd 的一头一尾都是 0 的时候，才会满足 if 条件，(rnd &amp; 0x80000001) == 0。 二进制的一头一尾都是 0，说明是一个正偶数。 随机出来一个正偶数的时候，表明需要对其进行索引的维护。 负偶数，负奇数，正偶数，正奇数。而这里只要正偶数，说明这里的概率其实是 1/4。 标号为 ③ 的地方是判断当前元素要维护到第几层索引中。默认是第 1 层。 ((rnd &gt;&gt;&gt;= 1) &amp; 1) != 0 ，已知 rnd 是一个正偶数，那么从其二进制的低位的第二位（第一位肯定是0嘛）开始，有几个连续的 1，就维护到第几层。 不明白？没关系，我举个例子。 假设随机出来的正偶数是 110，其二进制是 01101110。因为有 3 个连续的 1，那么 level 就是从 1 连续自增 3 次，最终的 level 就是 4。 那么问题就来了，如果我们当前最多只有 2 层索引呢？直接就把索引干到第 4 层吗？ 这个时候标号为 ④ 的代码的作用就出来了。 如果新增的层数大于现有的层数，那么只是在现有的层数上进行加一。 这个时候我们再回过头去看看火男论文里面的随机算法： 意思是一个意思，但是实现起来其实是两个不同的方案。但是核心实现都是随机。 所以，你现在知道了，由于有随机数的出现，所以即使是相同的参数，每次都可以构建出不一样的跳表结构。 比如还是前面演示的代码，我 debug 截图的时候有两层索引。 但是，其实有的时候我也会碰到 3 层索引的情况。 别问为什么，用心去感受，你心里应该有数。 另外，开篇用 redis 作为了切入点，其实 redis 的跳表整体思想是大同的，但是也是有小异的。 比如 Redis 在 skiplist 的 forward 指针（相当于 index）上，每个 forward 指针都增加了 span 属性。 在《Redis深度历险》一书里面对该属性进行了描述： 好了，那么这次的文章就到这里啦。","categories":[],"tags":[]},{"title":"Redis应用-位图","slug":"数据库/Redis/Redis应用-位图","date":"2021-07-20T05:09:35.031Z","updated":"2021-08-01T03:38:47.040Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis应用-位图/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis%E5%BA%94%E7%94%A8-%E4%BD%8D%E5%9B%BE/","excerpt":"我们都知道8bit = 1b = 2^-10kb， bitmap就是通过最小的单位 bit来进行0或者1的设置，表示某个元素对应的值或者状态。 一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。 位图并不是一种特殊的数据结构，其实本质上是二进制字符串，也可以看做是 byte 数组。可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。","text":"我们都知道8bit = 1b = 2^-10kb， bitmap就是通过最小的单位 bit来进行0或者1的设置，表示某个元素对应的值或者状态。 一个bit的值，或者是0，或者是1；也就是说一个bit能存储的最多信息是2。 位图并不是一种特殊的数据结构，其实本质上是二进制字符串，也可以看做是 byte 数组。可以使用普通的 get/set 直接获取和设置整个位图的内容，也可以使用位图操作 getbit/setbit 等将 byte 数组看成「位数组」来处理。 位图的优势: 基于最小的单位bit进行存储，所以非常省空间。 设置时候时间复杂度O(1)、读取时候时间复杂度O(n)，操作是非常快的 二进制数据的存储，进行相关计算的时候非常快 方便扩容 一般可以在如下场景使用： 用户签到 用户在线状态 统计活跃用户 各种状态值 常用命令SETBIT对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。SETBIT key offset valueoffset 参数必须大于或等于 0 ，小于 2^32 (bit 映射被限制在 512 MB 之内)。 GETBIT对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 1GETBIT key offset BITCOUNT计算给定字符串中，被设置为 1 的比特位的数量。 1BITCOUNT key BITPOS返回位图中第一个值为 bit 的二进制位的位置。 1BITPOS key bit[start][end] BITOP对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 BITOP operation destkey key[key…]operation 可以是 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种 BITOP AND destkey key[key...]，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITFIELDbitfield 有三个子指令，分别是 get/set/incrby，它们都可以对指定位片段进行读写，但是最多只能处理 64 个连续的位，如果超过 64 位，就得使用多个子指令，bitfield 可以一次执行多个子指令。 适用于各类统计应用记录用户的签到，每日在线情况等，可以将当天或者当天的偏移量对应的bit位设置为1即可，使用 BITCOUNT可以轻松统计签到次数。 还有一种使用比较多的情况，就是设置各类状态值，例如商城的设置：是否可以评价订单，是否展示售罄商品，是否正常营业等状态值可以使用bitmap来存储 在性能方面，如前面提到的签到，即使运行 10 年，占用的空间也只是每个用户 10*365 比特位(bit)，也即是每个用户 456 字节。对于这种大小的数据来说， BITCOUNT key [start] [end] 的处理速度就像 GET key 和 INCR key 这种 O(1) 复杂度的操作一样快。 当然如果你的 bitmap 数据非常大，那么可以考虑使用以下两种方法： 将一个大的 bitmap 分散到不同的 key 中，作为小的 bitmap 来处理。使用 Lua 脚本可以很方便地完成这一工作。 使用 BITCOUNT key [start] [end] 的 start 和 end 参数，每次只对所需的部分位进行计算，然后在进行累加。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://alloceee.github.io/tags/MySQL/"}],"author":"Alloceee"},{"title":"Redis常见问题","slug":"数据库/Redis/Redis常见问题","date":"2021-07-20T05:09:35.030Z","updated":"2021-07-27T08:38:12.716Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis常见问题/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"一、Redis雪崩、穿透、并发等5大难题解决方案缓存雪崩数据未加载到缓存中，或者缓存同一时间大面积的失效，从而导致所有请求都去查数据库，导致数据库CPU和内存负载过高，甚至宕机。 雪崩的简单过程： redis集群大面积故障 缓存失效，但依然大量请求访问缓存服务redis redis大量失效后，大量请求转向到mysql数据库 mysql的调用量暴增，很快就扛不住了，甚至直接宕机 由于大量的应用服务依赖mysql和redis的服务，这个时候很快会演变成各服务器集群的雪崩，最后网站彻底崩溃。 如何预防缓存雪崩： 缓存层设计成高可用，防止缓存大面积故障。即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如 Redis Sentinel 和 Redis Cluster 都实现了高可用。 2.缓存降级 可以利用ehcache等本地缓存(暂时支持)，但主要还是对源服务访问进行限流、资源隔离（熔断）、降级等。 当访问量剧增、服务出现问题仍然需要保证服务还是可用的。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级，这里会涉及到运维的配合。 降级的最终目的是保证核心服务可用，即使是有损的。 比如推荐服务中，很多都是个性化的需求，假如个性化需求不能提供服务了，可以降级补充热点数据，不至于造成前端页面是个大空白。 在进行降级之前要对系统进行梳理，比如：哪些业务是核心(必须保证)，哪些业务可以容许暂时不提供服务(利用静态页面替换)等，以及配合服务器核心指标，来后设置整体预案，比如： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； （3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 3.Redis备份和快速预热 (1)Redis数据备份和恢复 (2)快速缓存预热 4.提前演练 最后，建议还是在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，对高可用提前预演，提前发现问题。 缓存穿透缓存穿透是指查询一个一不存在的数据。例如：从缓存redis没有命中，需要从mysql数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决思路： 如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库。设置一个过期时间或者当有值的时候将缓存中的值替换掉即可。 可以给key设置一些格式规则，然后查询之前先过滤掉不符合规则的Key。 缓存并发这里的并发指的是多个redis的client同时set key引起的并发问题。其实redis自身就是单线程操作，多个client并发操作，按照先到先执行的原则，先到的先执行，其余的阻塞。当然，另外的解决方案是把redis.set操作放在队列中使其串行化，必须的一个一个执行。 缓存预热缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。 这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 直接写个缓存刷新页面，上线时手工操作下； 数据量不大，可以在项目启动的时候自动进行加载； 目的就是在系统上线前，将数据加载到缓存中。 二、Redis为什么是单线程，高并发快的3大原因详解Redis的高并发和快速原因 redis是基于内存的，内存的读写速度非常快； redis是单线程的，省去了很多上下文切换线程的时间； redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。 下面重点介绍单线程设计和IO多路复用核心设计快的原因。 为什么Redis是单线程的？1.官方答案 因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 2.性能指标 关于redis的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。 3.详细原因 1.不需要各种锁的性能消耗 Redis的数据结构并不全是简单的Key-Value，还有list，hash等复杂的结构，这些结构有可能会进行很细粒度的操作，比如在很长的列表后面添加一个元素，在hash当中添加或者删除一个对象。这些操作可能就需要加非常多的锁，导致的结果是同步开销大大增加。 总之，在单线程的情况下，就不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。 2.单线程多进程集群方案 单线程的威力实际上非常强大，每核心效率也非常高，多线程自然是可以比单线程有更高的性能上限，但是在今天的计算环境中，即使是单机多线程的上限也往往不能满足需要了，需要进一步摸索的是多服务器集群化的方案，这些方案中多线程的技术照样是用不上的。 所以单线程、多进程的集群不失为一个时髦的解决方案。 3.CPU消耗 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU。但是如果CPU成为Redis瓶颈，或者不想让服务器其他CUP核闲置，那怎么办？ 可以考虑多起几个Redis进程，Redis是key-value数据库，不是关系数据库，数据之间没有约束。只要客户端分清哪些key放在哪个Redis进程上就可以了。 Redis单线程的优劣势单进程单线程优势 代码更清晰，处理逻辑更简单不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗不存在多进程或者多线程导致的切换而消耗CPU 单进程单线程弊端 无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善； IO多路复用技术 redis 采用网络IO多路复用技术来保证在多连接的时候， 系统的高吞吐量。 多路-指的是多个socket连接，复用-指的是复用一个线程。多路复用主要有三种技术：select，poll，epoll。epoll是最新的也是目前最好的多路复用技术。 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量。 Redis高并发快总结 Redis是纯内存数据库，一般都是简单的存取操作，线程占用的时间很多，时间的花费主要集中在IO上，所以读取速度快。 再说一下IO，Redis使用的是非阻塞IO，IO多路复用，使用了单线程来轮询描述符，将数据库的开、关、读、写都转换成了事件，减少了线程切换时上下文的切换和竞争。 Redis采用了单线程的模型，保证了每个操作的原子性，也减少了线程的上下文切换和竞争。 另外，数据结构也帮了不少忙，Redis全程使用hash结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。 还有一点，Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。 三、Redis缓存和MySQL数据一致性方案详解 需求起因 在高并发的业务场景下，数据库大多数情况都是用户并发访问最薄弱的环节。所以，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问MySQL等数据库。 这个业务场景，主要是解决读数据从Redis缓存，一般都是按照下图的流程来进行业务操作。 读取缓存步骤一般没有什么问题，但是一旦涉及到数据更新：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。 不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。举一个例子： 如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。 如果先写了库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。 因为写和读是并发的，没法保证顺序,就会出现缓存和数据库的数据不一致的问题。 如来解决？这里给出两个解决方案，先易后难，结合业务和技术代价选择使用。 缓存和数据库一致性解决方案1.第一种方案：采用延时双删策略 在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。 伪代码如下： 123456public void write(String key,Object data)&#123; redis.delKey(key); db.updateData(data); Thread.sleep(500); redis.delKey(key);&#125; 具体的步骤就是： 先删除缓存；再写数据库；休眠500毫秒；再次删除缓存。 那么，这个500毫秒怎么确定的，具体该休眠多久呢？ 需要评估自己的项目的读数据业务逻辑的耗时。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 当然这种策略还要考虑redis和数据库主从同步的耗时。最后的的写数据的休眠时间：则在读数据业务逻辑的耗时基础上，加几百ms即可。比如：休眠1秒。 设置缓存过期时间 从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。所有的写操作以数据库为准，只要到达缓存过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。 该方案的弊端 结合双删策略+缓存超时设置，这样最差的情况就是在超时时间内数据存在不一致，而且又增加了写请求的耗时。 2、第二种方案：异步更新缓存(基于订阅binlog的同步机制) 技术整体思路： MySQL binlog增量订阅消费+消息队列+增量数据更新到redis 读Redis：热数据基本都在Redis 写MySQL:增删改都是操作MySQL 更新Redis数据：MySQL的数据操作binlog，来更新到Redis Redis更新 1）数据操作主要分为两大块： 一个是全量(将全部数据一次写入到redis)一个是增量（实时更新） 这里说的是增量,指的是mysql的update、insert、delate变更数据。 2）读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。 这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。 其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。 这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。 当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://alloceee.github.io/tags/Redis/"}]},{"title":"","slug":"数据库/Redis/Redis","date":"2021-07-20T05:09:35.027Z","updated":"2020-05-11T07:01:38.000Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis/","excerpt":"","text":"Redis面试题大全含答案 1.什么是Redis？答：Remote Dictionary Server(Redis)是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 2.Redis的特点什么是？\\1. 支持多种数据结构，如 string(字符串)、 list(双向链表)、dict(hash表)、set(集合)、zset(排序set)、hyperloglog(基数估算)\\2. 支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段。\\3. 支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制，支持多级复制和增量复制，master-slave机制是Redis进行HA的重要手段。单进程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题。 3.Redis数据类型有哪些？答：String(字符串)Hash(hash表)List(链表)Set(集合)SortedSet(有序集合zset) 4.Redis中的常用命令哪些？incr 让当前键值以1的数量递增，并返回递增后的值incrby 可以指定参数一次增加的数值，并返回递增后的值incrby 可以指定参数一次增加的数值，并返回递增后的值decrby 可以指定参数一次递减的数值，并返回递减后的值incrbyfloat 可以递增一个双精度浮点数append 作用是向键值的末尾追加value。如果键不存在则将该键的值设置为value。返回值是追加后字符串的总长度。mget/mset 作用与get/set相似，不过mget/mset可以同时获得/设置多个键的键值del 根据key来删除valueflushdb 清除当前库的所有数据hset 存储一个哈希键值对的集合hget获取一个哈希键的值hmset 存储一个或多个哈希是键值对的集合hmget 获取多个指定的键的值hexists 判断哈希表中的字段名是否存在 如果存在返回1 否则返回0hdel 删除一个或多个字段hgetall 获取一个哈希是键值对的集合hvals 只返回字段值hkeys 只返回字段名hlen 返回key的hash的元素个数lpush key value向链表左侧添加rpush key value向链表右侧添加lpop key 从左边移出一个元素rpop key 从右边移出一个元素llen key 返回链表中元素的个数 相当于关系型数据库中 select count()lrange key start end lrange命令将返回索引从start到stop之间的所有元素。Redis的列表起始索引为0。lrange也支持负索引 lrange nn -2 -1 如 -1表示最右边第一个元素 -2表示最右边第二个元素，依次类推。lindex key indexnumber 如果要将列表类型当做数组来用，lindex命令是必不可少的。lindex命令用来返回指定索引的元素，索引从0开始如果是负数表示从右边开始计算的索引，最右边元素的索引是-1。Lset key indexnumber value 是另一个通过索引操作列表的命令，它会将索引为index的元素赋值为value。sadd key value 添加一个string元素到,key对应的set集合中，成功返回1,如果元素已经在集合中返回0scard key 返回set的元素个数，如果set是空或者key不存在返回0smembers key 返回key对应set的所有元素，结果是无序的sismember key value 判断value 是否在set中，存在返回1，0表示不存在或者key不存在srem key value 从key对应set中移除给定元素，成功返回1，如果value 在集合中不存在或者key不存在返回0zadd key score value 将一个或多个value及其socre加入到set中zrange key start end 0和-1表示从索引为0的元素到最后一个元素（同LRANGE命令相似）zrange key 0 -1 withscores 也可以连同score一块输出，使用WITHSCORES参数zremrangebyscore key start end 可用于范围删除操作ping 测试redis是否链接 如果已链接返回 PONGecho value测试redis是否链接 如果已链接返回 echo命令后给定的值keys * 返回所有的key 可以加通配exists key判断string类型一个key是否存在 如果存在返回1 否则返回0expire key time(s) 设置一个key的过期时间 单位秒。时间到达后会删除key及valuettl key 查询已设置过期时间的key的剩余时间 如果返回-2表示该键值对已经被删除persist 移除给定key的过期时间select dbindex 选择数据库(0-15)move key dbIndex 将当前数据库中的key转移到其他数据库中dbsize 返回当前数据库中的key的数目info 获取服务器的信息和统计flushdb 删除当前选择的数据库中的keyflushall 删除所有数据库中的所有keyquit 退出连接 5.Redis的配置以及持久化方案有几种？以下两种RDB方式AOF方式 什么是RDB方式？是RDB是对内存中数据库状态进行快照RDB方式：将Redis在内存中的数据库状态保存到磁盘里面，RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态（默认下，持久化到dump.rdb文件，并且在redis重启后，自动读取其中文件，据悉，通常情况下一千万的字符串类型键，1GB的快照文件，同步到内存中的 时间是20-30秒）RDB的生成方式：1、执行命令手动生成有两个Redis命令可以用于生成RDB文件，一个是SAVE，另一个是BGSAVE SAVE命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求，BGSAVE命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程（父进程）继续处理命令请求，创建RDB文件结束之前，客户端发送的BGSAVE和SAVE命令会被服务器拒绝 2、通过配置自动生成可以设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令，可以通过save选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行BGSAVE命令例如：save 900 1save 300 10save 60 10000那么只要满足以下三个条件中的任意一个，BGSAVE命令就会被执行服务器在900秒之内，对数据库进行了至少1次修改服务器在300秒之内，对数据库进行了至少10次修改服务器在60秒之内，对数据库进行了至少10000次修改 什么是AOF方式？AOF持久化方式在redis中默认是关闭的，需要修改配置文件开启该方式。AOF：把每条命令都写入文件，类似mysql的binlog日志AOF方式：是通过保存Redis服务器所执行的写命令来记录数据库状态的文件。AOF文件刷新的方式，有三种：appendfsync always - 每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全appendfsync everysec - 每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据appendfsync no - 依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差默认并推荐每秒刷新，这样在速度和安全上都做到了兼顾AOF数据恢复方式服务器在启动时，通过载入和执行AOF文件中保存的命令来还原服务器关闭之前的数据库状态，具体过程：载入AOF文件创建模拟客户端从AOF文件中读取一条命令使用模拟客户端执行命令循环读取并执行命令，直到全部完成如果同时启用了RDB和AOF方式，AOF优先，启动时只加载AOF文件恢复数据","categories":[],"tags":[]},{"title":"","slug":"数据库/Redis/Redis-面试题","date":"2021-07-20T05:09:35.026Z","updated":"2020-08-06T03:10:42.000Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis-面试题/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis-%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"1.Redis是什么Redis是C语言开发的一个开源的（遵从BSD协议）高性能键值对（key-value）的内存数据库，可以用做数据库、缓存、消息中间件等 它是一种NoSQL（not-only sql，泛指非关系型数据库）的数据库。 Redis作为一个内存数据库： 性能优秀，数据在内存中，读写速度非常快，支持并发10w QPS 单进程单线程，是线程安全的，采用IO多路复合机制 丰富的数据类型，支持字符串（strings）、散列（hashes）、列表（lists）、集合（sets）、有序集合（sorted sets）等 支持数据持久化，可以将内存中数据保存在磁盘中，重启时加载 主从复制，哨兵，高可用 可以用作分布式锁 可以作为消息中间件使用，支持发布订阅 Redis为何这么快，还是单线程Redis确实是单进程单线程的模型，因为Redis完全是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。 既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章的采用单线程的方案了，毕竟采用多线程会有很多麻烦。 为什么单线程这么快： Redis是完全基于内存，绝大部分请求是纯粹的内存操作，非常迅速，数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度是O（1）。 数据结构简单，对数据操作也简单 采用单线程，避免了不必要的上下文切换和竞争条件，不存在多线程导致的CPU切换，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有死锁问题导致的性能消耗。 使用多路复用IO模型，非阻塞IO Redis和Memcached的区别 存储方式上：Memcached会把数据全部存储在内存之中，断电之后会挂掉，数据不能超过内存大小。Redis有部分数据存在硬盘上，这样能保证数据的持久性。 数据支持类型上：Memcached对数据类型的支持简单，只支持简单的key-value，而Redis支持五种数据类型。 使用底层模型不同：它们之间底层实现方式以及与客户端之间通信的应用协议不一样。Redis直接自己构建了VM机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 Value的大小：Redis可以达到1GB，而Memached只有1MB。 淘汰策略Redis有六种淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的KV集合中有限对最近最好使用（less recently used）的数据淘汰 volatile-ttl 从已设置过期时间的KV集合中优先对剩余时间短（time to live）的数据淘汰 volatile-random 从已设置过期时间的KV集合中随机选择数据淘汰 allkeys-lru 从所有KV集合中优先对最近最少使用（less recently used）的数据淘汰 allkeys-random 从所有KV集合中随机选择数据淘汰 noeviction 不淘汰策略，若超过最大内存，返回错误信息 Redis 4.0加入了LFU（least frequently use）淘汰策略，包括volatile-lfu和allkeys-lfu，通过统计访问频率，将访问频率最少，即最不经常使用的KV淘汰 持久化机制Redis为了保证效率，数据缓存在内存中，但是会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中，以保证数据的持久化。 Redis的持久化策略有两种： RDB：快照形式是直接把内存中的数据保存到一个dump的文件中，定时保存策略 AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令集合。 Redis默认是快照RDB的持久化方式。 当Redis重启的时候，它会优先使用AOF文件来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整，你甚至可以关闭持久化功能，让数据只在服务器运行时存。 主从复制","categories":[],"tags":[]},{"title":"","slug":"数据库/Redis/Redis-集群的原理与搭建","date":"2021-07-20T05:09:35.022Z","updated":"2020-07-15T03:05:34.000Z","comments":true,"path":"2021/07/20/数据库/Redis/Redis-集群的原理与搭建/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis-%E9%9B%86%E7%BE%A4%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E6%90%AD%E5%BB%BA/","excerpt":"","text":"前言 Redis 是我们目前大规模使用的缓存中间件，由于它强大高效而又便捷的功能，得到了广泛的使用。单节点的Redis已经就达到了很高的性能，为了提高可用性我们可以使用Redis集群。本文参考了Rdis的官方文档和使用Redis官方提供的Redis Cluster工具搭建Rdis集群。 注意 ：Redis的版本要在3.0以上,截止今天，Redis的版本是3.2.9，本教程也使用3.2.9作为教程 Redis集群的概念 介绍 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的错误。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 数据分片 Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 我现在想设置一个key，叫my_name: 1set my_name zhangguoji 按照Redis Cluster的哈希槽算法，CRC16(‘my_name’)%16384 = 2412 那么这个key就被分配到了节点A上 。 同样的，当我连接(A,B,C)的任意一个节点想获取my_name这个key,都会转到节点A上 ，再比如 ，如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 增加一个D节点的结果可能如下： 节点A覆盖1365-5460 节点B覆盖6827-10922 节点C覆盖12288-16383 节点D覆盖0-1364,5461-6826,10923-1228 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。 因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 所以,Redis Cluster的模型大概是这样的形状 主从复制模型 为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis一致性保证 Redis 并不能保证数据的强一致性. 这意味这在实际中集群在特定的条件下可能会丢失写操作： 第一个原因是因为集群是用了异步复制. 写操作过程: 客户端向主节点B写入一条命令. 主节点B向客户端回复命令状态. 主节点将写操作复制给他得从节点 B1, B2 和 B3 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 ** 注意：Redis 集群可能会在将来提供同步写的方法。 Redis 集群另外一种可能会丢失命令的情况是集群出现了网络分区， 并且一个客户端与至少包括一个主节点在内的少数实例被孤立。 举个例子 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， A1 、B1 、C1 为A，B，C的从节点， 还有一个客户端 Z1 假设集群中发生网络分区，那么集群可能会分为两方，大部分的一方包含节点 A 、C 、A1 、B1 和 C1 ，小部分的一方则包含节点 B 和客户端 Z1 . Z1仍然能够向主节点B中写入, 如果网络分区发生时间较短,那么集群将会继续正常运作,如果分区的时间足够让大部分的一方将B1选举为新的master，那么Z1写入B中得数据便丢失了. 注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项 搭建Redis集群 要让集群正常工作至少需要3个主节点，在这里我们要创建6个redis节点，其中三个为主节点，三个为从节点，对应的redis节点的ip和端口对应关系如下（为了简单演示都在同一台机器上面） 123456127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002127.0.0.1:7003127.0.0.1:7004127.0.0.1:7005 安装和启动Redis 下载安装包 1wget http://download.redis.io/releases/redis-3.2.9.tar.gz 解压安装 123tar zxvf redis-3.2.9.tar.gzcd redis-3.2.9make &amp;&amp; make PREFIX=/usr/local/redis install 这里如果失败的自行yum安装gcc和tcl 12yum install gcc yum install tcl 创建目录 1234cd /usr/local/redismkdir clustercd clustermkdir 7000 7001 7002 7003 7004 7005 复制和修改配置文件 将redis目录下的配置文件复制到对应端口文件夹下,6个文件夹都要复制一份 1cp redis-3.2.9/redis.conf /usr/local/redis/cluster/7000 修改配置文件redis.conf，将下面的选项修改 123456789101112131415161718# 端口号port 7000# 后台启动daemonize yes# 开启集群cluster-enabled yes#集群节点配置文件cluster-config-file nodes-7000.conf# 集群连接超时时间cluster-node-timeout 5000# 进程pid的文件位置pidfile /var/run/redis-7000.pid# 开启aofappendonly yes# aof文件路径appendfilename &quot;appendonly-7005.aof&quot;# rdb文件路径dbfilename dump-7000.rdb 6个配置文件安装对应的端口分别修改配置文件 创建启动脚本 在/usr/local/redis目录下创建一个start.sh 1234567#!/bin/bashbin/redis-server cluster/7000/redis.confbin/redis-server cluster/7001/redis.confbin/redis-server cluster/7002/redis.confbin/redis-server cluster/7003/redis.confbin/redis-server cluster/7004/redis.confbin/redis-server cluster/7005/redis.conf 这个时候我们查看一下进程看启动情况 1ps -ef | grep redis 进程状态如下： 123456root 1731 1 1 18:21 ? 00:00:49 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:29 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:08 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:26 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:13 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:08 bin/redis-server *:7005 [cluster] 有6个redis进程在开启，说明我们的redis就启动成功了 开启集群 这里我们只是开启了6个redis进程而已，它们都还只是独立的状态，还么有组成集群 这里我们使用官方提供的工具redis-trib，不过这个工具是用ruby写的，要先安装ruby的环境 1yum install ruby rubygems -y 执行，报错 12345[root@centos]# redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005/usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `gem_original_require&#x27;: no such file to load -- redis (LoadError) from /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31:in `require&#x27; from /usr/local/bin/redis-trib.rb:25[root@centos]# 原来是ruby和redis的连接没安装好 安装gem-redis 1gem install redis 安装到这里的时候突然卡住很久不动，网上搜了下，这里需要翻墙或者换镜像 1gem source -a https://gems.ruby-china.org 这里可以将镜像换成ruby-china的镜像，不过我好像更换失败，最终还是翻墙下载了 12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 等下载好后我们就可以使用了 12345[root@centos]# gem install redisSuccessfully installed redis-3.2.11 gem installedInstalling ri documentation for redis-3.2.1...Installing RDoc documentation for redis-3.2.1... 将redis-3.2.9的src目录下的trib复制到相应文件夹 1cp redis-3.2.9/src/redis-trib.rb /usr/local/redis/bin/redis-trib 创建集群： 1redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。 选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。 简单来说，以上的命令的意思就是让redis-trib程序帮我们创建三个主节点和三个从节点的集群， 接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002Adding replica 127.0.0.1:7003 to 127.0.0.1:7000Adding replica 127.0.0.1:7004 to 127.0.0.1:7001Adding replica 127.0.0.1:7005 to 127.0.0.1:7002M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterS: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6Can I set the above configuration? (type &#x27;yes&#x27; to accept): 按下yes，集群就会将配置应用到各个节点，并连接起（join)各个节点，也即是，让各个节点开始通讯 123456789101112131415161718192021222324252627&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. Redis集群的使用 连接集群 这里我们使用reids-cli连接集群，使用时加上-c参数，就可以连接到集群 连接7000端口的节点 123456[root@centos1 redis]# ./redis-cli -c -p 7000127.0.0.1:7000&gt; set name zgj-&gt; Redirected to slot [5798] located at 127.0.0.1:7001OK127.0.0.1:7001&gt; get name&quot;zgj&quot; 前面的理论知识我们知道了，分配key的时候，它会使用CRC16算法，这里将keyname分配到了7001节点上 1Redirected to slot [5798] located at 127.0.0.1:7001 redis cluster 采用的方式很直接，它直接跳转到7001 节点了，而不是还在自身的7000节点。 好，现在我们连接7003这个从节点进入 1234[root@centos1 redis]# ./redis-cli -c -p 7003127.0.0.1:7003&gt; get name-&gt; Redirected to slot [5798] located at 127.0.0.1:7001&quot;zgj&quot; 这里获取name的值，也同样跳转到了7001上 我们再测试一下其他数据 123456789127.0.0.1:7001&gt; set age 20-&gt; Redirected to slot [741] located at 127.0.0.1:7000OK127.0.0.1:7000&gt; set message helloworld-&gt; Redirected to slot [11537] located at 127.0.0.1:7002OK127.0.0.1:7002&gt; set height 175-&gt; Redirected to slot [8223] located at 127.0.0.1:7001OK 我们发现数据会在7000-7002这3个节点之间来回跳转 测试集群中的节点挂掉 上面我们建立了一个集群，3个主节点和3个从节点，7000-7002负责存取数据，7003-7005负责把7000-7005的数据同步到自己的节点上来。 我们现在来模拟一下一台matser服务器宕机的情况 12345678910111213141516171819202122232425262728293031[root@centos1 redis]# ps -ef | grep redisroot 1731 1 0 18:21 ? 00:01:02 bin/redis-server *:7000 [cluster] root 1733 1 0 18:21 ? 00:00:43 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:22 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:40 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:27 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:22 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:42 ./redis-server *:6379 root 24491 1635 0 21:55 pts/1 00:00:00 grep redis[root@centos1 redis]# kill 1731[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 0 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里看得出来，现在已经有了3个节点了，7003被选取成了替代7000成为主节点了。 我们再来模拟 7000节点重新启动了的情况，那么它还会自动加入到集群中吗？那么，7000这个节点上充当什么角色呢？ 我们试一下： 12345678910111213141516171819202122232425[root@centos1 redis]# bin/redis-server cluster/7000/redis.conf[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots: (0 slots) slave replicates d403713ab9db48aeac5b5393b69e1201026ef479S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7000节点变成了7003节点的从节点 我们试着将7000和7003两个节点都关掉 123456789101112131415161718192021222324252627282930[root@centos1 redis]# ps -ef | grep redisroot 1733 1 0 18:21 ? 00:00:45 bin/redis-server *:7001 [cluster] root 1735 1 0 18:21 ? 00:00:24 bin/redis-server *:7002 [cluster] root 1743 1 0 18:21 ? 00:00:42 bin/redis-server *:7003 [cluster] root 1745 1 0 18:21 ? 00:00:29 bin/redis-server *:7004 [cluster] root 1749 1 0 18:21 ? 00:00:24 bin/redis-server *:7005 [cluster] root 23988 1 0 18:30 ? 00:00:43 ./redis-server *:6379 root 24527 1 0 22:04 ? 00:00:00 bin/redis-server *:7000 [cluster] root 24541 1635 0 22:07 pts/1 00:00:00 grep redis[root@centos1 redis] kill 1743[root@centos1 redis] kill 24527[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7001&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7001)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[ERR] Not all 16384 slots are covered by nodes. 这里我们的集群就不能工作了，因为两个节点主节点和从节点都挂掉了，原来7001分配的slot现在无节点接管，需要人工介入重新分配slots。 集群中加入新的主节点 这里在cluster目录下再新建一个7006并修改对应的配置文件，然后启动这个这个redis进程 然后再使用redis-trib的add node指令加入节点 1bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000 这里前面的节点表示要加入的节点，第二个节点表示要加入的集群中的任意一个节点，用来标识这个集群 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@centos1 redis]# bin/redis-trib add-node 127.0.0.1:7006 127.0.0.1:7000&gt;&gt;&gt; Adding node 127.0.0.1:7006 to cluster 127.0.0.1:7000&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7006 to make it join the cluster.[OK] New node added correctly.[root@centos1 redis]# bin/redis-trib check 127.0.0.1:7006&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这里我们可以看到7006节点已经变成了一个主节点，然鹅，等等，好像发现了有什么地方不对 12M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots: (0 slots) master 里面0 slots,也就是说节点6没有分配哈希槽，即不能进行数据的存取，拿我加上去干嘛。 原来redis cluster 不是在新加节点的时候帮我们做好了迁移工作，需要我们手动对集群进行重新分片迁移，也是这个命令： 1/bin/redis-trib reshard 127.0.0.1:7000 这个命令是用来迁移slot节点的，后面的127.0.0.1:7000是表示哪个集群的，7000-7006都是可以的 1234567891011121314151617181920212223242526272829303132333435[root@centos1]# redis-trib.rb reshard 127.0.0.1:7000Connecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7006)M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots: (0 slots) master 0 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:0-5460 (5461 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 它提示我们需要迁移多少slot到7006上，我们可以算一下：16384/4 = 4096，也就是说，为了平衡分配起见，我们需要移动4096个槽点到7006上。 好，那输入4096: 它又提示我们，接受的node ID是多少，7006的id 我们通过上面就可以看到是efc3131fbdc6cf929720e0e0f7136cae85657481: 12345What is the receiving node ID? efc3131fbdc6cf929720e0e0f7136cae85657481Please enter all the source node IDs. Type &#x27;all&#x27; to use all the nodes as source nodes for the hash slots. Type &#x27;done&#x27; once you entered all the source nodes IDs.Source node #1: 接着， redis-trib 会向你询问重新分片的源节点（source node）， 也即是， 要从哪个节点中取出 4096 个哈希槽， 并将这些槽移动到7006节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽， 那么可以向 redis-trib 输入 all ， 这样的话， 集群中的所有主节点都会成为源节点， redis-trib 将从各个源节点中各取出一部分哈希槽， 凑够 4096 个， 然后移动到7006节点上： 1Source node #1:all 接下来就开始迁移了，并且会询问你是否确认： 1234567Moving slot 1359 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1360 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1361 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1362 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1363 from d2237fdcfbba672de766b913d1186cebcb6e1761 Moving slot 1364 from d2237fdcfbba672de766b913d1186cebcb6e1761Do you want to proceed with the proposed reshard plan (yes/no)? 输入yes并回车后，redis-trib就会正式执行重新分片操作，将制定的哈希槽从源节点一个个移动到7006节点上 。 迁移结束之后，我们来检查一下 12345678910111213141516171819202122232425M: bdcddddd3d78a866b44b68c7ae0e5ccf875c446a 127.0.0.1:7000 slots:1365-5460 (4096 slots) master 1 additional replica(s)S: d403713ab9db48aeac5b5393b69e1201026ef479 127.0.0.1:7003 slots: (0 slots) slave replicates bdcddddd3d78a866b44b68c7ae0e5ccf875c446aS: b7ec92919e5bcffa76c8eee338f8ca5155293c64 127.0.0.1:7004 slots: (0 slots) slave replicates b85519795fa42aa33d4e88d25104cbae895933a6M: e55599320dabfb31bd22a01407e66121f075e7d3 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 0 additional replica(s)M: b85519795fa42aa33d4e88d25104cbae895933a6 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 8a0d2a3f271b349744a971e1b0a545405de2742e 127.0.0.1:7005 slots: (0 slots) slave replicates b681e1a151890cbf957d1ff08352ee48f6ae39e6M: b681e1a151890cbf957d1ff08352ee48f6ae39e6 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 我们可以看到 slots:0-1364,5461-6826,10923-12287 (4096 slots) 这些原来在其他节点上的哈希槽都迁移到了7006上 增加一个从节点 新建一个 7007从节点，作为7006的从节点 我们再新建一个节点7007，步骤类似，就先省略了。建好后，启动起来，我们看如何把它加入到集群中的从节点中： 1[root@centos1]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000 add-node的时候加上–slave表示是加入到从节点中，但是这样加，是随机的。这里的命令行完全像我们在添加一个新主服务器时使用的一样，所以我们没有指定要给哪个主服 务器添加副本。这种情况下，redis-trib会将7007作为一个具有较少副本的随机的主服务器的副本。 那么，你猜，它会作为谁的从节点，应该是7006，因为7006还没有从节点。我们运行下。 12345678910[root@web3 7007]# redis-trib.rb add-node --slave 127.0.0.1:7007 127.0.0.1:7000......[OK] All 16384 slots covered.Automatically selected master 127.0.0.1:7006Connecting to node 127.0.0.1:7007: OK&gt;&gt;&gt; Send CLUSTER MEET to node 127.0.0.1:7007 to make it join the cluster.Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 上面提示说，自动选择了7006作为master节点。并且成功了。我们检查下： 1234567891011121314151617181920212223242526272829303132333435363738[root@centos1]# redis-trib.rb check 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)S: 3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slots: (0 slots) slave replicates d2237fdcfbba672de766b913d1186cebcb6e1761M: efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 slots:0-1364,5461-6826,10923-12287 (4096 slots) master 1 additional replica(s)S: 4b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slots: (0 slots) slave replicates cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6cS: 30858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slots: (0 slots) slave replicates dfa0754c7854a874a6ebd2613b86140ad97701fcM: d2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 slots:1365-5460 (4096 slots) master 1 additional replica(s)M: cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 slots:6827-10922 (4096 slots) master 1 additional replica(s)S: 86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slots: (0 slots) slave replicates efc3131fbdc6cf929720e0e0f7136cae85657481M: dfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 slots:12288-16383 (4096 slots) master 1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 果然，7007加入到了7006的从节点当中。 你说，我如果想指定一个主节点行不行？当然可以。我们再建一个7008节点。 1bin/redis-trib.rb add-node --slave --master-id efc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7008 127.0.0.1:7000 –master-id 表示指定的主节点node id。这里指定的是 7006 这个主节点。 123Waiting for the cluster to join.&gt;&gt;&gt; Configure node as replica of 127.0.0.1:7006.[OK] New node added correctly. 提示我们已经作为7006的从节点了，也就是加入到7006的从节点来了，照这么说，7006就有2个从节点了，我们看一下： 1234bin/redis-cli -c -p 7008 cluster nodes |grep efc3131fbdc6cf929720e0e0f7136cae8565748186d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 1445089507786 8 connectedefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master - 0 1445089508289 8 connected 0-1364 5461-6826 10923-1228744321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 myself,slave efc3131fbdc6cf929720e0e0f7136cae85657481 0 0 0 connected 我们过滤了下看结果，果真，7007和7008是7006的从节点了。 刚好，我们再做一个实验，我把7006的进程杀掉，看7007和7008谁会变成主节点： 123456789101112131415161718192021222324252627[root@centos1]# ps -ef|grep redisroot 11384 1 0 09:56 ? 00:00:16 redis-server *:7001 [cluster]root 11388 1 0 09:56 ? 00:00:16 redis-server *:7002 [cluster]root 11392 1 0 09:56 ? 00:00:16 redis-server *:7003 [cluster]root 11396 1 0 09:56 ? 00:00:15 redis-server *:7004 [cluster]root 11400 1 0 09:56 ? 00:00:15 redis-server *:7005 [cluster]root 12100 1 0 11:01 ? 00:00:11 redis-server *:7000 [cluster]root 12132 1 0 11:28 ? 00:00:11 redis-server *:7006 [cluster]root 12202 1 0 13:14 ? 00:00:02 redis-server *:7007 [cluster]root 12219 1 0 13:39 ? 00:00:00 redis-server *:7008 [cluster]root 12239 8259 0 13:49 pts/0 00:00:00 grep redis[root@centos1]# kill 12132[root@centos1]# redis-cli -c -p 7008127.0.0.1:7008&gt; get ss5rtr-&gt; Redirected to slot [1188] located at 127.0.0.1:7007&quot;66&quot;127.0.0.1:7007&gt; cluster nodesefc3131fbdc6cf929720e0e0f7136cae85657481 127.0.0.1:7006 master,fail - 1445089780668 1445089779963 8 disconnectedd2237fdcfbba672de766b913d1186cebcb6e1761 127.0.0.1:7003 master - 0 1445089812195 7 connected 1365-546030858dbf483b61b9838d5c1f853a60beaa4e7afd 127.0.0.1:7005 slave dfa0754c7854a874a6ebd2613b86140ad97701fc 0 1445089813710 3 connected86d05e7c2b197dc182b5e71069e791d033cf899e 127.0.0.1:7007 myself,master - 0 0 10 connected 0-1364 5461-6826 10923-12287cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 127.0.0.1:7001 master - 0 1445089814214 2 connected 6827-109224b4aef8b48c427a3c903518339d53b6447c58b93 127.0.0.1:7004 slave cb5c04b6160c3b7e18cad5d49d8e2987b27e0d6c 0 1445089812701 2 connected44321e7d619410dc4e0a8745366610a0d06d2395 127.0.0.1:7008 slave 86d05e7c2b197dc182b5e71069e791d033cf899e 0 1445089814214 10 connected3707debcbe7be66d4a1968eaf3a5ffaf4308efa4 127.0.0.1:7000 slave d2237fdcfbba672de766b913d1186cebcb6e1761 0 1445089813204 7 connecteddfa0754c7854a874a6ebd2613b86140ad97701fc 127.0.0.1:7002 master - 0 1445089813204 3 connected 12288-16383127.0.0.1:7007&gt; 这里7007获得了成为主节点的机会，7008就变成了7007的从节点。 那么这个时候，重启7006节点，那么他就会变成了一个7007的从节点了。 移除一个节点 上面是增加一个节点，接下来就是移除一个节点了，移除节点的命令是 1bin/redis-trib del-node 127.0.0.1:7000 `&lt;node-id&gt;` 没我们尝试下输入以下命令 123456789101112[root@centos]# bin/redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK[ERR] Node 127.0.0.1:7007 is not empty! Reshard data away and try again. 这里报错了，提示我们7007节点里面有数据，让我们把7007节点里的数据移除出去，也就是说需要重新分片，这个和上面增加节点的方式一样，我们再来一遍 1bin/redis-trib.rb reshard 127.0.0.1:7000 省去中间内容，原来7007节点上已经有了4096个哈希槽，这里我们也移动4096个哈希槽 然后将这些哈希槽移动到7001节点上 123Source node #1:86d05e7c2b197dc182b5e71069e791d033cf899eSource node #2:doneDo you want to proceed with the proposed reshard plan (yes/no)? yes 然后我们再继续执行移除命令，结果如下 123456789101112131415[root@centos1]# redis-trib.rb del-node 127.0.0.1:7000 86d05e7c2b197dc182b5e71069e791d033cf899e&gt;&gt;&gt; Removing node 86d05e7c2b197dc182b5e71069e791d033cf899e from cluster 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7007: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7002: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; 127.0.0.1:7006 as replica of 127.0.0.1:7001&gt;&gt;&gt; 127.0.0.1:7008 as replica of 127.0.0.1:7001&gt;&gt;&gt; SHUTDOWN the node. 删除成功，而且还很人性化的将7006和7008这2个原来7007的附属节点送给了7001。考虑的真周到~ 移除一个从节点 移除一个从节点就比较简单了，因为从节点没有哈希槽，也不需要考虑数据迁移，直接移除就行 1234567891011121314[root@centos1]# redis-trib.rb del-node 127.0.0.1:7005 44321e7d619410dc4e0a8745366610a0d06d2395&gt;&gt;&gt; Removing node 44321e7d619410dc4e0a8745366610a0d06d2395 from cluster 127.0.0.1:7005Connecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7006: OKConnecting to node 127.0.0.1:7008: OKConnecting to node 127.0.0.1:7003: OK&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.[root@centos1]# redis-trib.rb check 127.0.0.1:7008Connecting to node 127.0.0.1:7008: [ERR] Sorry, can&#x27;t connect to node 127.0.0.1:7008 表示移除成功 Redis性能测试 Redis自带了性能测试工具redis-benchmark 使用说明如下： 1234567891011121314151617181920212223Usage: redis-benchmark [-h &lt;host&gt;] [-p &lt;port&gt;] [-c &lt;clients&gt;] [-n &lt;requests]&gt; [-k &lt;boolean&gt;]-h &lt;hostname&gt; Server hostname (default 127.0.0.1)-p &lt;port&gt; Server port (default 6379)-s &lt;socket&gt; Server socket (overrides host and port)-c &lt;clients&gt; Number of parallel connections (default 50)-n &lt;requests&gt; Total number of requests (default 10000)-d &lt;size&gt; Data size of SET/GET value in bytes (default 2)-k &lt;boolean&gt; 1=keep alive 0=reconnect (default 1)-r &lt;keyspacelen&gt; Use random keys for SET/GET/INCR, random values for SADD Using this option the benchmark will get/set keys in the form mykey_rand:000000012456 instead of constant keys, the &lt;keyspacelen&gt; argument determines the max number of values for the random number. For instance if set to 10 only rand:000000000000 - rand:000000000009 range will be allowed.-P &lt;numreq&gt; Pipeline &lt;numreq&gt; requests. Default 1 (no pipeline).-q Quiet. Just show query/sec values--csv Output in CSV format-l Loop. Run the tests forever-t &lt;tests&gt; Only run the comma-separated list of tests. The test names are the same as the ones produced as output.-I Idle mode. Just open N idle connections and wait. 基准测试 基准的测试命令： redis-benchmark -q -n 100000 结果入下： 1234567891011121314151617181920root@centos1 bin]# redis-benchmark -q -n 100000-bash: redis-benchmark: command not found[root@centos1 bin]# ./redis-benchmark -q -n 100000PING_INLINE: 61576.36 requests per secondPING_BULK: 60277.28 requests per secondSET: 61349.69 requests per secondGET: 60459.49 requests per secondINCR: 58858.15 requests per secondLPUSH: 59066.75 requests per secondRPUSH: 57339.45 requests per secondLPOP: 55586.44 requests per secondRPOP: 56465.27 requests per secondSADD: 57045.07 requests per secondSPOP: 53734.55 requests per secondLPUSH (needed to benchmark LRANGE): 57012.54 requests per secondLRANGE_100 (first 100 elements): 55803.57 requests per secondLRANGE_300 (first 300 elements): 54914.88 requests per secondLRANGE_500 (first 450 elements): 53333.33 requests per secondLRANGE_600 (first 600 elements): 56529.11 requests per secondMSET (10 keys): 59276.82 requests per second 这里可以看出，单机版的redis每秒可以处理6万个请求，这已经是一个非常厉害的数据了，不得不佩服 我们再来看下集群情况下是是什么情况 123456789101112131415161718[root@centos1 bin]# ./redis-benchmark -q -n 100000 -p 7000PING_INLINE: 64599.48 requests per secondPING_BULK: 64184.85 requests per secondSET: 66800.27 requests per secondGET: 65616.80 requests per secondINCR: 66269.05 requests per secondLPUSH: 40273.86 requests per secondRPUSH: 40355.12 requests per secondLPOP: 43421.62 requests per secondRPOP: 45187.53 requests per secondSADD: 62539.09 requests per secondSPOP: 61538.46 requests per secondLPUSH (needed to benchmark LRANGE): 38182.51 requests per secondLRANGE_100 (first 100 elements): 25555.84 requests per secondLRANGE_300 (first 300 elements): 9571.21 requests per secondLRANGE_500 (first 450 elements): 7214.49 requests per secondLRANGE_600 (first 600 elements): 5478.85 requests per secondMSET (10 keys): 41893.59 requests per second 这里看出大部分和单机版的性能查不多，主要是lrange命令的差别是很大的 流水线测试 使用流水线 默认情况下，每个客户端都是在一个请求完成之后才发送下一个请求（基准会模拟50个客户端除非使用-c指定特别的数量），这意味着服务器几乎是按顺序读取每个客户端的命令。RTT也加入了其中。 真实世界会更复杂，Redis支持/topics/pipelining，使得可以一次性执行多条命令成为可能。Redis流水线可以提高服务器的TPS redis-benchmark -n 1000000 -t set,get -P 16 -q 加入-P选项使用管道技术，一次执行多条命令 123./redis-benchmark -n 1000000 -t set,get -P 16 -qSET: 515198.34 requests per secondGET: 613873.56 requests per second 每秒处理get/sret请求达到了60/50W,真的厉害！ 遇到的问题 安装redis集群的时候遇到了挺多问题，踩了很多坑，单单是修改配置文件就出了不少问题，那些配置文件的内容都要一一修改，有些配置不修改就会出现无法创建进程的错误 注意配置集群的时候不要加密码，否则会出现无法连接的情况 gem install的时候需要修改镜像或者翻墙 昨天启动成功，今天启动的时候报错 1[ERR] Node 172.168.63.202:7001 is not empty. Either the nodealready knows other nodes (check with CLUSTER NODES) or contains some key in database 0 解决方法： 将需要新增的节点下aof、rdb等本地备份文件删除； 同时将新Node的集群配置文件删除,即：删除你redis.conf里面cluster-config-file所在的文件； 再次添加新节点如果还是报错，则登录新Node,执行bin/redis-cli–h x –p对数据库进行清除： 1127.0.0.1:7001&gt; flushdb #清空当前数据库 总结 之间对了Redis的了解并不是说非常多，只是简单的会用，因为现在企业里也很多都在用，刚好老大说接下来的项目可能会用到Redis集群，让我先去了解下，所以最近就在回头看，一边看文档，博客，一边实践，踩了很多的坑，出问题的时候的确是让人感到很痛苦很郁闷的，可是当运行成功的那一刻心情却是无比激动和开心的，可能这就是编程的魅力吧。","categories":[],"tags":[]},{"title":"为什么禁止使用外键","slug":"数据库/MySQL/other/为什么禁止使用外键","date":"2021-07-20T05:09:35.017Z","updated":"2021-07-27T01:33:05.354Z","comments":true,"path":"2021/07/20/数据库/MySQL/other/为什么禁止使用外键/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%A6%81%E6%AD%A2%E4%BD%BF%E7%94%A8%E5%A4%96%E9%94%AE/","excerpt":"","text":"外键的优点一、数据一致性由数据库自身保证数据一致性、完整性会更可靠，程序很难100％保证数据的一致性、完整性 二、ER图可靠性有主外键的数据库设计可以增加ER图的可读性 外键的缺点一、级联问题阿里巴巴的开发手册中，就曾指出强制要求不允许使用外键，一切外键概念必须在应用层解决。 因为每次级联delete或update的时候，都要级联操作相关的外键表，不论有没有这个必要，由其在高并发的场景下，这会导致性能瓶颈 二、增加数据库压力外键等于把数据的一致性事务实现，全部交给数据库服务器完成，并且有了外键，当做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，而不得不消耗资源 三、死锁问题若是高并发大流量事务场景，使用外键还可能容易造成死锁 四、开发不方便有外键时，无论开发还是维护，需要手工维护数据时，都不太方便，要考虑级联因素 总结一、如是单机且低并发，也不需要性能调优，再或者不能用程序保证数据的一致性，完整性，可以使用外键二、如果为了高并发，分布式，使系统性能更优，以及更好维护，则一定不能使用外键 引言其实这个话题是老生常谈，很多人在工作中确实也不会使用外键。包括在阿里的JAVA规范中也有下面这一条 **【强制】不得使用外键与级联，一切外键概念必须在应用层解决。 ** 但是呢，询问他们原因，大多是这么回答的 每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便。 坦白说，这么说也是对的。但是呢，不够全面，所以开一文来详细说明。 正文首先我们明确一点，外键约束是一种约束，这个约束的存在，会保证表间数据的关系“始终完整”。因此，外键约束的存在，并非全然没有优点。比如使用外键，可以 保证数据的完整性和一致性 级联操作方便 将数据完整性判断托付给了数据库完成，减少了程序的代码量 然而，鱼和熊掌不可兼得。外键是能够保证数据的完整性，但是会给系统带来很多缺陷。正是因为这些缺陷，才导致我们不推荐使用外键，具体如下 性能问题假设一张表名为user_tb。那么这张表里有两个外键字段，指向两张表。那么，每次往user_tb表里插入数据，就必须往两个外键对应的表里查询是否有对应数据。如果交由程序控制，这种查询过程就可以控制在我们手里，可以省略一些不必要的查询过程。但是如果由数据库控制，则是必须要去这两张表里判断。 并发问题在使用外键的情况下，每次修改数据都需要去另外一个表检查数据,需要获取额外的锁。若是在高并发大流量事务场景，使用外键更容易造成死锁。 扩展性问题这里主要是分为两点 做平台迁移方便，比如你从Mysql迁移到Oracle，像触发器、外键这种东西，都可以利用框架本身的特性来实现，而不用依赖于数据库本身的特性，做迁移更加方便。 分库分表方便，在水平拆分和分库的情况下，外键是无法生效的。将数据间关系的维护，放入应用程序中，为将来的分库分表省去很多的麻烦。 技术问题使用外键，其实将应用程序应该执行的判断逻辑转移到了数据库上。那么这意味着一点，数据库的性能开销变大了，那么这就对DBA的要求就更高了。很多中小型公司由于资金问题，并没有聘用专业的DBA，因此他们会选择不用外键，降低数据库的消耗。相反的，如果该约束逻辑在应用程序中，发现应用服务器性能不够，可以加机器，做水平扩展。如果是在数据库服务器上，数据库服务器会成为性能瓶颈，做水平扩展比较困难。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"外键","slug":"外键","permalink":"https://alloceee.github.io/tags/%E5%A4%96%E9%94%AE/"}]},{"title":"","slug":"数据库/MySQL/other/mysql count() 函数 字段按条件统计数量并排除某个字段重复值","date":"2021-07-20T05:09:35.015Z","updated":"2021-05-31T09:48:08.904Z","comments":true,"path":"2021/07/20/数据库/MySQL/other/mysql count() 函数 字段按条件统计数量并排除某个字段重复值/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/mysql%20count()%20%E5%87%BD%E6%95%B0%20%E5%AD%97%E6%AE%B5%E6%8C%89%E6%9D%A1%E4%BB%B6%E7%BB%9F%E8%AE%A1%E6%95%B0%E9%87%8F%E5%B9%B6%E6%8E%92%E9%99%A4%E6%9F%90%E4%B8%AA%E5%AD%97%E6%AE%B5%E9%87%8D%E5%A4%8D%E5%80%BC/","excerpt":"","text":"mysql count() 函数 字段按条件统计数量并排除某个字段重复值 12345678910111213141516SELECT *, count(c.recruit_info_id) AS apply_number, count( IF (c.approval_status= 5, 1, NULL) ) AS s_numberFROM `sh_further_position_posting` `a`LEFT JOIN `sh_student_keshi` `b` ON `a`.`student_keshi_id` = `b`.`student_keshi_id`LEFT JOIN `sh_further_recruit_info` `c` ON `c`.`post_id` = `a`.`post_id`WHERE 1 = 1GROUP BY `a`.`post_id`ORDER BY `a`.`create_time` DESCLIMIT 10","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/other/left join、right join和join的区别","date":"2021-07-20T05:09:35.013Z","updated":"2021-03-17T01:24:58.396Z","comments":true,"path":"2021/07/20/数据库/MySQL/other/left join、right join和join的区别/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/left%20join%E3%80%81right%20join%E5%92%8Cjoin%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"真的是一张图道清所有join的区别啊，可惜我还是看不懂，可能人比较懒，然后基本一个left join给我就是够用的了，所以就没怎么去仔细研究了，但是现实还是逼我去搞清楚，索性自己动手，总算理解图中的含义了，下面就听我一一道来。 首先，我们先来建两张表，第一张表命名为kemu，第二张表命名为score： 一、left join顾名思义，就是“左连接”，表1左连接表2，以左为主，表示以表1为主，关联上表2的数据，查出来的结果显示左边的所有数据，然后右边显示的是和左边有交集部分的数据。如下： 12345select *from kemuleft join score on kemu.id = score.id 结果集： 二、right join “右连接”，表1右连接表2，以右为主，表示以表2为主，关联查询表1的数据，查出表2所有数据以及表1和表2有交集的数据，如下： 12345select *from kemuright join score on kemu.id = score.id 结果集： 三、joinjoin，其实就是“inner join”，为了简写才写成join，两个是表示一个的，内连接，表示以两个表的交集为主，查出来是两个表有交集的部分，其余没有关联就不额外显示出来，这个用的情况也是挺多的，如下 12345select *from kemujoin score on kemu.id = score.id 结果集：","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/other/Find_in_set","date":"2021-07-20T05:09:35.011Z","updated":"2021-07-28T03:49:38.885Z","comments":true,"path":"2021/07/20/数据库/MySQL/other/Find_in_set/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/other/Find_in_set/","excerpt":"","text":"Returns a value in the range of 1 to N if the string str is in the string list strlist consisting of N substrings. A string list is a string composed of substrings separated by , characters. If the first argument is a constant string and the second is a column of type SET, the FIND_IN_SET() function is optimized to use bit arithmetic. Returns 0 if str is not in strlist or if strlist is the empty string. Returns NULL if either argument is NULL. This function does not work properly if the first argument contains a comma (,) character. 12mysql&gt; SELECT FIND_IN_SET(&#x27;b&#x27;,&#x27;a,b,c,d&#x27;); -&gt; 2 mysql中FIND_IN_SET函数用来比较是不是包含，不管‘list’字段是变量或给定的字符串常量都能很好的工作。MySQL中原型为：FIND_IN_SET(str,strlist)。 假如字符串str 在由N 子链组成的字符串列表strlist 中，则返回值的范围在 1 到 N 之间。 一个字符串列表就是一个由一些被‘,’符号分开的子链组成的字符串。如果第一个参数是一个常数字符串，而第二个是type SET列，则 FIND_IN_SET() 函数被优化，使用比特计算。 如果str不在strlist 或strlist 为空字符串，则返回值为 0 。如任意一个参数为NULL，则返回值为 NULL。这个函数在第一个参数包含一个逗号(‘,’)时将无法正常运行。 str也可以是变量，比如表中的一个字段。 虽然这样很好用，但问题是如果数据量大的情况下怎么办，性能会是问题么，手册上有说对find_in_set 做的优化，但在没有索引的情况下他的性能应该是个问题。","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/面试","date":"2021-07-20T05:09:35.002Z","updated":"2021-07-19T02:37:46.649Z","comments":true,"path":"2021/07/20/数据库/MySQL/面试/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E9%9D%A2%E8%AF%95/","excerpt":"","text":"为什么在写SQL语句时遵守最左前缀原则才能用到索引？不遵守就用不到索引？其底层工作机制是怎样的？ MySQL中写缓冲区为什么能优化写入的速度？如何做到的？ MySQL在执行一个SQL语句时会经过哪些步骤？这每个步骤可以如何优化？ 写了一个很长的SQL，这个SQL最终的执行顺序是怎样的？如何优化复杂SQL？ 到底多大数据的表才是大表？500万条？2000万条？5000万条？ 如果一个表中数据量很大，这个时候如何建立索引，如何优化索引？ 高并发场景下，使用MySQL事务时应该要注意哪些方面，如何进行优化？","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/开篇","date":"2021-07-20T05:09:34.998Z","updated":"2021-01-26T03:57:14.000Z","comments":true,"path":"2021/07/20/数据库/MySQL/开篇/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E5%BC%80%E7%AF%87/","excerpt":"","text":"基础篇1.基础架构：一条SQL查询语句是如何执行的？ Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数字和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 连接器 连接器负责跟客户端建立链接、获取权限、维持和管理链接。 如果长时间没有动静，连接器默认8小时会自动断开，再次发送请求需要进行重连。 数据库中长连接指的是连接成功后，如果客户端持续有请求，则一直使用同一个连接，短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 全部使用长连接，会非常占用内存，导致内存占用太大，被系统强行杀掉（OOM），导致MySQL异常重启。 解决方案： 1.定期断开连接，使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 2.MySQL5.7及以上版本，可以在每次执行一个比较大的操作后，通过执行mysql_reset_connection来重新初始化连接资源，这个过程不需要重连和重新做权限验证，但是会将连接回复到刚刚创建完的状态。 查询缓存 不建议使用查询缓存： 因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要对一个表的更新，这个表上所有的查询花奴才能都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低，除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 MySQL8.0已经移除该功能。 分析器 分析器会先做”词法分析”。你输入的是由多个字符喜欢和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。 然后是“语法分析”，根据词法分析的结果，语法分析器会根据语法规则，判断你输入的额这个NySQL语句是否满足MySQL语法。 优化器 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 开始执行的时候，会先判断你对这个表T哟没有执行查询的权限，如果没有，就会返回没有权限的错误（在工程上实现，如果命中查询缓存，就会在查询缓存返回结果的时候，做权限验证，查询也会在优化器之前调用precheck权限验证）。 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 2.日志系统：一条SQL更新语句是如何执行的？MySQL的逻辑架构图和查询一直，执行语句前要先连接数据库，这是连接器的工作。 一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空，这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引，然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块:redo log(重做日志)和bin log(归档日志)。 redo log(重做日志)在MySQL有一个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高，为了解决这个问题，MySQL的设计者采用WAL技术，全称 Write-Ahead Logging。它的关键点就是先写日志，再写磁盘。 具体俩说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 于此类似，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以自己录4GB操作。从头开始写，写到末尾就又回到开头循环写，如图所示。 write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的操作，得停下来先擦掉一些记录，把checkpoint推进一下。 有了redo log，InnoDB就可以保存即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 binlog(归档日志)MySQL整体来看，分为两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块就是引擎层，负责存储相关的具体事宜。redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog(归档日志)。 为什么会有两个日志 因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档，而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统-也就是redo log来实现crash-safe能力。 两种日志的区别 1.redo log是InnoDB引擎特有的；binlog是MySQL的server层实现的，所有引擎都可以使用。 2.redo log是物理日志，记录的是“在某个数据页上做了什么修改”;binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2 这一行的c字段加 1”。 3.redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 执行器和InnoDB引擎在执行这个简单Update语句时的内部流程 1.执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在是的数据项本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 2.执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 3.引擎将这行新数据更新到内存中，同时将这个更新数据操作记录到redo log里面，此时redo log处于prepare状态，然后告知执行器执行完成了，随时可以提交事务。 4.执行器生成这个操作的binlog，并把binlog写入磁盘。 5.执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 两阶段提交为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起： 怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的 update 语句来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 你可能会说，这个概率是不是很低，平时也没有什么动不动就需要恢复临时库的场景呀？ 其实不是的，不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 3.事务隔离：为什么你改了我还看不见？简单来说，事务就是保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。MyISAM就不支持事务，InnoDB支持。 隔离性与隔离级别提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（readuncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释： 1.读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 2.读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 3.可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 4.串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。 在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数tranaction-isolation的值设置成READ-COMMITTED。你可以用show variables来查看当前的值。 总结来说，哪个隔离级别都有它自己的使用场景，要根据业务情况来定。 事务隔离的实现在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作，记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。 基于上面的说明，建议不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在MySQL5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里，即使长事务最终提交，回滚段被清理，文件也不会变小。有数据只有20GB，而回滚段有200GB的库，最终只好为了清理回滚段，重建整个库。 除了对回滚段的印象，长事务还占用锁资源，也可能拖垮整个库。 事务的启动方式如前面所述，长事务有这些潜在风险，建议尽量避免，其实很多时候业务开发不是有意使用长事务，而是由于误用所致。 MySQL的事务启动方式有一下几种： 1.显示启动事务语句，begin或start transaction。配套的提交语句是commit，回滚语句是rollback。 2.set autocommit=0，这命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会主动提交，这个事务持续存到知道你主动执行commit或rollback语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个set autocommit=0命令，这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，建议总是使用set autocommit=1，通过显式语句的方式来启动事务。 在autocommit=1的情况下，用begin显式启动的事务。 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。 如果你也有这个顾虑，我建议你使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 如何避免长事务对业务的影响？首先，从应用开发端来看： 1.确认是否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志 来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它 改成 1。 2.确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框 起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。 这种只读事务可以去掉。 3.业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命 令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 其次，从数据库端来看： 1.监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill； 2.Percona 的 pt-kill 这个工具不错，推荐使用； 3.在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题； 4.如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方 便。 4.深入浅出索引（上）索引的出现其实是为了提高数据查询的效率，就像书的目录一样。 索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却又很多种，所以这里也就引入了索引模型的概念，可以用于提高读写效率的数据结构很多，比较常见、简单的数据结构分别是哈希表、有序数组和索引树。 从使用的角度，简单分析这三种模型的区别。 哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找到其对应的值即value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。 不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示： 图中，User2 和 User4 根据身份证号算出来的值都是 N，但没关系，后面还跟了一个链 表。假设，这时候你要查 ID_card_n2 对应的名字是什么，处理步骤就是：首先，将 ID_card_n2 通过哈希函数算出 N；然后，按顺序遍历，找到 User2。 需要注意的是，图中四个 ID_card_n 的值并不是递增的，这样做的好处是增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询 的速度是很慢的。 你可以设想下，如果你现在要找身份证号在 [ID_card_X, ID_card_Y] 这个区间的所有用 户，就必须全部扫描一遍了。 所以，哈希表这种结构适用于只有等值查询的场景，比如 Memcached 及其他一些 NoSQL 引擎。 而有序数组在等值查询和范围查询场景中的性能就都非常优秀。还是上面这个根据身份证 号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示： 这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候 如果你要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。 同时很显然，这个索引结构支持范围查询。你要查身份证号在 [ID_card_X, ID_card_Y] 区 间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证 号，退出循环。 如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就 麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。 所以，有序数组索引只适用于静态存储引擎，比如你要保存的是 2017 年某个城市的所有 人口信息，这类不会再修改的数据。 二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我 们用二叉搜索树来实现的话，示意图如下所示： 二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你 要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -&gt; UserC -&gt; UserF -&gt; User2 这个路径得到。这个时间复杂度是 O(log(N))。 当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个 保证，更新的时间复杂度也是 O(log(N))。 树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从 左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉 树。其原因是，索引不止存在内存中，还要写到磁盘上。 为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就 不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块 的大小。 N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引 擎中了。 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引 擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实 现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就 以 InnoDB 为例，和你分析一下其中的索引模型。 InnoDB 的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组 织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 每一个索引在 InnoDB 里面对应一棵 B+ 树。 假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树 的示例示意图如下。 从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引 （clustered index）。 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引 （secondary index）。 基于主键索引和普通索引的查询有什么 区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引 树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量 使用主键查询。 索引维护B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例， 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请 一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。 当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合 并。合并的过程，可以认为是分裂过程的逆过程。 基于上面的索引维护过程说明，我们来讨论一个案例： 你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自 增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而 哪些场景下不应该。 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。 也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插 入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字 段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级 索引的叶子节点占用约 20 个字节，而如果用整型做主键，则只要 4 个字节，如果是长整 型（bigint）则是 8 个字节。 显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求 是这样的： 只有一个索引； 该索引必须是唯一索引。 你一定看出来了，这就是典型的 KV 场景。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。 这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置 为主键，可以避免每次查询需要搜索两棵树。 总结InnoDB采用B+树结构，因为B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问数。 由于InnoDB是索引组织表，一般情况下建议创建一个自增组件，这样非主键索引占用的空间最小，但是事无绝对，根据业务场景也可以使用业务逻辑字段做主键。 5.深入浅出索引（下）6.全局锁和表锁：给表加个字段怎么有这么多阻碍？数据库锁设计的初衷是处理并发问题，作为多用户共享的资源，当出现并发访问的时候，数据需要合理地控制资源的访问规则，而锁就是用来实现这些访问规则的重要数据结构。 根据加锁的范围，MySQL里面的锁大致可以分为全局锁、表级锁和行锁三类。 全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是Flush tables with read lock(FTWRL)。当你需要整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。 以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做 备份。注意，在备份过程中整个库完全处于只读状态。 但是让整库都只读，听上去就很危险： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从 延迟。 不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑 不一致的。其实是有一个方法能够拿到一 致性视图的，就是在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持， 这个过程中数据是可以正常更新的。 你一定在疑惑，有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎 要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有 更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了 不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员 使用 InnoDB 替代 MyISAM 的原因之一。 你也许会问，既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确 实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要 有两个原因： 1.在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库 还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 2.在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个 库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状 态，这样会导致整个库长时间处于不可写状态，风险较高。 业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作 （DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操 作，都是会被锁住的。 但是，即使没有被全局锁住，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们 要介绍的表级锁。 表级锁MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock， MDL)。 **表锁的语法是lock tables…read/write.**与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放，需要注意，lock tables语法除了会限制别的线程的读写外，也限制了本线程接下来的操作对象。 举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 在还没出现更细粒度的锁的时候，表锁是最常用的处理并发的方式，而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁MDL（metadata lock）。DML不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性，你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果更表结构对不上，肯定是不行的。 因此，在MySQL5.5版本中引入了MDL，当对一个表做增删查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变成表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然MDL锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，我经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。 你肯定知道，给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响，而实际上，即使是小表，操作不慎也会出问题。 我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需 要的也是 MDL 读锁，因此可以正常执行。 之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读 锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会 马上释放，而会等到整个事务提交后再释放。 基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请 求很频繁，而你不得不加个字段，你该怎么做呢？ 这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不 到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这 个过程。 MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。 12ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ... 小结全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用– single-transaction 参数，对应用会更友好。 表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。 MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。 最后，我给你留一个问题吧。备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一 列。这时候，从备库上会看到什么现象呢？ online ddl Online DDL的过程是这样的： 拿MDL写锁 降级成MDL读锁 真正做DDL 升级成MDL写锁 释放MDL锁 1、2、4、5如果没有锁冲突，执行时间非常短。第3步占用了DDL绝大部分时间，这期间这个表 可以正常读写数据，是因此称为“online ” 我们文中的例子，是在第一步就堵住了 7.行锁功过：怎么减少行锁对性能的影响？8.事务到底是隔离还是不隔离的？","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/实践篇3","date":"2021-07-20T05:09:34.997Z","updated":"2021-05-21T07:12:28.976Z","comments":true,"path":"2021/07/20/数据库/MySQL/实践篇3/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E5%AE%9E%E8%B7%B5%E7%AF%873/","excerpt":"","text":"33.我查这么多数据，会不会把数据内存打爆？34.到底可不可以使用join?在实际生产中，关于 join 语句使用的问题，一般会集中在以下两类： 我们 DBA 不让使用 join，使用 join 有什么问题呢？ 如果有两个大小不同的表做 join，应该用哪个表做驱动表呢？ Index Nested-Loop Join这个过程是先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2 中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。 35.join语句怎么优化？36.为什么临时表可以重名？37.什么时候会使用内部临时表？38.都说InnoDB好，那还要不要使用Memory引擎？39.自增主键为什么不是连续的？40.insert语句的锁为什么这么多？41.怎么最快地复制一张表？42.grant之后要跟着flush privileges吗？43.要不要使用分区表？44.45.递增id用完了怎么办？","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/实践篇2","date":"2021-07-20T05:09:34.995Z","updated":"2021-05-20T08:17:29.913Z","comments":true,"path":"2021/07/20/数据库/MySQL/实践篇2/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E5%AE%9E%E8%B7%B5%E7%AF%872/","excerpt":"","text":"21.为什么我只查一行的语句，锁这么多？22.MySQL有哪些“饮鸩止渴”提高性能的方法？23.MySQL是怎么保证数据不丢的？只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。 binlog的写入机制其实，binlog的写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。 一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了binlog cache的保存问题。 系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空binlog cache。状态如图 1 所示。 24.MySQL是怎么保证主备一致的？25.MySQL是怎么保证高可用的？26.备库为什么会延迟好几个小时？27.主库出问题了，从库怎么办？28.读写分离有哪些坑？29.如何判断一个数据库是不是出问题了?30.答疑文章（二）：用动态的观点看加锁31.误删数据后除了跑路，还能怎么办？32.为什么还有kill不掉的语句？","categories":[],"tags":[]},{"title":"","slug":"数据库/MySQL/实践篇1","date":"2021-07-20T05:09:34.993Z","updated":"2021-04-08T01:16:29.938Z","comments":true,"path":"2021/07/20/数据库/MySQL/实践篇1/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E5%AE%9E%E8%B7%B5%E7%AF%871/","excerpt":"","text":"实践篇9.普通索引和唯一索引，应该怎么选择？10.MySQL为什么有时候会选错索引？11.怎么给字符串字段加索引？12.为什么我的MySQL会“抖”一下？13.为什么表数据删除一半，表文件大小不变？14.count(*)这么慢，我改怎么办？count(*)的实现方式你首先要明确的是，在不同的 MySQL 引擎中，count(*) 有不同的实现方式。 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 15.日志和索引相关问题16.“order by”是怎么工作的？17.如何正确地显示随机消息？18.为什么这些SQL语句逻辑相同，性能却差异巨大？19.为什么我只查一行的语句，也执行这么慢？20.幻读是什么，幻读有什么问题？","categories":[],"tags":[]},{"title":"","slug":"搜索引擎/搜索引擎-Lucene","date":"2021-07-20T05:09:34.986Z","updated":"2020-05-11T07:00:28.000Z","comments":true,"path":"2021/07/20/搜索引擎/搜索引擎-Lucene/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-Lucene/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"搜索引擎/ES倒排索引","date":"2021-07-20T05:09:34.982Z","updated":"2020-08-20T02:19:26.000Z","comments":true,"path":"2021/07/20/搜索引擎/ES倒排索引/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/ES%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/","excerpt":"","text":"面试题ES 写入数据的工作原理是什么啊？ES 查询数据的工作原理是什么啊？底层的 Lucene 介绍一下呗？倒排索引了解吗？ 面试官心理分析问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的是…… 对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了。要是出点什么问题，你啥都不知道，那还能指望你什么呢？ 面试题剖析es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es-write es 读数据过程可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node 。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node 。 coordinate node 返回 document 给客户端。 es 搜索数据过程es 最强大的是做全文检索，就是比如你有三条数据： 123java真好玩儿啊java好难学啊j2ee特别牛Copy to clipboardErrorCopied 你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。 客户端发送请求到一个 coordinate node 。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理es-write-detail 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除/更新数据底层原理如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。 底层 lucene简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1, 2, 3, 4, 5 2 地图 1, 2, 3, 4, 5 3 之父 1, 2, 4, 5 4 跳槽 1, 4 5 Facebook 1, 2, 3, 4, 5 6 加盟 2, 3, 5 7 创始人 3 8 拉斯 3, 5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的例子，并没有严格按照字典顺序升序排列。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/浅谈JavaScript的防抖与节流","date":"2021-07-20T05:09:34.916Z","updated":"2021-05-21T09:43:28.928Z","comments":true,"path":"2021/07/20/大前端/other/浅谈JavaScript的防抖与节流/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E6%B5%85%E8%B0%88JavaScript%E7%9A%84%E9%98%B2%E6%8A%96%E4%B8%8E%E8%8A%82%E6%B5%81/","excerpt":"","text":"前言在前端开发的过程中，我们经常会需要绑定一些持续触发的事件，如 resize、scroll、mousemove 等等，但有些时候我们并不希望在事件持续触发的过程中那么频繁地去执行函数。这时候就用到防抖与节流。 函数防抖（debounce）：当持续触发事件时，一定时间段内没有再触发事件，事件处理函数才会执行一次，如果设定的时间到来之前，又一次触发了事件，就重新开始延时。如下图，持续触发scroll事件时，并不执行handle函数，当1000毫秒内没有触发scroll事件时，才会延时触发scroll事件。 非立即执行版 1234567891011function debounce(func, wait) &#123; let timeout; return function () &#123; let context = this; let args = arguments; if (timeout) clearTimeout(timeout); timeout = setTimeout(() =&gt; &#123; func.apply(context, args) &#125;, wait); &#125;&#125; 非立即执行版的意思是触发事件后函数不会立即执行，而是在 n 秒后执行，如果在 n 秒内又触发了事件，则会重新计算函数执行时间。 立即执行版 12345678910111213function debounce(func,wait) &#123; let timeout; return function () &#123; let context = this; let args = arguments; if (timeout) clearTimeout(timeout); let callNow = !timeout; timeout = setTimeout(() =&gt; &#123; timeout = null; &#125;, wait) if (callNow) func.apply(context, args) &#125;&#125; 立即执行版的意思是触发事件后函数会立即执行，然后n秒内不触发事件才能继续执行函数的效果。 防抖应用场景 搜索框输入查询，如果用户一直在输入中，没有必要不停地调用去请求服务器接口，等用户停止输入的时候，再调用，设置一个合适的时间间隔，有效减轻服务器压力； 表单验证； 按钮提交事件； 浏览器窗口缩放，resize事件（如窗口停止改变大小之后重新计算布局）等。 函数节流（throttle）：当持续触发事件时，保证一定时间段内只调用一次事件处理函数。节流通俗解释就比如我们水龙头放水，阀门一打开，水哗哗的往下流，秉着勤俭节约的优良传统美德，我们要把水龙头关小点，最好是如我们心意按照一定规律在某个时间间隔内一滴一滴的往下滴。如下图，持续触发scroll事件时，并不立即执行handle函数，每隔1000毫秒才会执行一次handle函数。函数节流主要有两种实现方法：时间戳和定时器 时间戳版: 123456789101112function throttle(func,wait)&#123; let previous = 0; return function()&#123; let now = Date.now(); let context = this; let args = arguments; if(now - previous &gt; wait)&#123; func.apply(context,args); previous = now; &#125; &#125;&#125; 定时器版: 12345678910111213function throttle(func, wait) &#123; let timeout; return function() &#123; let context = this; let args = arguments; if (!timeout) &#123; timeout = setTimeout(() =&gt; &#123; timeout = null; func.apply(context, args) &#125;, wait) &#125; &#125;&#125; 节流应用场景 按钮点击事件； 拖拽事件； onScroll； 计算鼠标移动的距离（mouseover） 总结 函数防抖：将几次操作合并为一次操作进行。原理是维护一个计时器，规定在delay时间后触发函数，但是在delay时间内再次触发的话，就会取消之前的计时器而重新设置。这样一来，只有最后一次操作能被触发。 函数节流：使得一次时间内只触发一次函数。原理是通过判断是否达到一定时间来触发函数。 区别函数节流不管事件触发有多频繁，都会保证在规定时间内一定会执行一次真正的时间处理函数，而函数防抖只是在最后一次事件后才触发一次函数。比如在页面的无限加载场景下，我们需要用户在滚动页面时，每隔一段时间发一次Ajax请求，而不是在用户停下滚动页面操作时才去请求数据。这样的场景，就适合用节流技术来实现。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/杂","date":"2021-07-20T05:09:34.915Z","updated":"2021-03-16T02:08:38.491Z","comments":true,"path":"2021/07/20/大前端/other/杂/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E6%9D%82/","excerpt":"","text":"12345678910111213active = &#123; reload: function()&#123; //执行重载 table.reload(&#x27;table_id_base&#x27;, &#123; page: &#123; curr: 1 //重新从第 1 页开始 &#125; ,where: &#123; base_name: $(&#x27;#basename&#x27;).val() &#125; &#125;, &#x27;data&#x27;); &#125;, &#125; 12var type = $(this).data(&#x27;type&#x27;); active[type] ? active[type].call(this) : &#x27;&#x27;; $(this).data() 获取自定义属性 $(this).data(‘type’) data-type 对象名[方法名].call(this) 调用对应方法","categories":[],"tags":[]},{"title":"","slug":"大前端/other/前端路由、后端路由、单页面应用、多页面应用","date":"2021-07-20T05:09:34.913Z","updated":"2021-05-24T01:42:53.794Z","comments":true,"path":"2021/07/20/大前端/other/前端路由、后端路由、单页面应用、多页面应用/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E5%89%8D%E7%AB%AF%E8%B7%AF%E7%94%B1%E3%80%81%E5%90%8E%E7%AB%AF%E8%B7%AF%E7%94%B1%E3%80%81%E5%8D%95%E9%A1%B5%E9%9D%A2%E5%BA%94%E7%94%A8%E3%80%81%E5%A4%9A%E9%A1%B5%E9%9D%A2%E5%BA%94%E7%94%A8/","excerpt":"","text":"前端路由 定义：在单页面应用，大部分页面结构不变，只改变部分内容的使用 优点：用户体验好，不需要每次都从服务器全部获取，快速展示给用户 缺点：使用浏览器的前进，后退键的时候会重新发送请求，没有合理地利用缓存。单页面无法记住之前滚动的位置，无法在前进，后退的时候记住滚动的位置。 后端路由通过用户请求的url导航到具体的html页面；每跳转到不同的URL，都是重新访问服务端，然后服务端返回页面，页面也可以是服务端获取数据，然后和模板组合，返回HTML，也可以是直接返回模板HTML，然后由前端js再去请求数据，使用前端模板和数据进行组合，生成想要的HTML 前后端路由对比 从性能和用户体验的层面来比较的话，后端路由每次访问一个新页面的时候都要向服务器发送请求，然后服务器再响应请求，这个过程肯定会有延迟。而前端路由在访问一个新页面的时候仅仅是变换了一下路径而已，没有了网络延迟，对于用户体验来说会有相当大的提升。 在某些场合中，用ajax请求，可以让页面无刷新，页面变了但Url没有变化，用户就不能复制到想要的地址，用前端路由做单页面网页就很好的解决了这个问题。但是前端路由使用浏览器的前进，后退键的时候会重新发送请求，没有合理地利用缓存。 单页面的优势 不存在页面切换问题，因为只在同一个页面间切换，会更流畅，而且可以附加各种动画和过度效果，用户体验更好。 可以用到vue的路由和状态保持，不用担心切换造成的数据不同步。 打包方便，有现成的脚手架可以用，也比较不容易出问题 只有一张Web页面的应用，是一种从Web服务器加载的富客户端，单页面跳转仅刷新局部资源 ，公共资源(js、css等)仅需加载一次 页面跳转：使用js中的append/remove或者show/hide的方式来进行页面内容的更换； 数据传递：可通过全局变量或者参数传递，进行相关数据交互 使用场景： 适用于高度追求高度支持搜索引擎的应用 多页面的优势 逻辑清楚，各个页面按照功能和逻辑划分，不用担心业务复杂度 单个页面体积较小，加载速度比较有保证 多页面跳转需要刷新所有资源，每个公共资源(js、css等)需选择性重新加载 页面跳转：使用window.location.href = “./index.html”进行页面间的跳转； 数据传递：可以使用path?account=”123”&amp;password=””路径携带数据传递的方式，或者localstorage、cookie等存储方式 使用场景： 高要求的体验度，追求界面流畅的应用 多页面的劣势 重复代码较多 页面经常需要切换，切换效果取决于浏览器和网络情况，对用户体验会有一定负面影响 无法充分利用vue的路由和状态保持，在多个页面之间共享和同步数据状态会成为一个难题 hash 模式这里的 hash 就是指 url 后的 # 号以及后面的字符。比如说 “www.baidu.com/#hashhash&quot; ，其中 “#hashhash” 就是我们期望的 hash 值。 由于 hash 值的变化不会导致浏览器像服务器发送请求，而且 hash 的改变会触发 hashchange 事件，浏览器的前进后退也能对其进行控制，所以在 H5 的 history 模式出现之前，基本都是使用 hash 模式来实现前端路由。 12345// 监听hash变化，点击浏览器的前进后退会触发window.addEventListener(&#x27;hashchange&#x27;, function(event)&#123; let newURL = event.newURL; // hash 改变后的新 url let oldURL = event.oldURL; // hash 改变前的旧 url&#125;,false) 下面实现一个路由对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class HashRouter&#123; constructor()&#123; //用于存储不同hash值对应的回调函数 this.routers = &#123;&#125;; window.addEventListener(&#x27;hashchange&#x27;,this.load.bind(this),false) &#125; //用于注册每个视图 register(hash,callback = function()&#123;&#125;)&#123; this.routers[hash] = callback; &#125; //用于注册首页 registerIndex(callback = function()&#123;&#125;)&#123; this.routers[&#x27;index&#x27;] = callback; &#125; //用于处理视图未找到的情况 registerNotFound(callback = function()&#123;&#125;)&#123; this.routers[&#x27;404&#x27;] = callback; &#125; //用于处理异常情况 registerError(callback = function()&#123;&#125;)&#123; this.routers[&#x27;error&#x27;] = callback; &#125; //用于调用不同视图的回调函数 load()&#123; let hash = location.hash.slice(1), handler; //没有hash 默认为首页 if(!hash)&#123; handler = this.routers.index; &#125; //未找到对应hash值 else if(!this.routers.hasOwnProperty(hash))&#123; handler = this.routers[&#x27;404&#x27;] || function()&#123;&#125;; &#125; else&#123; handler = this.routers[hash] &#125; //执行注册的回调函数 try&#123; handler.apply(this); &#125;catch(e)&#123; console.error(e); (this.routers[&#x27;error&#x27;] || function()&#123;&#125;).call(this,e); &#125; &#125;&#125; 再来一个例子 12345678910&lt;body&gt; &lt;div id=&quot;nav&quot;&gt; &lt;a href=&quot;#/page1&quot;&gt;page1&lt;/a&gt; &lt;a href=&quot;#/page2&quot;&gt;page2&lt;/a&gt; &lt;a href=&quot;#/page3&quot;&gt;page3&lt;/a&gt; &lt;a href=&quot;#/page4&quot;&gt;page4&lt;/a&gt; &lt;a href=&quot;#/page5&quot;&gt;page5&lt;/a&gt; &lt;/div&gt; &lt;div id=&quot;container&quot;&gt;&lt;/div&gt;&lt;/body&gt; 12345678910111213141516171819let router = new HashRouter();let container = document.getElementById(&#x27;container&#x27;);//注册首页回调函数router.registerIndex(()=&gt; container.innerHTML = &#x27;我是首页&#x27;);//注册其他视图回到函数router.register(&#x27;/page1&#x27;,()=&gt; container.innerHTML = &#x27;我是page1&#x27;);router.register(&#x27;/page2&#x27;,()=&gt; container.innerHTML = &#x27;我是page2&#x27;);router.register(&#x27;/page3&#x27;,()=&gt; container.innerHTML = &#x27;我是page3&#x27;);router.register(&#x27;/page4&#x27;,()=&gt; &#123;throw new Error(&#x27;抛出一个异常&#x27;)&#125;);//加载视图router.load();//注册未找到对应hash值时的回调router.registerNotFound(()=&gt;container.innerHTML = &#x27;页面未找到&#x27;);//注册出现异常时的回调router.registerError((e)=&gt;container.innerHTML = &#x27;页面异常，错误消息：&lt;br&gt;&#x27; + e.message); history 模式在 HTML5 之前，浏览器就已经有了 history 对象。但在早期的 history 中只能用于多页面的跳转： 1234history.go(-1); // 后退一页history.go(2); // 前进两页history.forward(); // 前进一页history.back(); // 后退一页 在 HTML5 的规范中，history 新增了以下几个 API 123history.pushState(); // 添加新的状态到历史状态栈history.replaceState(); // 用新的状态代替当前状态history.state // 返回当前状态对象 由于 history.pushState() 和 history.replaceState() 可以改变 url 同时，不会刷新页面，所以在 HTML5 中的 histroy 具备了实现前端路由的能力。 对于单页应用的 history 模式而言，url 的改变只能由下面四种方式引起： 点击浏览器的前进或后退按钮 点击 a 标签 在 JS 代码中触发 history.pushState 函数 在 JS 代码中触发 history.replaceState 函数 下面实现一个路由对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475class HistoryRouter&#123; constructor()&#123; //用于存储不同path值对应的回调函数 this.routers = &#123;&#125;; this.listenPopState(); this.listenLink(); &#125; //监听popstate listenPopState()&#123; window.addEventListener(&#x27;popstate&#x27;,(e)=&gt;&#123; let state = e.state || &#123;&#125;, path = state.path || &#x27;&#x27;; this.dealPathHandler(path) &#125;,false) &#125; //全局监听A链接 listenLink()&#123; window.addEventListener(&#x27;click&#x27;,(e)=&gt;&#123; let dom = e.target; if(dom.tagName.toUpperCase() === &#x27;A&#x27; &amp;&amp; dom.getAttribute(&#x27;href&#x27;))&#123; e.preventDefault() this.assign(dom.getAttribute(&#x27;href&#x27;)); &#125; &#125;,false) &#125; //用于首次进入页面时调用 load()&#123; let path = location.pathname; this.dealPathHandler(path) &#125; //用于注册每个视图 register(path,callback = function()&#123;&#125;)&#123; this.routers[path] = callback; &#125; //用于注册首页 registerIndex(callback = function()&#123;&#125;)&#123; this.routers[&#x27;/&#x27;] = callback; &#125; //用于处理视图未找到的情况 registerNotFound(callback = function()&#123;&#125;)&#123; this.routers[&#x27;404&#x27;] = callback; &#125; //用于处理异常情况 registerError(callback = function()&#123;&#125;)&#123; this.routers[&#x27;error&#x27;] = callback; &#125; //跳转到path assign(path)&#123; history.pushState(&#123;path&#125;,null,path); this.dealPathHandler(path) &#125; //替换为path replace(path)&#123; history.replaceState(&#123;path&#125;,null,path); this.dealPathHandler(path) &#125; //通用处理 path 调用回调函数 dealPathHandler(path)&#123; let handler; //没有对应path if(!this.routers.hasOwnProperty(path))&#123; handler = this.routers[&#x27;404&#x27;] || function()&#123;&#125;; &#125; //有对应path else&#123; handler = this.routers[path]; &#125; try&#123; handler.call(this) &#125;catch(e)&#123; console.error(e); (this.routers[&#x27;error&#x27;] || function()&#123;&#125;).call(this,e); &#125; &#125;&#125; 再来一个例子 1234567891011121314&lt;body&gt; &lt;div id=&quot;nav&quot;&gt; &lt;a href=&quot;/page1&quot;&gt;page1&lt;/a&gt; &lt;a href=&quot;/page2&quot;&gt;page2&lt;/a&gt; &lt;a href=&quot;/page3&quot;&gt;page3&lt;/a&gt; &lt;a href=&quot;/page4&quot;&gt;page4&lt;/a&gt; &lt;a href=&quot;/page5&quot;&gt;page5&lt;/a&gt; &lt;button id=&quot;btn&quot;&gt;page2&lt;/button&gt; &lt;/div&gt; &lt;div id=&quot;container&quot;&gt; &lt;/div&gt;&lt;/body&gt; 123456789101112131415161718192021222324let router = new HistoryRouter();let container = document.getElementById(&#x27;container&#x27;);//注册首页回调函数router.registerIndex(() =&gt; container.innerHTML = &#x27;我是首页&#x27;);//注册其他视图回到函数router.register(&#x27;/page1&#x27;, () =&gt; container.innerHTML = &#x27;我是page1&#x27;);router.register(&#x27;/page2&#x27;, () =&gt; container.innerHTML = &#x27;我是page2&#x27;);router.register(&#x27;/page3&#x27;, () =&gt; container.innerHTML = &#x27;我是page3&#x27;);router.register(&#x27;/page4&#x27;, () =&gt; &#123; throw new Error(&#x27;抛出一个异常&#x27;)&#125;);document.getElementById(&#x27;btn&#x27;).onclick = () =&gt; router.assign(&#x27;/page2&#x27;)//注册未找到对应path值时的回调router.registerNotFound(() =&gt; container.innerHTML = &#x27;页面未找到&#x27;);//注册出现异常时的回调router.registerError((e) =&gt; container.innerHTML = &#x27;页面异常，错误消息：&lt;br&gt;&#x27; + e.message);//加载页面router.load();","categories":[],"tags":[]},{"title":"","slug":"大前端/other/什么是回流，什么是重绘，有什么区别？","date":"2021-07-20T05:09:34.909Z","updated":"2021-04-16T05:24:14.821Z","comments":true,"path":"2021/07/20/大前端/other/什么是回流，什么是重绘，有什么区别？/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%9E%E6%B5%81%EF%BC%8C%E4%BB%80%E4%B9%88%E6%98%AF%E9%87%8D%E7%BB%98%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F/","excerpt":"","text":"html 加载时发生了什么在页面加载时，浏览器把获取到的HTML代码解析成1个DOM树，DOM树里包含了所有HTML标签，包括display:none隐藏，还有用JS动态添加的元素等。 浏览器把所有样式(用户定义的CSS和用户代理)解析成样式结构体 DOM Tree 和样式结构体组合后构建render tree, render tree类似于DOM tree，但区别很大，因为render tree能识别样式，render tree中每个NODE都有自己的style，而且render tree不包含隐藏的节点(比如display:none的节点，还有head节点)，因为这些节点不会用于呈现，而且不会影响呈现的，所以就不会包含到 render tree中。我自己简单的理解就是DOM Tree和我们写的CSS结合在一起之后，渲染出了render tree。 什么是回流当render tree中的一部分(或全部)因为元素的规模尺寸，布局，隐藏等改变而需要重新构建。这就称为回流(reflow)。每个页面至少需要一次回流，就是在页面第一次加载的时候，这时候是一定会发生回流的，因为要构建render tree。在回流的时候，浏览器会使渲染树中受到影响的部分失效，并重新构造这部分渲染树，完成回流后，浏览器会重新绘制受影响的部分到屏幕中，该过程成为重绘。 什么是重绘当render tree中的一些元素需要更新属性，而这些属性只是影响元素的外观，风格，而不会影响布局的，比如background-color。则就叫称为重绘。 区别：他们的区别很大： 回流必将引起重绘，而重绘不一定会引起回流。比如：只有颜色改变的时候就只会发生重绘而不会引起回流 当页面布局和几何属性改变时就需要回流 比如：添加或者删除可见的DOM元素，元素位置改变，元素尺寸改变——边距、填充、边框、宽度和高度，内容改变 扩展：浏览器的帮忙所以我们能得知回流比重绘的代价要更高，回流的花销跟render tree有多少节点需要重新构建有关系 因为这些机制的存在，所以浏览器会帮助我们优化这些操作，浏览器会维护1个队列，把所有会引起回流、重绘的操作放入这个队列，等队列中的操作到了一定的数量或者到了一定的时间间隔，浏览器就会flush队列，进行一个批处理。这样就会让多次的回流、重绘变成一次回流重绘。 自己的优化但是靠浏览器不如靠自己，我们可以改变一些写法减少回流和重绘 比如改变样式的时候，不去改变他们每个的样式，而是直接改变className 就要用到cssText 但是要注意有一个问题，会把原有的cssText清掉，比如原来的style中有’display:none;’，那么执行完上面的JS后，display就被删掉了。 为了解决这个问题，可以采用cssText累加的方法，但是IE不支持累加，前面添一个分号可以解决。 还有添加节点的时候比如要添加一个div里面有三个子元素p，如果添加div再在里面添加三次p，这样就触发很多次回流和重绘，我们可以用cloneNode(true or false) 来避免，一次把要添加的都克隆好再appened就好了，还有其他很多的方法就不一一说了","categories":[],"tags":[]},{"title":"","slug":"大前端/other/事件委托","date":"2021-07-20T05:09:34.907Z","updated":"2021-07-19T02:12:42.160Z","comments":true,"path":"2021/07/20/大前端/other/事件委托/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E4%BA%8B%E4%BB%B6%E5%A7%94%E6%89%98/","excerpt":"","text":"基本概念事件代理（Event Delegation），又称之为事件委托。是JavaScript中常用绑定事件的常用技巧。顾名思义，“事件代理”即是把原本需要绑定在子元素的响应事件（click、keydown……）委托给父元素，让父元素担当事件监听的职务。事件代理的原理是DOM元素的事件冒泡。 举个通俗的例子 比如一个宿舍的同学同时快递到了，一种方法就是他们一个个去领取，还有一种方法就是把这件事情委托给宿舍长，让一个人出去拿好所有快递，然后再根据收件人一 一分发给每个宿舍同学； 在这里，取快递就是一个事件，每个同学指的是需要响应事件的 DOM 元素，而出去统一领取快递的宿舍长就是代理的元素，所以真正绑定事件的是这个元素，按照收件人分发快递的过程就是在事件执行中，需要判断当前响应的事件应该匹配到被代理元素中的哪一个或者哪几个。 事件冒泡前面提到事件委托的原理是DOM元素的事件冒泡，那么事件冒泡是什么呢？ 一个事件触发后，会在子元素和父元素之间传播（propagation）。这种传播分成三个阶段 如上图所示，事件传播分成三个阶段： 捕获阶段：从window对象传导到目标节点（上层传到底层）称为“捕获阶段”（capture phase），捕获阶段不会响应任何事件； 目标阶段：在目标节点上触发，称为“目标阶段” 冒泡阶段：从目标节点传导回window对象（从底层传回上层），称为“冒泡阶段”（bubbling phase）。事件代理即是利用事件冒泡的机制把里层所需要响应的事件绑定到外层； 事件委托的优点【1】可以大量节省内存占用，减少事件注册，比如在ul上代理所有li的click事件就非常棒 1234567&lt;ul id=&quot;list&quot;&gt; &lt;li&gt;item 1&lt;/li&gt; &lt;li&gt;item 2&lt;/li&gt; &lt;li&gt;item 3&lt;/li&gt; ...... &lt;li&gt;item n&lt;/li&gt;&lt;/ul&gt; // …… 代表中间还有未知数个 li如上面代码所示，如果给每个li列表项都绑定一个函数，那对内存的消耗是非常大的，因此较好的解决办法就是将li元素的点击事件绑定到它的父元素ul身上，执行事件的时候再去匹配判断目标元素。 【2】可以实现当新增子对象时无需再次对其绑定（动态绑定事件） 假设上述的例子中列表项li就几个，我们给每个列表项都绑定了事件； 在很多时候，我们需要通过 AJAX 或者用户操作动态的增加或者删除列表项li元素，那么在每一次改变的时候都需要重新给新增的元素绑定事件，给即将删去的元素解绑事件； 如果用了事件委托就没有这种麻烦了，因为事件是绑定在父层的，和目标元素的增减是没有关系的，执行到目标元素是在真正响应执行事件函数的过程中去匹配的；所以使用事件在动态绑定事件的情况下是可以减少很多重复工作的。 基本实现【1】JavaScript原生实现事件委托 比如我们有这样的一个 HTML 片段： Go somewhere Do something Say hi 按照传统的做法，需要像下面这样为它们添加 3 个事 件处理程序 12345678910111213var item1 = document.getElementById(&quot;goSomewhere&quot;);var item2 = document.getElementById(&quot;doSomething&quot;);var item3 = document.getElementById(&quot;sayHi&quot;); item1.onclick = function() &#123; location.href = &quot;http://www.baidu.com&quot;;&#125;;item2.onclick = function() &#123; document.title = &quot;事件委托&quot;;&#125;;item3.onclick = function() &#123; alert(&quot;hi&quot;);&#125;; 如果在一个复杂的 Web 应用程序中，对所有可单击的元素都采用这种方式，那么结果就会有数不 清的代码用于添加事件处理程序。此时，可以利用事件委托技术解决这个问题。使用事件委托，只需在 DOM 树中尽量最高的层次上添加一个事件处理程序，如下面的例子所示 1234567891011121314151617var item1 = document.getElementById(&quot;goSomewhere&quot;);var item2 = document.getElementById(&quot;doSomething&quot;);var item3 = document.getElementById(&quot;sayHi&quot;); document.addEventListener(&quot;click&quot;, function (event) &#123; var target = event.target; switch (target.id) &#123; case &quot;doSomething&quot;: document.title = &quot;事件委托&quot;; break; case &quot;goSomewhere&quot;: location.href = &quot;http://www.baidu.com&quot;; break; case &quot;sayHi&quot;: alert(&quot;hi&quot;); break; &#125;&#125;) 【2】jQuery事件delegate()实现事件委托 delegate() 方法为指定的元素（属于被选元素的子元素）添加一个或多个事件处理程序，并规定当这些事件发生时运行的函数。 格式：$(selector).delegate(childSelector, event, data, function) 参数 描述childSelector 必需，规定要附加事件处理程序的一个或多个子元素。event必需，规定附加到元素的一个或多个事件。由空格分隔多个事件值。必须是有效的事件。 data 可选，规定传递到函数的额外数据。function 必需，规定当事件发生时运行的函数。 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;script src=&quot;http://lib.sinaapp.com/js/jquery/2.0.2/jquery-2.0.2.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;ul id=&quot;myLinks&quot;&gt; &lt;li id=&quot;goSomewhere&quot;&gt;Go somewhere&lt;/li&gt; &lt;li id=&quot;doSomething&quot;&gt;Do something&lt;/li&gt; &lt;li id=&quot;sayHi&quot;&gt;Say hi&lt;/li&gt; &lt;/ul&gt; &lt;script&gt; $(document).ready(function () &#123; $(&quot;#myLinks&quot;).delegate(&quot;#goSomewhere&quot;, &quot;click&quot;, function () &#123; location.href = &quot;http://www.baidu.com&quot;; &#125;); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 使用事件委托注意事项 使用“事件委托”时，并不是说把事件委托给的元素越靠近顶层就越好。事件冒泡的过程也需要耗时，越靠近顶层，事件的”事件传播链”越长，也就越耗时。如果DOM嵌套结构很深，事件冒泡通过大量祖先元素会导致性能损失。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/yarn的安装和使用","date":"2021-07-20T05:09:34.905Z","updated":"2021-05-17T01:15:16.791Z","comments":true,"path":"2021/07/20/大前端/other/yarn的安装和使用/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/yarn%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"","text":"yarn的简介：Yarn是facebook发布的一款取代npm的包管理工具。 yarn的特点：速度超快。Yarn 缓存了每个下载过的包，所以再次使用时无需重复下载。 同时利用并行下载以最大化资源利用率，因此安装速度更快。超级安全。在执行代码之前，Yarn 会通过算法校验每个安装包的完整性。超级可靠。使用详细、简洁的锁文件格式和明确的安装算法，Yarn 能够保证在不同系统上无差异的工作。yarn的安装:下载node.js，使用npm安装npm install -g yarn查看版本：yarn –version安装node.js,下载yarn的安装程序:提供一个.msi文件，在运行时将引导您在Windows上安装YarnYarn 淘宝源安装，分别复制粘贴以下代码行到黑窗口运行即可yarn config set registry https://registry.npm.taobao.org -gyarn config set sass_binary_site http://cdn.npm.taobao.org/dist/node-sass -gyarn的常用命令：安装yarn npm install -g yarn安装成功后，查看版本号： yarn –version创建文件夹 yarn md yarn进入yarn文件夹 cd yarn初始化项目 yarn init // 同npm init，执行输入信息后，会生成package.json文件yarn的配置项： yarn config list // 显示所有配置项yarn config get //显示某配置项yarn config delete //删除某配置项yarn config set [-g|–global] //设置配置项安装包： yarn install //安装package.json里所有包，并将包及它的所有依赖项保存进yarn.lockyarn install –flat //安装一个包的单一版本yarn install –force //强制重新下载所有包yarn install –production //只安装dependencies里的包yarn install –no-lockfile //不读取或生成yarn.lockyarn install –pure-lockfile //不生成yarn.lock添加包（会更新package.json和yarn.lock）： yarn add [package] // 在当前的项目中添加一个依赖包，会自动更新到package.json和yarn.lock文件中yarn add [package]@[version] // 安装指定版本，这里指的是主要版本，如果需要精确到小版本，使用-E参数yarn add [package]@[tag] // 安装某个tag（比如beta,next或者latest）//不指定依赖类型默认安装到dependencies里，你也可以指定依赖类型： yarn add –dev/-D // 加到 devDependenciesyarn add –peer/-P // 加到 peerDependenciesyarn add –optional/-O // 加到 optionalDependencies//默认安装包的主要版本里的最新版本，下面两个命令可以指定版本： yarn add –exact/-E // 安装包的精确版本。例如yarn add &#x66;&#x6f;&#111;&#64;&#49;&#x2e;&#50;&#x2e;&#51;会接受1.9.1版，但是yarn add &#x66;&#x6f;&#111;&#64;&#49;&#x2e;&#50;&#46;&#51; –exact只会接受1.2.3版yarn add –tilde/-T // 安装包的次要版本里的最新版。例如yarn add &#x66;&#x6f;&#x6f;&#64;&#49;&#x2e;&#50;&#46;&#x33; –tilde会接受1.2.9，但不接受1.3.0发布包 yarn publish移除一个包 yarn remove ：移除一个包，会自动更新package.json和yarn.lock更新一个依赖 yarn upgrade 用于更新包到基于规范范围的最新版本运行脚本 yarn run 用来执行在 package.json 中 scripts 属性下定义的脚本显示某个包的信息 yarn info 可以用来查看某个模块的最新版本信息缓存 yarn cacheyarn cache list # 列出已缓存的每个包 yarn cache dir # 返回 全局缓存位置 yarn cache clean # 清除缓存npm 与 yarn命令比较: npm 与 yarn相关问题比较:npm模块的依赖: npm存在一些历史遗留问题，请看下图： 比如说你的项目模块依赖是图中描述的，@1.2.1代表这个模块的版本。在你安装A的时候需要安装依赖C和D，很多依赖不会指定版本号，默认会安装最新的版本，这样就会出现问题：比如今天安装模块的时候C和D是某一个版本，而当以后C、D更新的时候，再次安装模块就会安装C和D的最新版本，如果新的版本无法兼容你的项目，你的程序可能就会出BUG，甚至无法运行。这就是npm的弊端，而yarn为了解决这个问题推出了yarn.lock的机制，这是作者项目中的yarn.lock文件。 yarn.lock文件格式: 大家会看到，这个文件已经把依赖模块的版本号全部锁定，当你执行yarn install的时候，yarn会读取这个文件获得依赖的版本号，然后依照这个版本号去安装对应的依赖模块，这样依赖就会被锁定，以后再也不用担心版本号的问题了。其他人或者其他环境下使用的时候，把这个yarn.lock拷贝到相应的环境项目下再安装即可。注意：这个文件不要手动修改它，当你使用一些操作如yarn add时，yarn会自动更新yarn.lock。 使用yrm工具管理一些npm源安装yarn global add yrm查看可用源yrm ls选择源yrm use yarn快速删除node_modules手动删除真的很慢： 安装： npm install rimraf -g使用：rimraf node_modulesrimraf是node的一个包，可以快速删除node_modules，再也不用等半天了","categories":[],"tags":[]},{"title":"","slug":"大前端/other/vue-动态移动端适配px转化为rem","date":"2021-07-20T05:09:34.903Z","updated":"2021-05-14T02:44:10.632Z","comments":true,"path":"2021/07/20/大前端/other/vue-动态移动端适配px转化为rem/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/vue-%E5%8A%A8%E6%80%81%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%80%82%E9%85%8Dpx%E8%BD%AC%E5%8C%96%E4%B8%BArem/","excerpt":"","text":"下载依赖1npm install postcss-pxtorem -D 新建一个rem.js的文件，在main.js中引用。12345678910111213141516// 基准大小const baseSize = 32// 设置 rem 函数function setRem () &#123; // 当前页面宽度相对于 750 宽的缩放比例，可根据自己需要修改。设计稿 const scale = document.documentElement.clientWidth / 750;//375 // 设置页面根节点字体大小 document.documentElement.style.fontSize = (baseSize * Math.min(scale, 2)) + &#x27;px&#x27;&#125;// 初始化setRem()// 改变窗口大小时重新设置 remwindow.onresize = function () &#123; setRem()&#125; postcss.config.js123456789101112131415161718192021222324252627module.exports = &#123; plugins: &#123; // 兼容浏览器，添加前缀 autoprefixer: &#123; overrideBrowserslist: [ &quot;Android 4.1&quot;, &quot;iOS 7.1&quot;, &quot;Chrome &gt; 31&quot;, &quot;ff &gt; 31&quot;, &quot;ie &gt;= 8&quot;, &quot;last 10 versions&quot;, // 所有主流浏览器最近10版本用 ], grid: true, &#125;, &quot;postcss-pxtorem&quot;: &#123; rootValue: 16, //结果为：设计稿元素尺寸/16，比如元素宽320px,最终页面会换算成 20rem propList: [&quot;*&quot;], //是一个存储哪些将被转换的属性列表，这里设置为[&#x27;*&#x27;]全部，假设需要仅对边框进行设置，可以写[&#x27;*&#x27;, &#x27;!border*&#x27;] unitPrecision: 5, //保留rem小数点多少位 //selectorBlackList: [&#x27;.radius&#x27;], //则是一个对css选择器进行过滤的数组，比如你设置为[&#x27;fs&#x27;]，那例如fs-xl类名，里面有关px的样式将不被转换，这里也支持正则写法。 replace: true, //这个真不知到干嘛用的。有知道的告诉我一下 mediaQuery: false, //媒体查询( @media screen 之类的)中不生效 minPixelValue: 12, //px小于12的不会被转换 &#125;, &#125;,&#125;;// 蓝湖上设计稿自定义为375px 测量值直接写入即可 若设计稿为750px 则rootValue: 32 PostCSS plugin postcss-pxtorem requires PostCSS 8.postcss-pxtorem使用方法：https://www.cnblogs.com/liangziaha/p/13636936.html 这里报错 postcss-pxtorem 需要 8. 我以为要使用postcss-pxtorem 8.以上的版本就去看了一下postcss-pxtorem版本最高才6.0 这里降低 &#112;&#x6f;&#x73;&#116;&#99;&#115;&#x73;&#x2d;&#x70;&#120;&#116;&#111;&#x72;&#101;&#x6d;&#x40;&#x35;&#x2e;&#x31;&#x2e;&#49; 1npm i postcss-pxtorem@5.1.1 再次运行npm start 就可以了","categories":[],"tags":[]},{"title":"","slug":"大前端/other/requestIdleCallback和requestAnimationFrame详解","date":"2021-07-20T05:09:34.901Z","updated":"2021-04-16T05:23:58.248Z","comments":true,"path":"2021/07/20/大前端/other/requestIdleCallback和requestAnimationFrame详解/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/requestIdleCallback%E5%92%8CrequestAnimationFrame%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"页面流畅与 FPS页面是一帧一帧绘制出来的，当每秒绘制的帧数（FPS）达到 60时，页面是流畅的，小于这个值时，用户会感觉到卡顿。 1s 60帧，所以每一帧分到的时间是 1000/60 ≈ 16 ms。所以我们书写代码时力求不让一帧的工作量超过 16ms。 Frame那么浏览器每一帧都需要完成哪些工作？ 通过上图可看到，一帧内需要完成如下六个步骤的任务： 处理用户的交互 JS 解析执行 帧开始。窗口尺寸变更，页面滚去等的处理 requestAnimationFrame(rAF) 布局 绘制 requestIdleCallback上面六个步骤完成后没超过 16 ms，说明时间有富余，此时就会执行 requestIdleCallback里注册的任务。 从上图也可看出，和 requestAnimationFrame 每一帧必定会执行不同，requestIdleCallback 是捡浏览器空闲来执行任务。 如此一来，假如浏览器一直处于非常忙碌的状态，requestIdleCallback 注册的任务有可能永远不会执行。此时可通过设置 timeout （见下面 API 介绍）来保证执行。 callback：回调，即空闲时需要执行的任务，该回调函数接收一个IdleDeadline对象作为入参。其中IdleDeadline对象包含： didTimeout，布尔值，表示任务是否超时，结合 timeRemaining 使用。 timeRemaining()，表示当前帧剩余的时间，也可理解为留给任务的时间还有多少。 options：目前 options 只有一个参数 timeout。表示超过这个时间后，如果任务还没执行，则强制执行，不必等待空闲。 IdleDeadline对象参考MDN:https://developer.mozilla.org/zh-CN/docs/Web/API/IdleDeadline API1var handle = window.requestIdleCallback(callback[, options]) 示例 1234567891011121314151617181920212223242526272829requestIdleCallback(myNonEssentialWork, &#123; timeout: 2000 &#125;);// 任务队列const tasks = [ () =&gt; &#123; console.log(&quot;第一个任务&quot;); &#125;, () =&gt; &#123; console.log(&quot;第二个任务&quot;); &#125;, () =&gt; &#123; console.log(&quot;第三个任务&quot;); &#125;,];function myNonEssentialWork (deadline) &#123; // 如果帧内有富余的时间，或者超时 while ((deadline.timeRemaining() &gt; 0 || deadline.didTimeout) &amp;&amp; tasks.length &gt; 0) &#123; work(); &#125; if (tasks.length &gt; 0) requestIdleCallback(myNonEssentialWork); &#125;function work () &#123; tasks.shift()(); console.log(&#x27;执行任务&#x27;);&#125; 超时的情况，其实就是浏览器很忙，没有空闲时间，此时会等待指定的 timeout 那么久再执行，通过入参 dealine 拿到的 didTmieout 会为 true，同时 timeRemaining () 返回的也是 0。超时的情况下如果选择继续执行的话，肯定会出现卡顿的，因为必然会将一帧的时间拉长。 cancelIdleCallback与 setTimeout 类似，返回一个唯一 id，可通过 cancelIdleCallback 来取消任务。 总结 一些低优先级的任务可使用 requestIdleCallback 等浏览器不忙的时候来执行，同时因为时间有限，它所执行的任务应该尽量是能够量化，细分的微任务（micro task）。 因为它发生在一帧的最后，此时页面布局已经完成，所以不建议在 requestIdleCallback 里再操作 DOM，这样会导致页面再次重绘。DOM 操作建议在 rAF 中进行。同时，操作 DOM 所需要的耗时是不确定的，因为会导致重新计算布局和视图的绘制，所以这类操作不具备可预测性。 Promise 也不建议在这里面进行，因为 Promise 的回调属性 Event loop 中优先级较高的一种微任务，会在 requestIdleCallback 结束时立即执行，不管此时是否还有富余的时间，这样有很大可能会让一帧超过 16 ms。 额外补充一下window.requestAnimationFrame 在没有 requestAnimationFrame 方法的时候，执行动画，我们可能使用 setTimeout 或setInterval 来触发视觉变化；但是这种做法的问题是：回调函数执行的时间是不固定的，可能刚好就在末尾，或者直接就不执行了，经常会引起丢帧而导致页面卡顿。 归根到底发生上面这个问题的原因在于时机，也就是浏览器要知道何时对回调函数进行响应。**setTimeout 或 setInterval 是使用定时器来触发回调函数的，而定时器并无法保证能够准确无误的执行，有许多因素会影响它的运行时机，比如说：当有同步代码执行时，会先等同步代码执行完毕，异步队列中没有其他任务，才会轮到自己执行**。并且，我们知道每一次重新渲染的最佳时间大约是 16.6 ms，如果定时器的时间间隔过短，就会造成 过度渲染，增加开销；过长又会延迟渲染，使动画不流畅。 requestAnimationFrame 方法不同与 setTimeout 或 setInterval，它是由系统来决定回调函数的执行时机的，会请求浏览器在下一次重新渲染之前执行回调函数。无论设备的刷新率是多少，**requestAnimationFrame 的时间间隔都会紧跟屏幕刷新一次所需要的时间；例如某一设备的刷新率是 75 Hz，那这时的时间间隔就是 13.3 ms（1 秒 / 75 次）。需要注意的是这个方法虽然能够保证回调函数在每一帧内只渲染一次，但是如果这一帧有太多任务执行，还是会造成卡顿的；因此它只能保证重新渲染的时间间隔最短是屏幕的刷新时间。** requestAnimationFrame 方法的具体说明可以看 MDN 的相关文档，下面通过一个网页动画的示例来了解一下如何使用。 1234567let offsetTop = 0;const div = document.querySelector(&quot;.div&quot;);const run = () =&gt; &#123; div.style.transform = `translate3d(0, $&#123;offsetTop += 10&#125;px, 0)`; window.requestAnimationFrame(run);&#125;;run(); 如果想要实现动画效果，每一次执行回调函数，必须要再次调用 requestAnimationFrame 方法；与 setTimeout 实现动画效果的方式是一样的，只不过不需要设置时间间隔。 来源 https://www.jianshu.com/p/2771cb695c81","categories":[],"tags":[]},{"title":"","slug":"大前端/other/requestAnimationFrame详解","date":"2021-07-20T05:09:34.899Z","updated":"2021-01-06T09:21:34.000Z","comments":true,"path":"2021/07/20/大前端/other/requestAnimationFrame详解/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/requestAnimationFrame%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"为什么要说它，源于看到的一道面试题：问题是用js实现一个无限循环的动画。 首先想到的是定时器 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; #e&#123; width: 100px; height: 100px; background: red; position: absolute; left: 0; top: 0; zoom: 1; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;e&quot;&gt;&lt;/div&gt;&lt;script&gt; var e = document.getElementById(&quot;e&quot;); var flag = true; var left = 0; function render() &#123; if(flag == true)&#123; if(left&gt;=100)&#123; flag = false &#125; e.style.left = ` $&#123;left++&#125;px` &#125;else&#123; if(left&lt;=0)&#123; flag = true &#125; e.style.left = ` $&#123;left--&#125;px` &#125; &#125; setInterval(function()&#123; render() &#125;,1000/60)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 可以说是完美实现！ 至于时间间隔为什么是1000/60,这是因为大多数屏幕渲染的时间间隔是每秒60帧。 既然setInterval可以搞定为啥还要用requestAnimationFrame呢？最直观的感觉就是，添加api的人是个大神级牛人，我只能怀疑自己。 所以搜索相关问题发现以下两点 requestAnimationFrame 比起 setTimeout、setInterval的优势主要有两点：1、requestAnimationFrame 会把每一帧中的所有DOM操作集中起来，在一次重绘或回流中就完成，并且重绘或回流的时间间隔紧紧跟随浏览器的刷新频率，一般来说，这个频率为每秒60帧。2、在隐藏或不可见的元素中，requestAnimationFrame将不会进行重绘或回流，这当然就意味着更少的的cpu，gpu和内存使用量。直接上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; #e&#123; width: 100px; height: 100px; background: red; position: absolute; left: 0; top: 0; zoom: 1; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;e&quot;&gt;&lt;/div&gt;&lt;script&gt; var e = document.getElementById(&quot;e&quot;); var flag = true; var left = 0; function render() &#123; if(flag == true)&#123; if(left&gt;=100)&#123; flag = false &#125; e.style.left = ` $&#123;left++&#125;px` &#125;else&#123; if(left&lt;=0)&#123; flag = true &#125; e.style.left = ` $&#123;left--&#125;px` &#125; &#125; //requestAnimationFrame效果 (function animloop() &#123; render(); window.requestAnimationFrame(animloop); &#125;)();&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 我没有添加各个浏览器的兼容写法，这里只说用法。 效果是实现了，不过我想到两个问题。 1、怎么停止requestAnimationFrame？是否有类似clearInterval这样的类似方法？ 第一个问题：答案是确定的 必须有：cancelAnimationFrame()接收一个参数 requestAnimationFrame默认返回一个id，cancelAnimationFrame只需要传入这个id就可以停止了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; #e&#123; width: 100px; height: 100px; background: red; position: absolute; left: 0; top: 0; zoom: 1; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;e&quot;&gt;&lt;/div&gt;&lt;script&gt; var e = document.getElementById(&quot;e&quot;); var flag = true; var left = 0; var rafId = null function render() &#123; if(flag == true)&#123; if(left&gt;=100)&#123; flag = false &#125; e.style.left = ` $&#123;left++&#125;px` &#125;else&#123; if(left&lt;=0)&#123; flag = true &#125; e.style.left = ` $&#123;left--&#125;px` &#125; &#125; //requestAnimationFrame效果 (function animloop(time) &#123; console.log(time,Date.now()) render(); rafId = requestAnimationFrame(animloop); //如果left等于50 停止动画 if(left == 50)&#123; cancelAnimationFrame(rafId) &#125; &#125;)(); //setInterval效果 // setInterval(function()&#123; // render() // &#125;,1000/60)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2019-10-10 at 10.19.07.gif 附上一个效果图。也可直接capy代码测试。 2、如果我想动画频率降低怎么做，为什么不考虑加快呵呵 当前刷新频率已经是屏幕的刷新频率了再快也没有意义了 这个略微麻烦点 默认情况下，requestAnimationFrame执行频率是1000/60,大概是16ms多执一次。 如果我们想每50ms执行一次怎么办呢？ requestAnimationFrame执行条件类似递归调用 （说的是类似）别咬我，既然这样的话我们能否自定一个时间间隔再执行呢？当然定时器这么low的东西我们就不考虑了，都已经抛弃它用rAF了（都快结束了我才想起写简写太他妈长了）， 这个思路来源于我几年前搞IM的一个项目，服务端推送消息为了减小包的大小不给时间戳，这个我们做前端的都知道，我们虽然很牛逼 不过用户更牛逼，万一改了时间就不好玩了。 解决方案是 当和服务端通信时 记录下一个时间差，（时间差等于服务端时间-本地时间）不管正负我们只要这个时间差。这样每当我们接受到消息 或者发送消息的时候我们就拿本地时间和是价差相加。这样就可以保证和服务端时间是一致的了，思路是不是很牛逼哈哈。 撤了半天我们通过以上思路来解决下rAF改变间隔的问题 上代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style&gt; #e&#123; width: 100px; height: 100px; background: red; position: absolute; left: 0; top: 0; zoom: 1; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;e&quot;&gt;&lt;/div&gt;&lt;script&gt; var e = document.getElementById(&quot;e&quot;); var flag = true; var left = 0; //当前执行时间 var nowTime = 0; //记录每次动画执行结束的时间 var lastTime = Date.now(); //我们自己定义的动画时间差值 var diffTime = 40; function render() &#123; if(flag == true)&#123; if(left&gt;=100)&#123; flag = false &#125; e.style.left = ` $&#123;left++&#125;px` &#125;else&#123; if(left&lt;=0)&#123; flag = true &#125; e.style.left = ` $&#123;left--&#125;px` &#125; &#125; //requestAnimationFrame效果 (function animloop() &#123; //记录当前时间 nowTime = Date.now() // 当前时间-上次执行时间如果大于diffTime，那么执行动画，并更新上次执行时间 if(nowTime-lastTime &gt; diffTime)&#123; lastTime = nowTime render(); &#125; requestAnimationFrame(animloop); &#125;)()&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 附上一个效果： 2019-10-10 at 10.58.30.gif 作者：我是一个前端链接：https://www.jianshu.com/p/fa5512dfb4f5来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/i图标标签元素与span行内元素内容不对齐的解决方法","date":"2021-07-20T05:09:34.897Z","updated":"2021-06-21T00:53:32.624Z","comments":true,"path":"2021/07/20/大前端/other/i图标标签元素与span行内元素内容不对齐的解决方法/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/i%E5%9B%BE%E6%A0%87%E6%A0%87%E7%AD%BE%E5%85%83%E7%B4%A0%E4%B8%8Espan%E8%A1%8C%E5%86%85%E5%85%83%E7%B4%A0%E5%86%85%E5%AE%B9%E4%B8%8D%E5%AF%B9%E9%BD%90%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","excerpt":"","text":"Dom结构大致是： 1234&lt;a&gt; &lt;i&gt;图标&lt;/i&gt; &lt;span&gt;Delete&lt;/span&gt;&lt;/a&gt; 如上图，子节点i图标标签有vertical-align:middle属性。为使得子节点内容都居中，父节点a标签设置如下属性： 12line-height:2rem;text-align:center; 图标与文字“Delete”没有对齐，文字“Delete”居中了，然而icon图标并未居中。在icon字体图标具有vertical-align:middle属性并且不改变的前提下，要使icon与文字对齐，在父节点设置text-align:center的方式貌似不是较好的解决方式。 一种解决方法：让内容span也设置vertical-align:middle属性，i标签与span标签都设置line-height:2rem和text-align:center属性。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/ETag","date":"2021-07-20T05:09:34.895Z","updated":"2021-07-04T05:39:35.927Z","comments":true,"path":"2021/07/20/大前端/other/ETag/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/ETag/","excerpt":"","text":"基础概念什么是ETagETag 是 Entity Tag 的缩写，中文译过来就是实体标签的意思。在HTTP1.1协议中其实就是请求HEAD中的一个属性而已。 12345678910HTTP/1.1 200 OKDate: Mon, 23 May 2005 22:38:34 GMTContent-Type: text/html; charset=UTF-8Content-Encoding: UTF-8Content-Length: 138Last-Modified: Wed, 08 Jan 2003 23:11:55 GMTServer: Apache/1.3.3.7 (Unix) (Red-Hat/Linux)ETag: &quot;3f80f-1b6-3e1cb03b&quot;Accept-Ranges: bytesConnection: close 12345678&lt;html&gt;&lt;head&gt; &lt;title&gt;An Example Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt; Hello World, this is a very simple HTML document.&lt;/body&gt;&lt;/html&gt; 请注意第8行的 ETag: “3f80f-1b6-3e1cb03b”配置，那么为什么要用ETag呢？ ETag是HTTP1.1中才加入的一个属性，用来帮助服务器控制Web端的缓存验证。它的原理是这样的，当浏览器请求服务器的某项资源(A)时, 服务器根据A算出一个哈希值(3f80f-1b6-3e1cb03b)并通过 ETag 返回给浏览器，浏览器把”3f80f-1b6-3e1cb03b” 和 A 同时缓存在本地，当下次再次向服务器请求A时，会通过类似 If-None-Match: “3f80f-1b6-3e1cb03b” 的请求头把ETag发送给服务器，服务器再次计算A的哈希值并和浏览器返回的值做比较，如果发现A发生了变化就把A返回给浏览器(200)，如果发现A没有变化就给浏览器返回一个304未修改。这样通过控制浏览器端的缓存，可以节省服务器的带宽，因为服务器不需要每次都把全量数据返回给客户端。 注：HTTP中并没有指定如何生成ETag，哈希是比较理想的选择。 通常情况下，ETag更类似于资源指纹(fingerprints)，如果资源发生变化了就会生成一个新的指纹，这样可以快速的比较资源的变化。在服务器端实现中，很多情况下并不会用哈希来计算ETag，这会严重浪费服务器端资源，很多网站默认是禁用ETag的。有些情况下，可以把ETag退化，比如通过资源的版本或者修改时间来生成ETag。 如果通过资源修改时间来生成ETag，那么效果和HTTP协议里面的另外一个控制属性(Last-Modified)就雷同了，使用 Last-Modified 的问题在于它的精度在秒(s)的级别，比较适合不太敏感的静态资源。 后记在友盟的时候做过一个项目 - 在线参数，这个功能在很多游戏类App中非常流行，也有很多App会通过它来控制内置广告的开关，曾经一度流量过大以至于阿里那边发邮件过来要我们关闭这个服务区（这是个免费的服务）。为了继续造福群众，我们做了一些优化流量的措施： 减少SDK请求的次数设计了一种服务器端缓存验证机制前者是因为很多开发者把调用方法写在了Activity里面，这样每次Activity重建的时候都会导致给服务器发请求。但是对于SDK来说无法判断请求是开发者故意发起的还是由于Activity重建导致的，所以解决这个问题需要开发者配合 - 这明显是不可能的。考虑到在线参数并不会经常性的发生变化，SDK限制了请求间隔为10分钟 - 考虑大部分App的声明周期是3 - 10 分钟（对于读书、视频等媒体类应用例外），这等于App每次启动只能请求一次服务器，这种限制在开发者DEBUG阶段会造成奇怪的问题 - 明明更新了在线参数，本地却不起效果。 第二个措施其实是进一步优化，在最早设计在线参数的时候已经添加了服务器端的验证机制 - 每次请求数据的时候带上服务器返回的最后修改时间，如果服务器端发现数据没有变化，那么就不再返回数据。但是这个协议是在HTTP-Body里面通过私有协议实现的，每次请求服务器的时候依然会发送大量数据(&lt;1K)，所以优化的措施是每次先发送一个最简的协议给服务器如果数据有变化再发送一个标准的协议给服务器取回数据。 当时我对HTTP协议并不熟悉，这些设计都是通过私有协议实现的，其实完全可以通过HTTP来实现，这样可以较好的分离控制逻辑和业务数据 GZIP压缩前言在基于 HTTP 协议的网络传输中 GZip 经常被使用，Nginx 中也可以使用半行代码开启 GZip。GZip 压缩的原理是什么呢？本篇文章是我在网上阅读了一些文档后做的简单总结。 从 RFC 1952 看起RFC 1952 是 GZIP file format specification version 4.3。该规范主要定义了 GZip 压缩的在数据格式方面的规范，以方便不同的操作系统、CPU、文件系统等之间进行文件传输交换。下面挑有意思的几个点说，感兴趣的可以阅读 RFC 1952 的原文。 GZIP 的文件格式在设计上其实是可以允许一个文件里有多个压缩数据集（compressed data sets）—— GZIP 压缩后的片段拼接而成的。但就我们大多数应用场景来说，基本上都是一个文件一个压缩数据集，如果是多个文件一起打包的话，也往往是将多个包合并成一个 tar 文件。 每个压缩数据集都是下面的结构： | ID1 | ID2 | CM | FLG | MTIME（4字节） | XFL | OS | —-&gt; more | 与 | 之间是 1 byte，都是大端字节（Big Edian） 其中 ID1 和 ID2 分别是 0x1f 和 0x8b，用来标识文件格式是 gzip CM 标识 加密算法，目前 0-7是保留字，8 指的是 deflate 算法 FLG 从低地址到高地址分别是 FTEXT、FHCRC、FEXTRA、FNAME、FCOMMENT、reserved、 reserved、reserved，这里每个 bit 被设置了之后有什么意义感兴趣的话可以详细参考 RFC 1952。比较有意思的是 FEXTRA，如果它被设置了表示存在额外的拓展字段。拓展字段的结构如下： | SI1 | SI2 | LEN | … LEN bytes of subfield data … | SI1、SI2 是对子域的 ID，由 ASCII 码组成。如果你需要使用的话，可以向他的维护者 Jean-Loup Gailly &#x67;&#x7a;&#105;&#x70;&#64;&#112;&#114;&#x65;&#x70;&#46;&#x61;&#105;&#46;&#109;&#105;&#x74;&#46;&#101;&#x64;&#x75; 发邮件申请。目前 Apollo file 就有自己的专属 ID MTIME 指的是源文件最近一次修改时间，存的是 Unix 时间戳 XFL 是给压缩算法传的一些参数，用来标识如何解压。defalte 算法中 2 表示使用压缩率最高的算法，4 表示使用压缩速度最快的算法 OS 标识压缩程序运行的文件系统，以处理 EOF 等的问题 more 后面是根据 FLG 的开启情况决定的，可能会有 循环冗余校验码、源文件长度、附加信息等多种其他信息 压缩核心之 DeflateGZIP 的核心是 Deflate，在 RFC 1951 中被标准化，并且在当时作为 LZW 的替代品有了非常广泛的使用。 Deflate 是一个同时使用 LZ77 与 Huffman Coding 的算法，这里简单介绍下这两种算法的大致思路： LZ77 LZ77 的核心思路是如果一个串中有两个重复的串，那么只需要知道第一个串的内容和后面串相对于第一个串起始位置的距离 + 串的长度。 比如： ABCDEFGABCDEFH → ABCDEFG(7,6)H。7 指的是往前第 7 个数开始，6 指的是重复串的长度，ABCDEFG(7,6)H 完全可以表示前面的串，并且是没有二义性的。 LZ77 用 滑动窗口（sliding-window compression）来实现这个算法。具体思路是扫描头从串的头部开始扫描串，在扫描头的前面有一个长度为 N 的滑动窗口。如果发现扫描头处的串和窗口里的 最长匹配串 是相同的，则用（两个串之间的距离，串的长度）来代替后一个重复的串，同时还需要添加一个表示是真实串还是替换后的“串”的字节在前面以方便解压（此串需要在 真实串和替换“串” 之前都有存在）。 实际过程中滑动窗口的大小是固定的，匹配的串也有最小长度限制，以方便 标识+两个串之间的距离+串的长度 所占用的字节是固定的 以及 不要约压缩体积越大。更加详细的实现可以参考：Standford Edu. lz77 algorithm、 LZ77 Compression Algorithm、 LZ77压缩算法编码原理详解(结合图片和简单代码) 这里通过这个压缩机制也就能比较容易的解释为啥 CSS BEM 写法 GZIP 压缩之后可以忽略长度以及 JPEG 图片 GZIP 之后可能会变大 的情况了 解压：GZIP 的压缩因为要在窗口里寻找重复串相对来说效率是比较低的（LZ77 还是通过 Hash 等系列方法提高了很多），那解压又是怎么个情况呢？观察压缩后的整个串，每个小串前都有一个标识要标记是原始串还是替换“串”，通过这个标识就能以 O（1）的复杂度直接读完并且替换完替换“串”，整体上效率是非常可观的。 Huffman Coding Huffman Coding 是大学课本中一般都会提到的算法。核心思路是通过构造 Huffman Tree 的方式给字符重新编码（核心是避免一个叶子的路径是另外一个叶子路径的前缀），以保证出现频路越高的字符占用的字节越少。关于 Huffman Tree 的构造这里不再细说，不太清楚的可以参考：Huffman Coding。 解压：Huffman Coding 之后需要维护一张 Huffman Map 表，来记录重新编码后的字符串，根据这张表，还原原始串也是非常高效的。 Deflate 综合使用了 LZ77 和 Huffman Coding 来压缩文件，相对而言又提升了很多。详细可以参考 gzip原理与实现 网站中的使用在 RFC 2016 中 GZIP 已经成为了规定的三种标准HTTP压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML、CSS、JavaScript 等资源文件。 Nginx 开启 Nginx 的 ngx_http_gzip_module 也提供了开启 GZIP 压缩的方式，有下面的一些常用配置： 12345678# 开启gzip on;# 压缩等级，1-9。设置多少可以参考：http://serverfault.com/questions/253074/what-is-the-best-nginx-compression-gzip-levelgzip_comp_level 2;# &quot;MSIE [1-6]\\.&quot; 比如禁止 IE6 使用 GZIPgzip_disable regex ...# 最小压缩文件长度gzip_min_length 20;# 使用 GZIP 压缩的最小 HTTP 版本gzip_http_version 1.1;# 压缩的文件类型，值是 [MIME type](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Complete_list_of_MIME_types)gzip_types text/html; 相关探测 Nginx 上开启 GZIP 之后，理论上会按照 GZIP 配置打开压缩。那如何检测是否开启成功了呢？ 打开浏览器，访问你的网站，看 Chrome 的 Network，如果 Size 上有两个不一样大小的体积（如：222KB 和 613KB），则代表 GZIP 已经成功开启。 那浏览器又是如何和服务器配合的呢？ 浏览器在请求资源的时候再 header 里面带上 accept-encoding: gzip 的参数。Nginx 在接收到 Header 之后，发现如果有这个配置，则发送 GZIP 之后的文件（返回的 header 里也包含相关的说明），如果没有则发送源文件。浏览器根据 response header 来处理要不要针对返回的文件进行解压缩然后展示。","categories":[],"tags":[]},{"title":"","slug":"大前端/other/ES6","date":"2021-07-20T05:09:34.893Z","updated":"2021-04-13T07:46:28.619Z","comments":true,"path":"2021/07/20/大前端/other/ES6/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/ES6/","excerpt":"","text":"https://www.jianshu.com/p/287e0bb867ae","categories":[],"tags":[]},{"title":"","slug":"大前端/other/css文字如何垂直居中","date":"2021-07-20T05:09:34.891Z","updated":"2021-06-08T07:24:14.150Z","comments":true,"path":"2021/07/20/大前端/other/css文字如何垂直居中/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/css%E6%96%87%E5%AD%97%E5%A6%82%E4%BD%95%E5%9E%82%E7%9B%B4%E5%B1%85%E4%B8%AD/","excerpt":"","text":"方法1：使用line-height属性使文字垂直居中 line-height属性设置行间的距离（行高）；该属性不允许使用负值。 line-height属性会影响行框的布局。在应用到一个块级元素时，它定义了该元素中基线之间的最小距离而不是最大距离。 line-height 与 font-size 的计算值之差（在 CSS 中成为“行间距”）分为两半，分别加到一个文本行内容的顶部和底部。可以包含这些内容的最小框就是行框。 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;css 垂直居中&lt;/title&gt; &lt;style&gt; .box&#123; width: 300px; height: 300px; background: #ddd; line-height:300px; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;box&quot;&gt;css 垂直居中了--文本文字&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 方法2：将外部块格式化为表格单元格 表格单元格的内容可以垂直居中，将外部块格式化为表格单元格就可垂直居中文本。 示例：将段落置于具有特定给定高度的块内 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;css 垂直居中&lt;/title&gt; &lt;style&gt; .box&#123; width: 400px; height: 200px; background: #ddd; display: table-cell; vertical-align: middle; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;box&quot;&gt;css 垂直居中了--文本文字&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 方法3：使用CSS3的flex布局 使文字垂直居中 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;css 垂直居中&lt;/title&gt; &lt;style&gt; .box&#123; width: 300px; height: 200px; background: #ddd; /*设置为伸缩容器*/ display: -webkit-box; display: -moz-box; display: -ms-flexbox; display: -webkit-flex; display: flex; /*垂直居中*/ -webkit-box-align: center;/*旧版本*/ -moz-box-align: center;/*旧版本*/ -ms-flex-align: center;/*混合版本*/ -webkit-align-items: center;/*新版本*/ align-items: center;/*新版本*/ &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;box&quot;&gt;css 垂直居中--文本文字（弹性布局）&lt;/div&gt; &lt;/body&gt;&lt;/html&gt;","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/JavaScript对象封装的简单实现方法(3种方法)","date":"2021-07-20T05:09:34.857Z","updated":"2021-04-13T03:33:31.583Z","comments":true,"path":"2021/07/20/大前端/JavaScript/JavaScript对象封装的简单实现方法(3种方法)/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/JavaScript%E5%AF%B9%E8%B1%A1%E5%B0%81%E8%A3%85%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95(3%E7%A7%8D%E6%96%B9%E6%B3%95)/","excerpt":"","text":"1. 使用关键字new创建对象 12345function Person(name,age)&#123; this.name = name; this.age = age;&#125;var p = new Persion(); 2. 使用Object直接创建对象 可以看出，这种方法扩展很方便。 123var obj = new Object(); // 这里也可以写成 var = &#123;&#125;obj.name = &#x27;ling&#x27;;obj.age = 22; 3. 使用JSON创建（对象字面量的说法更准确，但JSON更好理解） 从Javascript1.2开始，创建对象有了更快捷的方式。 1234var p = &#123; name :&#x27;lin&#x27;, //&quot;name&quot;:&quot;lingceng这样加引号解析方式相同 gander:&#x27;mage&#x27;&#125; 实践方式 结合构造函数和原型模式创建对象的方式很适合实践。 12345678910111213141516171819function Person(name,age)&#123; //实例属性 //实例多份拷贝 this.name = name; this.age = age;&#125;Person.prototype = &#123; //因为原型被替换，所以需要恢复construtor的默认指向 constructor:Person, showName:function()&#123; alert(&#x27;ShowName in prototype:&#x27;+this.name); &#125;, showAge:function()&#123; alert(this.age); &#125;&#125;var p = new Persion(&#x27;lin&#x27;,22);p.showAge();","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/Javascript-节流与防抖","date":"2021-07-20T05:09:34.854Z","updated":"2021-01-11T08:28:38.000Z","comments":true,"path":"2021/07/20/大前端/JavaScript/Javascript-节流与防抖/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/Javascript-%E8%8A%82%E6%B5%81%E4%B8%8E%E9%98%B2%E6%8A%96/","excerpt":"","text":"函数节流(throttle)是指阻止一个函数在很短时间间隔内连续调用. 只有当上一次函数执行后达到规定的时间间隔, 才能进行下一次调用. 但要保证一个累计最小调用时间间隔(例如拖拽类的节流需要有连续效果) 函数节流用于onresize, onscroll等短时间内会多次触发的事件 函数节流的原理: 使用定时器做时间节流. 当触发一个事件时, 先用setTimeout让这个事件延迟一小段时间在执行. 如果在这个时间间隔内又触发了事件, 就clearTimeout原来的定时器, 在setTimeout一个新的定时器重复以上流程 函数节流简单实现: function throttle(method, context) &#123; clearTimeout(method.tId); method.tId = setTimeout(function() &#123; method.call(context); &#125;, 100); &#125; window.onresize = function() &#123; throttle(myFunc, window) &#125; // 封装 var throttle = function(fn, delay, mustRunDelay)&#123; var timer = null; var t_start; return function()&#123; var context = this, args = arguments, t_curr = +new Date(); clearTimeout(timer); if(!t_start)&#123; t_start = t_curr; &#125; if(t_curr - t_start &gt;= mustRunDelay)&#123; fn.apply(context, args); t_start = t_curr; &#125; else &#123; timer = setTimeout(function()&#123; fn.apply(context, args); &#125;, delay); &#125; &#125;; &#125;; // 调用（两次间隔50ms内连续触发不执行，但每累计100ms至少执行一次 window.onresize = throttle(myFunc, 50, 100);","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/JavaScript-提取数组对象中的某一个属性组成新数组","date":"2021-07-20T05:09:34.852Z","updated":"2021-06-04T03:48:42.351Z","comments":true,"path":"2021/07/20/大前端/JavaScript/JavaScript-提取数组对象中的某一个属性组成新数组/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/JavaScript-%E6%8F%90%E5%8F%96%E6%95%B0%E7%BB%84%E5%AF%B9%E8%B1%A1%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%80%E4%B8%AA%E5%B1%9E%E6%80%A7%E7%BB%84%E6%88%90%E6%96%B0%E6%95%B0%E7%BB%84/","excerpt":"","text":"【js】提取数组对象中的某一个属性组成新数组场景 ：批量新增的时候后台接收的数组对象只需要一两个参数的信息 antd table的多选会把整行的信息带去 处理 12345console.log(&quot;selectedRows--&quot;, selectedRows)let arrnew = selectedRows.map((item,index) =&gt; &#123; return Object.assign(&#123;&#125;,&#123;&#x27;group_Dr&#x27;:item.group_Dr,&quot;mapInterface_Dr&quot;:item.mapInterface_Dr&#125;)&#125;)console.log(&quot;arrnew--&quot;,arrnew)","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/JavaScript-如何判断是否为空对象","date":"2021-07-20T05:09:34.845Z","updated":"2021-06-02T03:40:07.440Z","comments":true,"path":"2021/07/20/大前端/JavaScript/JavaScript-如何判断是否为空对象/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/JavaScript-%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E4%B8%BA%E7%A9%BA%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"如何判断是否为空对象1.将对象转为字符串，然后判断是否等于’{}’如果等于’{}’则为空对象，如果不等于’{}’则不为空对象。 1234567var obj = &#123;&#125;;var objStr = JSON.stringify(obj);if(objStr === &#x27;&#123;&#125;&#x27;) &#123; console.log(&quot;空对象&quot;);&#125;else &#123; console.log(&quot;不是空对象&quot;);&#125; 输出： 1空对象 2、使用Object.getOwmPropertyNames()方法+length属性进行判断 Object对象的getOwnPropertyNames方法，获取到对象中的属性名，存到一个数组中，返回数组对象；然后通过使用数组的length属性来判断此对象是否为空对象。 1234567var obj = &#123;&#125;;var arr = Object.getOwnPropertyNames(obj);if (arr.length == 0)&#123; console.log(&quot;空对象&quot;);&#125;else &#123; console.log(&quot;不是空对象&quot;);&#125; 输出： 1空对象 3、使用es6的方法Object.keys()+length属性进行判断 这是ES6的新方法，Object.keys方法是JavaScript中用于遍历对象属性的一个方法 。它传入的参数是一个对象，返回的是一个数组，数组中包含的是该对象所有的属性名。 1234567var obj = &#123;&#125;;var arr = Object.keys(obj);if (arr.length == 0)&#123; console.log(&quot;空对象&quot;);&#125;else &#123; console.log(&quot;不是空对象&quot;);&#125; 输出： 1空对象","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/JavaScript-如何判断变量是否为数字","date":"2021-07-20T05:09:34.843Z","updated":"2021-06-16T01:26:08.952Z","comments":true,"path":"2021/07/20/大前端/JavaScript/JavaScript-如何判断变量是否为数字/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/JavaScript-%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E4%B8%BA%E6%95%B0%E5%AD%97/","excerpt":"","text":"简介JavaScript 是一种动态类型语言，这意味着解释器在运行时确定变量的类型。实际上，这也允许我们在相同的代码中使用相同的变量来存储不同类型的数据。如果没有文档和一致性，我们在使用代码时并不总是知道变量的类型。 当我们期望一个变量是数字时，对字符串或数组进行操作可能会在代码中导致奇怪的结果。在本文中，我们将会介绍一些判断变量是否为数字的函数。 像&quot;10&quot;之类的数字的字符串不应被接受。 在JavaScript中，诸如NaN，Infinity和-Infinity之类的特殊值也是数字类型的。 根据这些要求，最好使用的函数是内置Number对象中的isFinite()函数。但是，开发人员通常会使用其他函数，如Number.isNaN()和typeof()函数。 我们先创建一些变量： 1234567let intVar = 2;let floatVar = 10.5;let stringVar = &#x27;4&#x27;;let nanVar = NaN;let infinityVar = Infinity;let nullVar = null;let undefinedVar = undefined; 使用 Number.isFinite() 函数名Number.isFinite()函数检查变量是否为数字，还检查其是否为有限值。 因此，对于NaN，Infinity或-Infinity的数字，它返回false。 我们用上面定义的变量来检验一下: 1234567891011121314&gt; Number.isFinite(intVar);true&gt; Number.isFinite(floatVar);true&gt; Number.isFinite(stringVar);false&gt; Number.isFinite(nanVar);false&gt; Number.isFinite(infinityVar);false&gt; Number.isFinite(nullVar);false&gt; Number.isFinite(undefined);false 这正是我们想要的。特殊的非有限数以及非数字类型的任何变量都会被忽略。所以，如果你想检查一个变量是否是一个数字，最好的方法是使用Number.isFinite()函数。 使用 Number.isNaN() 方法标准Number对象有一个isNaN()方法。它接受一个参数，并确定其值是否为NaN。因为我们想检查一个变量是否是一个数字，所以我们将在检查中使用非操作符!。 1234567891011121314&gt; !Number.isNaN(intVar);true&gt; !Number.isNaN(floatVar);true&gt; !Number.isNaN(stringVar);true # Wrong&gt; !Number.isNaN(nanVar);false&gt; !Number.isNaN(infinityVar);true # Wrong&gt; !Number.isNaN(nullVar);true # Wrong&gt; !Number.isNaN(undefinedVar);true # Wrong 这个方法是相当宽松的，因为它接受根本不是数字的值。这种方法最适合于当你知道你有一个数字并且要检查它是否是一个NaN值时，而不是一般的数字检查。 使用 typeof() 方法typeof()函数是一个全局函数，它接受变量或值作为参数，并返回其类型的字符串表示。JavaScript 总共有9种类型 undefined boolean number string bigint symbol object null (typeof() 显示的是 object) function (一种特殊类型的 object) 为了验证变量是否为数字，我们只需要检查typeof()返回的值是否为`”number”。 让我们尝试一下测试变量： 1234567891011121314&gt; typeof(intVar) == &#x27;number&#x27;;true&gt; typeof(floatVar) == &#x27;number&#x27;;true&gt; typeof(stringVar) == &#x27;number&#x27;;false&gt; typeof(nanVar) == &#x27;number&#x27;;true # Wrong&gt; typeof(infinityVar) == &#x27;number&#x27;;true # Wrong&gt; typeof(nullVar) == &#x27;number&#x27;;false&gt; typeof(undefined) == &#x27;number&#x27;;false typeof()函数的性能比Number.isNaN()要好得多。它正确地确定了字符串变量null和undefined不是数字。但是，对于NaN和Infinity，它返回true。 尽管从技术上来说这是正确的结果，但NaN和Infinity是特殊的数字值，对于大多数使用情况，我们宁愿忽略它们。 总结在本文中，我们学习了如何检查JavaScript中的变量是否为数字。 Number.isNaN()函数仅在我们知道变量为数字并且需要验证它是否为NaN`时才适用。 如果代码中有NaN，Infinity或-Infinity以及其他数字，则 typeof()`函数适用。 Number.isFinite()方法捕获所有有限数，是最适合我们的要求。","categories":[],"tags":[]},{"title":"","slug":"大前端/JavaScript/JavaScript-各种js数组骚操作","date":"2021-07-20T05:09:34.842Z","updated":"2021-06-09T01:36:48.893Z","comments":true,"path":"2021/07/20/大前端/JavaScript/JavaScript-各种js数组骚操作/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/JavaScript-%E5%90%84%E7%A7%8Djs%E6%95%B0%E7%BB%84%E9%AA%9A%E6%93%8D%E4%BD%9C/","excerpt":"","text":"各种js数组骚操作在开发中，数组的使用场景非常多，平日中也涉及到很多数组相关操作，对一些常见的操作方法进行总结和收藏，在开发中就能信手拈来，大大提高开发效率。 本文在github做了收录 github.com/Michael-lzg… 随机排序1、生成随机数遍历数组，每次循环都随机一个在数组长度范围内的数，并交换本次循环的位置和随机数位置上的元素 1234567891011121314function randomSort1(arr)&#123; for (let i = 0, l = arr.length; i &lt; l; i++) &#123; let rc = parseInt(Math.random() * l) // 让当前循环的数组元素和随机出来的数组元素交换位置 const empty = arr[i] arr[i] = arr[rc] arr[rc] = empty &#125; return arr;&#125;var arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]// 下面两次的结果肯定是不一样的；console.log(randomSort1(arr1))console.log(randomSort1(arr1)) 2、生成新数组 申明一个新的空数组,利用 while 循环，如果数组长度大于 0，就继续循环； 每次循环都随机一个在数组长度范围内的数，将随机数位置上的元素 push 到新数组里， 并利用 splice（对 splice 不太理解的同学可以看这里）截取出随机数位置上的元素，同时也修改了原始数组的长度； 12345678910111213function randomSort2(arr) &#123; var mixedArr = [] while (arr.length &gt; 0) &#123; let rc = parseInt(Math.random() * arr.length) mixedArr.push(arr[rc]) arr.splice(rc, 1) &#125; return mixedArr&#125;// 例子var arr1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]console.log(randomSort2(arr1)) 3、 arr.sort 如果 compareFunction(a, b)的返回值 小于 0 ，那么 a 会被排列到 b 之前； 如果 compareFunction(a, b)的返回值 等于 0 ，那么 a 和 b 的相对位置不变； 如果 compareFunction(a, b)的返回值 大于 0 ，那么 b 会被排列到 a 之前； 123456789function randomSort3(arr)&#123; arr.sort(function(a,b)&#123; return Math.random() - 0.5 &#125;) return arr;&#125;//例子var arr1 = [1,2,3,4,5,6,7,8,9]console.log(randomSort3(arr1)) 数组对象排序1、单个属性排序12345678910111213function compare(property)&#123; return function(a,b)&#123; let value1 = a[property] let value2 = b[property] return value1 - value2 &#125;&#125;let arr = [ &#123;name:&#x27;zopp&#x27;,age:10&#125;, &#123;name:&#x27;gpp&#x27;,age:18&#125;, &#123;name:&#x27;yjj&#x27;,age:8&#125;]console.log(arr.sort(compare(&#x27;age&#x27;))) 2、多个属性排序123456789101112131415161718function by(name, minor) &#123; return function(o, p) &#123; let a, b if (o &amp;&amp; p &amp;&amp; typeof o === &#x27;object&#x27; &amp;&amp; typeof p === &#x27;object&#x27;) &#123; a = o[name] b = p[name] if (a === b) &#123; return typeof minor === &#x27;function&#x27; ? minor(o, p) : 0 &#125; if (typeof a === typeof b) &#123; return a &lt; b ? -1 : 1 &#125; return typeof a &lt; typeof b ? -1 : 1 &#125; else &#123; thro(&#x27;error&#x27;) &#125; &#125;&#125;, 数组扁平化1、调用 ES6 中的 flat 方法12array = arr.flat(Infinity);console.log([1,[2,3,[4,5,[6,7]]]].flat(Infinity)); 2、普通递归1234567891011121314let result = []let flatten = function(arr)&#123; for(let i = 0;i &lt; arr.length;i++)&#123; let item = arr[i]; if(Array.isArray(arr[i]))&#123; flatten(item) &#125;else&#123; result.push(item) &#125; &#125; return result;&#125;let arr = [1,2,[3,4],[5,[6,7]]]console.log(flatten(arr)) 3、利用reduce函数迭代1234567function flatten(arr)&#123; return arr.reduce((pre,cur)=&gt;&#123; return pre.concat(Array.isArray(cur) ? flatten(cur) : cur) &#125;,[])&#125;let arr = [1,2,[3,4],5,[6,7]]console.log(flatten(arr)) 4、扩展运算符12345678function flatten(arr)&#123; while(arr.some((item)=&gt;Array.isArray(item)))&#123; arr = [].concat(...arr) &#125; return arr&#125;let arr = [1,2,[3,4],[5,[6,7]]]console.log(flatten(arr)) 数组去重1、利用数组的indexOf下标属性来查询12345678910function unique(arr)&#123; var newArr = [] for(var i = 0; i &lt; arr.length; i++)&#123; if(newArr.indexOf(arr[i]) === -1)&#123; newArr.push(arr[i]); &#125; &#125; return newArr&#125;console.log(unique([1,1,2,3,5,3,1,5,6,7,4])) 2、先将原数组排序，在与相邻的进行比较，如果不同则存入新数组1234567891011function unique(arr)&#123; var formArr = arr.sort() var newArr = [formArr[0]] for(let i = 1;i&lt; formArr.length;i++)&#123; if(formArr[i] !== formArr[i - 1])&#123; newArr.push(formArr[i] &#125; &#125; return newArr&#125;console.log(unique([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 3、利用对象属性存在的特性，如果没有该属性则存入新数组。123456789101112function unique(arr) &#123; var obj = &#123;&#125; var newArr = [] for (let i = 0; i &lt; arr.length; i++) &#123; if (!obj[arr[i]]) &#123; obj[arr[i]] = 1 newArr.push(arr[i]) &#125; &#125; return newArr&#125;console.log(unique([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 4、利用数组原型对象上的includes方法12345678910function unique(arr) &#123; var newArr = [] for (var i = 0; i &lt; arr.length; i++) &#123; if (!newArr.includes(arr[i])) &#123; newArr.push(arr[i]) &#125; &#125; return newArr&#125;console.log(unique([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 5、利用数组原型对象上的filter和includes方法12345678function unique(arr)&#123; var newArr = [] newArr = arr.filter(function (item)&#123; return newArr.includes(item) ? &#x27;&#x27; : newArr.push(item) &#125;) return newArr&#125;console.log(unique([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 6、利用 ES6 的 set 方法。1234function unique(arr) &#123; return Array.from(new Set(arr)) // 利用Array.from将Set结构转换成数组&#125;console.log(unique([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 根据属性去重方法一 1234function unique(arr) &#123; const res = new Map() return arr.filter((item) =&gt; !res.has(item.productName) &amp;&amp; res.set(item.productName, 1))&#125; 方法二 12345678910function unique(arr) &#123; let result = &#123;&#125; let obj = &#123;&#125; for (var i = 0; i &lt; arr.length; i++) &#123; if (!obj[arr[i].key]) &#123; result.push(arr[i]) obj[arr[i].key] = true &#125; &#125;&#125; 交集/并集/差集1、includes 方法结合 filter 方法1234567891011121314let a = [1,2,3]let b = [2,4,5]// 并集let union = a.concat(b.filter((v) =&gt; !a.includes(v)))// [1,2,3,4,5]// 交集let intersection = a.filter((v) =&gt; b.includes(v))// [2]// 差集let difference = a.concat(b).filter((v) =&gt; !a.includes(v) || !b.includes(v))// [1,3,4,5] 2、ES6的Set数据结构1234567891011121314let a = new Set([1, 2, 3])let b = new Set([2, 4, 5])// 并集let union = new Set([...a, ...b])// Set &#123;1, 2, 3, 4,5&#125;// 交集let intersect = new Set([...a].filter((x) =&gt; b.has(x)))// set &#123;2&#125;// a 相对于 b 的）差集let difference = new Set([...a].filter((x) =&gt; !b.has(x)))// Set &#123;1, 3&#125; 数组求和1、万能的for循环123456789function sum(arr) &#123; var s = 0 for (var i = arr.length - 1; i &gt;= 0; i--) &#123; s += arr[i] &#125; return s&#125;sum([1, 2, 3, 4, 5]) // 15 2、递归方法1234567891011function sum(arr)&#123; var len = arr.length if(len == 0)&#123; return 0 &#125; else if(len == 1)&#123; return arr[0] &#125; else &#123; return arr[0] + sum(arr.slice(1)) &#125;&#125;sum([1, 2, 3, 4, 5]) // 15 3、ES6的reduce方法1234567function sum(arr) &#123; return arr.reduce(function (prev, curr) &#123; return prev + curr &#125;, 0)&#125;sum([1, 2, 3, 4, 5]) // 15 类数组转化1、Array的slice方法1let arr = Array.prototype.slice.call(arguments) 2、ES6 的 Array.from()1let arr = Array.from(arguments) 3、扩展运算符1let arr = [...arguments] 数组上下移动123456789101112131415161718function swapItems(arr, index1, index2) &#123; arr[index1] = arr.splice(index2, 1, arr[index1])[0] return arr&#125;function up(arr, index) &#123; if (index === 0) &#123; return &#125; this.swapItems(arr, index, index - 1)&#125;function down(arr, index) &#123; if (index === this.list.length - 1) &#123; return &#125; this.swapItems(arr, index, index + 1)&#125; 数组转化为树形结构将如下数据转化为树状结构 123456789101112131415161718192021222324252627282930313233343536let arr = [ &#123; id: 1, name: &#x27;1&#x27;, pid: 0, &#125;, &#123; id: 2, name: &#x27;1-1&#x27;, pid: 1, &#125;, &#123; id: 3, name: &#x27;1-1-1&#x27;, pid: 2, &#125;, &#123; id: 4, name: &#x27;1-2&#x27;, pid: 1, &#125;, &#123; id: 5, name: &#x27;1-2-2&#x27;, pid: 4, &#125;, &#123; id: 6, name: &#x27;1-1-1-1&#x27;, pid: 3, &#125;, &#123; id: 7, name: &#x27;2&#x27;, &#125;,] 实现方法 123456789101112131415161718function toTree(data, parentId = 0) &#123; var itemArr = [] for (var i = 0; i &lt; data.length; i++) &#123; var node = data[i] if (node.pid === parentId) &#123; var newNode = &#123; ...node, name: node.name, id: node.id, children: toTree(data, node.id), &#125; itemArr.push(newNode) &#125; &#125; return itemArr&#125;console.log(toTree(arr))","categories":[],"tags":[]},{"title":"","slug":"大前端/css3/聚光灯","date":"2021-07-20T05:09:34.823Z","updated":"2021-05-31T02:39:48.765Z","comments":true,"path":"2021/07/20/大前端/css3/聚光灯/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/css3/%E8%81%9A%E5%85%89%E7%81%AF/","excerpt":"","text":"案例来源本例来自CodingStartup的视频：[CSS] 聚光灯效果 学到知识 关于after伪元素中content内容的设置，除了直接写在后面，也可以将其值写在html自定义的属性值中，利用attr()函数通过属性名，获取属性值。 after伪元素的所在位置相当于是所使用的元素的子元素，所以可以使用父元素相对定位，伪元素绝对定位的方法，调整伪元素的位置。 使用clip-path来实现遮罩效果，本例中属性值为ellipse(100px 100px at 0% 50%)，前两个参数为椭圆的x轴长度和y轴高度，at后的两个参数为相对于自身元素的x轴位置和y轴位置。 注意：使用这个属性时，需要注意浏览器的兼容情况，例如：Safari需要加前缀-webkit- 在使用兼容前缀的时候，一定要保留原css（非前缀）代码，不可删去。 设置对文字设置背景时，默认是文字所在的块级元素覆盖背景，如果想要将背景仅作用于文字上，类似color的效果，可以使用背景遮罩 background-clip: text; 同样要注意兼容问题。 想要实现以上效果，背景仅覆盖文字区域，还需要将文字的color设置成transparent，表示全透明色彩。 网站分享：caniuse查看css属性在各种浏览器中的支持情况，以及需要使用的前缀。 主要代码html123&lt;body&gt; &lt;h1 dot-light=&quot;Macbook Pro&quot;&gt;Macbook Pro&lt;/h1&gt;&lt;/body&gt; css123456789101112131415161718192021222324252627282930313233343536373839404142434445body &#123; margin: 0; display: flex; justify-content: center; align-items: center; background-color: #222; min-height: 100vh;&#125;h1 &#123; font-size: 9rem; font-family: Helvetica; letter-spacing: -.3rem; color: #333; position: relative;&#125;h1::after &#123; content: attr(dot-light); position: absolute; top: 0; left: 0; color: transparent; clip-path: ellipse(100px 100px at 0% 50%); animation: SpotLight 5s infinite; background-image: url(&quot;https://images.unsplash.com/photo-1568279898331-4870e84677dd?ixid=MnwxMjA3fDB8MHxzZWFyY2h8MTh8fGxpbmVhcnxlbnwwfHwwfHw%3D&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=800&amp;q=60&quot;); background-size: 150%; background-position: center center; -webkit-background-clip: text; background-clip: text;&#125;@keyframes SpotLight &#123; 0% &#123; clip-path: ellipse(100px 100px at 0% 50%); &#125; 50% &#123; clip-path: ellipse(100px 100px at 100% 50%); &#125; 100% &#123; clip-path: ellipse(100px 100px at 0% 50%); &#125;&#125;","categories":[],"tags":[]},{"title":"","slug":"大前端/css3/绘制三角形—border法","date":"2021-07-20T05:09:34.821Z","updated":"2021-05-31T02:41:12.539Z","comments":true,"path":"2021/07/20/大前端/css3/绘制三角形—border法/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/css3/%E7%BB%98%E5%88%B6%E4%B8%89%E8%A7%92%E5%BD%A2%E2%80%94border%E6%B3%95/","excerpt":"","text":"1. 实现一个简单的三角形使用CSS盒模型中的border（边框）即可实现如下所示的三角形： CSS实现简单三角形 实现原理：首先来看在为元素添加border时，border的样子；假设有如下代码： 1234567&lt;div&gt;&lt;/div&gt;div &#123; width: 50px; height: 50px; border: 2px solid orange;&#125; 效果图： border的一般使用 这是我们平常使用border最普遍的情况——往往只给border一个较小的宽度（通常为1-2px）；然而这样的日常用法就会容易让大家对border的形成方式产生误解，即认为元素的border是由四个矩形边框拼接而成。 然而事实并不是这样。实际上，元素的border是由三角形组合而成，为了说明这个问题，我们可以增大border的宽度，并为各border边设置不同的颜色： 123456div &#123; width: 50px; height: 50px; border: 40px solid; border-color: orange blue red green;&#125; 效果图： border的形成方式 既然如此，那么更进一步，把元素的内容尺寸设置为0会发生什么情况呢？ 123456div &#123; width: 0; height: 0; border: 40px solid; border-color: orange blue red green;&#125; 效果图： 元素内容尺寸为0 我们将惊奇地发现，此时元素由上下左右4个三角形“拼接”而成；那么，为了实现最终的效果，即保留最下方的三角形，还应该怎么做？很简单，我们只需要把其它border边的颜色设置为白色或透明色： 123456div &#123; width: 0; height: 0; border: 40px solid; border-color: transparent transparent red;&#125; 最终效果 Duang~ 最终的简单三角形就绘制出来了。同理，如果想要得到其它边上的三角形，只需要将剩余的border边颜色设置为白色或透明色即可。 不过，被“隐藏”的上border仍然占据着空间，要想使得绘制出的三角形尺寸最小化，还需要将上border的宽度设置为0（其它情况同理）： 1234567div &#123; width: 0; height: 0; border-width: 0 40px 40px; border-style: solid; border-color: transparent transparent red;&#125; 2. 实现带边框的三角形带边框的三角形是指为三角形添加其它颜色的边框，如同为元素添加border一样： 带边框的三角形 由于不能继续通过为已有三角形设置border的方法来为其设置边框（因为三角形本身就是利用border实现的），所以只好另想办法。而能想到的一个最自然的方法就是三角形叠放，即把当前三角形叠放在更大的三角形上方，上图所示的实现方法就是把黄色三角形放在了尺寸更大的蓝色三角形上。 为了实现这样的效果，需要利用绝对定位方法： 首先定义出外面的蓝色三角形： 12345678910&lt;div id=&quot;blue&quot;&gt;&lt;div&gt;#blue &#123; position:relative; width: 0; height: 0; border-width: 0 40px 40px; border-style: solid; border-color: transparent transparent blue;&#125; 效果为： 外围蓝色三角形 随后需要定义黄色三角形，由于黄色三角形的定位需要参考蓝色三角形的位置，所以需要用到绝对定位方法。为此还需要将黄色三角形作为蓝色三角形的子元素。一个可行的办法是在蓝色三角形内部定义一个额外的标签以表示黄色三角形，但为了节约标签起见，更好的办法是使用伪元素: 1234567891011#blue:after &#123; content: &quot;&quot;; width: 0; height: 0; position: absolute; top: 0px; left: 0px; border-width: 0 40px 40px; border-style: solid; border-color: transparent transparent yellow;&#125; 得到的效果为： 定义黄色三角形 需要特别注意此时定义出的黄色三角形与蓝色三角形之间位置的偏移关系，该偏移将受到top、left（本例中）以及黄色三角形本身border宽度的共同影响。 可能会有这样的疑问：为什么黄色三角形会向左偏移一段距离呢，按道理不应该完全重合在蓝色三角形上吗，就像下面这样？ 黄色三角形与蓝色三角形完全重合 如果有这样的疑问，说明还没有对绝对定位产生足够的认识。绝对定位的区域是基于绝对定位父元素的padding区域，然后在此基础上运用top、left、right、bottom等一系列属性来约束绝对定位子元素的位置。在本例中，由于蓝色三角形作为绝对定位父元素，其内容的尺寸为0，则内容区域就是该三角形的上顶点： 绝对定位区域 对于黄色三角形，由于设置了left: 0和top: 0，所以黄色三角形的所有内容（包括border、margin）将根据蓝色三角形的上顶点进行定位。可以把此时left: 0和top: 0分别看作是两面“隔墙”——即上隔墙和左隔墙，黄色三角形的所有内容只能在上隔墙的下方和左隔墙的右方区域。 由于黄色三角形的内容区域也位于其顶点处，且对其设置了左右各40px的border，所以黄色三角形的内容区域将向右偏移40px，从而形成了之前的效果。 想想看将黄色三角形的位置设置为left: 0和bottom: 0,会得到怎样的定位效果？（下图所示） 黄色三角形设置为left: 0和bottom: 0 搞懂了绝对定位后，只需要在原代码上稍作修改就可以将黄色三角形的顶点与蓝色三角形顶点相重合，同时还应该适当缩小黄色三角形的尺寸（按相似三角形等比例缩小）： 123456789#blue:after &#123; content: &quot;&quot;; position: absolute; top: 0px; left: -38px; border-width: 0 38px 38px; border-style: solid; border-color: transparent transparent yellow;&#125; 得到： 黄色三角形与蓝色三角形顶点重合 在上面的代码中，特意删除了之前对width: 0和height: 0的设置，因为子元素具有position:absolute设置，这会使得元素尺寸在不显式设置宽度和高度的情况下，收缩到元素内容的尺寸，由于内容设置的是content: &quot;&quot;，所以子元素的尺寸默认也就是0了。故设置width: 0和height: 0就变得多余了。 最后一步就是利用top将黄色三角形向下移动至合适的位置： 123456789#blue:after &#123; content: &quot;&quot;; position: absolute; top: 1px; left: -38px; border-width: 0 38px 38px; border-style: solid; border-color: transparent transparent yellow;&#125; 得到最终效果： 最终效果 学会了带边框三角形的绘制，那么实现类似如下三角形箭头自然也是不在话下了： 三角形箭头 实现代码： 123456789#blue:after &#123; content: &quot;&quot;; position: absolute; top: 2px; left: -38px; border-width: 0 38px 38px; border-style: solid; border-color: transparent transparent #fff;&#125; 3. 绘制其它角度的三角形绘制其它角度的三角形，如： 右直角三角 或者： 左直角三角 就更简单了，其实它们都是基于之前绘制的三角形而来的。如果想绘制右直角三角，则将左border设置为0；如果想绘制左直角三角，将右border设置为0即可（其它情况同理）。","categories":[],"tags":[]},{"title":"","slug":"大前端/css3/@media媒体查询","date":"2021-07-20T05:09:34.820Z","updated":"2021-06-16T01:49:08.271Z","comments":true,"path":"2021/07/20/大前端/css3/@media媒体查询/","link":"","permalink":"https://alloceee.github.io/2021/07/20/%E5%A4%A7%E5%89%8D%E7%AB%AF/css3/@media%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"12345678910111213141516/* 小屏适配 */@media screen and (max-width: 995px) &#123; .layui-layout-admin .layui-body&#123; left: 0; padding-left: 0; &#125; .layui-layout-admin .layui-body .layadmin-tabsbody-item&#123; left: 0 !important; &#125; .layui-header .hos_logo,.layui-header .layui-system:after&#123; display: none; &#125; .layui-header .layui-system&#123; margin-left: 10px; &#125;&#125;","categories":[],"tags":[]},{"title":"Redis常见问题","slug":"数据库/Redis/Redis-雪崩、穿透、并发等5大难题解决方案","date":"2020-12-19T14:04:23.000Z","updated":"2020-12-19T07:14:40.000Z","comments":true,"path":"2020/12/19/数据库/Redis/Redis-雪崩、穿透、并发等5大难题解决方案/","link":"","permalink":"https://alloceee.github.io/2020/12/19/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/Redis-%E9%9B%AA%E5%B4%A9%E3%80%81%E7%A9%BF%E9%80%8F%E3%80%81%E5%B9%B6%E5%8F%91%E7%AD%895%E5%A4%A7%E9%9A%BE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"一、Redis雪崩、穿透、并发等5大难题解决方案缓存雪崩数据未加载到缓存中，或者缓存同一时间大面积的失效，从而导致所有请求都去查数据库，导致数据库CPU和内存负载过高，甚至宕机。 雪崩的简单过程： redis集群大面积故障 缓存失效，但依然大量请求访问缓存服务redis redis大量失效后，大量请求转向到mysql数据库 mysql的调用量暴增，很快就扛不住了，甚至直接宕机 由于大量的应用服务依赖mysql和redis的服务，这个时候很快会演变成各服务器集群的雪崩，最后网站彻底崩溃。 如何预防缓存雪崩： 缓存层设计成高可用，防止缓存大面积故障。即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如 Redis Sentinel 和 Redis Cluster 都实现了高可用。 2.缓存降级 可以利用ehcache等本地缓存(暂时支持)，但主要还是对源服务访问进行限流、资源隔离（熔断）、降级等。 当访问量剧增、服务出现问题仍然需要保证服务还是可用的。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级，这里会涉及到运维的配合。 降级的最终目的是保证核心服务可用，即使是有损的。 比如推荐服务中，很多都是个性化的需求，假如个性化需求不能提供服务了，可以降级补充热点数据，不至于造成前端页面是个大空白。 在进行降级之前要对系统进行梳理，比如：哪些业务是核心(必须保证)，哪些业务可以容许暂时不提供服务(利用静态页面替换)等，以及配合服务器核心指标，来后设置整体预案，比如： （1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； （2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； （3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； （4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 3.Redis备份和快速预热 (1)Redis数据备份和恢复 (2)快速缓存预热 4.提前演练 最后，建议还是在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，对高可用提前预演，提前发现问题。 缓存穿透缓存穿透是指查询一个一不存在的数据。例如：从缓存redis没有命中，需要从mysql数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决思路： 如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库。设置一个过期时间或者当有值的时候将缓存中的值替换掉即可。 可以给key设置一些格式规则，然后查询之前先过滤掉不符合规则的Key。 缓存并发这里的并发指的是多个redis的client同时set key引起的并发问题。其实redis自身就是单线程操作，多个client并发操作，按照先到先执行的原则，先到的先执行，其余的阻塞。当然，另外的解决方案是把redis.set操作放在队列中使其串行化，必须的一个一个执行。 缓存预热缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。 这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路： 直接写个缓存刷新页面，上线时手工操作下； 数据量不大，可以在项目启动的时候自动进行加载； 目的就是在系统上线前，将数据加载到缓存中。 二、Redis为什么是单线程，高并发快的3大原因详解Redis的高并发和快速原因 redis是基于内存的，内存的读写速度非常快； redis是单线程的，省去了很多上下文切换线程的时间； redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。 下面重点介绍单线程设计和IO多路复用核心设计快的原因。 为什么Redis是单线程的？1.官方答案 因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 2.性能指标 关于redis的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。 3.详细原因 1.不需要各种锁的性能消耗 Redis的数据结构并不全是简单的Key-Value，还有list，hash等复杂的结构，这些结构有可能会进行很细粒度的操作，比如在很长的列表后面添加一个元素，在hash当中添加或者删除一个对象。这些操作可能就需要加非常多的锁，导致的结果是同步开销大大增加。 总之，在单线程的情况下，就不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。 2.单线程多进程集群方案 单线程的威力实际上非常强大，每核心效率也非常高，多线程自然是可以比单线程有更高的性能上限，但是在今天的计算环境中，即使是单机多线程的上限也往往不能满足需要了，需要进一步摸索的是多服务器集群化的方案，这些方案中多线程的技术照样是用不上的。 所以单线程、多进程的集群不失为一个时髦的解决方案。 3.CPU消耗 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU。但是如果CPU成为Redis瓶颈，或者不想让服务器其他CUP核闲置，那怎么办？ 可以考虑多起几个Redis进程，Redis是key-value数据库，不是关系数据库，数据之间没有约束。只要客户端分清哪些key放在哪个Redis进程上就可以了。 Redis单线程的优劣势单进程单线程优势 代码更清晰，处理逻辑更简单不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗不存在多进程或者多线程导致的切换而消耗CPU 单进程单线程弊端 无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善； IO多路复用技术 redis 采用网络IO多路复用技术来保证在多连接的时候， 系统的高吞吐量。 多路-指的是多个socket连接，复用-指的是复用一个线程。多路复用主要有三种技术：select，poll，epoll。epoll是最新的也是目前最好的多路复用技术。 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），且Redis在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了Redis具有很高的吞吐量。 Redis高并发快总结 Redis是纯内存数据库，一般都是简单的存取操作，线程占用的时间很多，时间的花费主要集中在IO上，所以读取速度快。 再说一下IO，Redis使用的是非阻塞IO，IO多路复用，使用了单线程来轮询描述符，将数据库的开、关、读、写都转换成了事件，减少了线程切换时上下文的切换和竞争。 Redis采用了单线程的模型，保证了每个操作的原子性，也减少了线程的上下文切换和竞争。 另外，数据结构也帮了不少忙，Redis全程使用hash结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。 还有一点，Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。 三、Redis缓存和MySQL数据一致性方案详解 需求起因 在高并发的业务场景下，数据库大多数情况都是用户并发访问最薄弱的环节。所以，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问MySQL等数据库。 这个业务场景，主要是解决读数据从Redis缓存，一般都是按照下图的流程来进行业务操作。 读取缓存步骤一般没有什么问题，但是一旦涉及到数据更新：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。 不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。举一个例子： 如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。 如果先写了库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。 因为写和读是并发的，没法保证顺序,就会出现缓存和数据库的数据不一致的问题。 如来解决？这里给出两个解决方案，先易后难，结合业务和技术代价选择使用。 缓存和数据库一致性解决方案1.第一种方案：采用延时双删策略 在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。 伪代码如下： 123456public void write(String key,Object data)&#123; redis.delKey(key); db.updateData(data); Thread.sleep(500); redis.delKey(key);&#125; 具体的步骤就是： 先删除缓存；再写数据库；休眠500毫秒；再次删除缓存。 那么，这个500毫秒怎么确定的，具体该休眠多久呢？ 需要评估自己的项目的读数据业务逻辑的耗时。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 当然这种策略还要考虑redis和数据库主从同步的耗时。最后的的写数据的休眠时间：则在读数据业务逻辑的耗时基础上，加几百ms即可。比如：休眠1秒。 设置缓存过期时间 从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。所有的写操作以数据库为准，只要到达缓存过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。 该方案的弊端 结合双删策略+缓存超时设置，这样最差的情况就是在超时时间内数据存在不一致，而且又增加了写请求的耗时。 2、第二种方案：异步更新缓存(基于订阅binlog的同步机制) 技术整体思路： MySQL binlog增量订阅消费+消息队列+增量数据更新到redis 读Redis：热数据基本都在Redis 写MySQL:增删改都是操作MySQL 更新Redis数据：MySQL的数据操作binlog，来更新到Redis Redis更新 1）数据操作主要分为两大块： 一个是全量(将全部数据一次写入到redis)一个是增量（实时更新） 这里说的是增量,指的是mysql的update、insert、delate变更数据。 2）读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。 这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。 其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。 这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。 当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis。","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://alloceee.github.io/tags/redis/"}]},{"title":"HTTP常用的14种状态码","slug":"计算机网络/HTTP-常见状态码","date":"2020-07-30T05:04:42.000Z","updated":"2020-08-06T03:46:46.000Z","comments":true,"path":"2020/07/30/计算机网络/HTTP-常见状态码/","link":"","permalink":"https://alloceee.github.io/2020/07/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP-%E5%B8%B8%E8%A7%81%E7%8A%B6%E6%80%81%E7%A0%81/","excerpt":"","text":"状态码的职责是当客户端向服务器发送请求时，描述返回的请求结果。借助状态码，用户可以知道服务器端是正常处理了请求还是出现了错误。 状态码的类别： 类别 原因 1XX Informational（信息性状态码） 接受的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 2XX——表明请求被正常处理了1、200 OK：请求已正常处理。 2、204 No Content：请求处理成功，但没有任何资源可以返回给客户端，一般在只需要从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。 3、206 Partial Content：是对资源某一部分的请求，该状态码表示客户端进行了范围请求，而服务器成功执行了这部分的GET请求。响应报文中包含由Content-Range指定范围的实体内容。 3XX——表明浏览器需要执行某些特殊的处理以正确处理请求4、301 Moved Permanently：资源的uri已更新，你也更新下你的书签引用吧。永久性重定向，请求的资源已经被分配了新的URI，以后应使用资源现在所指的URI。 5、302 Found：资源的URI已临时定位到其他位置了，姑且算你已经知道了这个情况了。临时性重定向。和301相似，但302代表的资源不是永久性移动，只是临时性性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。 6、303 See Other：资源的URI已更新，你是否能临时按新的URI访问。该状态码表示由于请求对应的资源存在着另一个URL，应使用GET方法定向获取请求的资源。303状态码和302状态码有着相同的功能，但303状态码明确表示客户端应当采用GET方法获取资源，这点与302状态码有区别。 当301,302,303响应状态码返回时，几乎所有的浏览器都会把POST改成GET，并删除请求报文内的主体，之后请求会自动再次发送。 7、304 Not Modified：资源已找到，但未符合条件请求。该状态码表示客户端发送附带条件的请求时（采用GET方法的请求报文中包含If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since中任一首部）服务端允许请求访问资源，但因发生请求未满足条件的情况后，直接返回304.。 8、307 Temporary Redirect：临时重定向。与302有相同的含义。 4XX——表明客户端是发生错误的原因所在。9、400 Bad Request：服务器端无法理解客户端发送的请求，请求报文中可能存在语法错误。 10、401 Unauthorized：该状态码表示发送的请求需要有通过HTTP认证（BASIC认证，DIGEST认证）的认证信息。 11、403 Forbidden：不允许访问那个资源。该状态码表明对请求资源的访问被服务器拒绝了。（权限，未授权IP等） 12、404 Not Found：服务器上没有请求的资源。路径错误等。 5XX——服务器本身发生错误13、500 Internal Server Error：貌似内部资源出故障了。该状态码表明服务器端在执行请求时发生了错误。也有可能是web应用存在bug或某些临时故障。 14、503 Service Unavailable：抱歉，我现在正在忙着。该状态码表明服务器暂时处于超负载或正在停机维护，现在无法处理请求。","categories":[],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://alloceee.github.io/tags/HTTP/"}]},{"title":"HTTP和HTTPS的区别","slug":"计算机网络/HTTP和HTTPS的区别","date":"2020-07-15T03:21:02.000Z","updated":"2020-07-15T03:23:42.000Z","comments":true,"path":"2020/07/15/计算机网络/HTTP和HTTPS的区别/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP%E5%92%8CHTTPS%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"HTTP和HTTPS的区别HTTP明文传输，不安全 数据容易被拦截、篡改和攻击 HTTPS数据加密 身份验证 数据完整性 对称加密对称加密就是一个密钥，可以加密也可以解密 非对称加密非对称加密就是公钥加密，必须用私钥才能解密，私钥加密的内容，必须用公钥才能解密 HTTPS 请求HTTPS连接获取证书（公钥） 客户端给服务器发送（对称加密&lt;公钥&gt;）：随机数 的密文 客户端同时给服务器发送：（对称加密&lt;公钥&gt;）：随机数 + 私钥 的密文 服务器根据公钥解密出随机数，同时解密出私钥 客户端使用非对称加密进行数据传输，客户端使用公钥加密，服务器使用私钥解密","categories":[],"tags":[{"name":"HTTP和HTTPS","slug":"HTTP和HTTPS","permalink":"https://alloceee.github.io/tags/HTTP%E5%92%8CHTTPS/"}]},{"title":"ElasticSearch的使用","slug":"搜索引擎/搜索引擎-ElasticSearch","date":"2020-07-15T03:21:02.000Z","updated":"2020-07-15T03:22:04.000Z","comments":true,"path":"2020/07/15/搜索引擎/搜索引擎-ElasticSearch/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-ElasticSearch/","excerpt":"","text":"ElasticSearchWindows下安装elasticsearch在安装Elasticsearch引擎之前，必须安装ES需要的软件环境 注意：安装elasticsearch7.4运行需要jdk11及以上 elasticsearch与jdk有依赖关系 下载地址 配置环境变量 测试运行 点击/bin目录下的elasticsearch.bat开始安装 安装elasticsearch的head插件 es5以上版本安装head需要安装node和grunt(之前的直接用plugin命令即可安装) (一)从地址：https://nodejs.org/en/download/ 下载相应系统的msi，双击安装。 （二）安装完成用cmd进入安装目录执行 node -v可查看版本号 （三）执行 npm install -g grunt-cli 安装grunt ，安装完成后执行grunt -version查看是否安装成功，会显示安装的版本号 （四）开始安装head① 进入安装目录下的config目录，修改elasticsearch.yml文件.在文件的末尾加入以下代码 http.cors.enabled: true http.cors.allow-origin: “*” node.master: true node.data: true 然后去掉network.host: 192.168.0.1的注释并改为network.host: 0.0.0.0，去掉cluster.name；node.name；http.port的注释（也就是去掉#） elasticsearch-7.1.1 版本 yml文件需要这个 cluster.initial_master_nodes: [“node-1”] ②双击elasticsearch.bat重启es③在https://github.com/mobz/elasticsearch-head中下载head插件，选择下载zip ④解压到指定文件夹下，G:\\elasticsearch-6.4.1\\elasticsearch-head-master 进入该文件夹，修改G:\\elasticsearch-6.4.1\\elasticsearch-head-master\\Gruntfile.js 在对应的位置加上hostname:’*’ ⑤在G:\\elasticsearch-6.6.2\\elasticsearch-head-master 下执行npm install 安装完成后执行grunt server 或者npm run start 运行head插件，如果不成功重新安装grunt。成功如下 ⑥浏览器下访问http://localhost:9100/ 整合springboot参考博客 https://segmentfault.com/a/1190000018625101?utm_source=tag-newest https://blog.csdn.net/chen_2890/article/details/83895646 properties 123spring.data.elasticsearch.cluster-name=elasticsearchspring.data.elasticsearch.cluster-nodes=127.0.0.1:9300spring.data.elasticsearch.repositories.enabled=true pom.xml 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--集合工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;19.0&lt;/version&gt; &lt;/dependency&gt; ```java/** * * @Description:matchQuery *@Author: https://blog.csdn.net/chen_2890 */ @Test public void testMathQuery()&#123; // 创建对象 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); // 在queryBuilder对象中自定义查询 //matchQuery:底层就是使用的termQuery queryBuilder.withQuery(QueryBuilders.matchQuery(&quot;title&quot;,&quot;坚果&quot;)); //查询，search 默认就是分页查找 Page&lt;Item&gt; page = this.itemRepository.search(queryBuilder.build()); //获取数据 long totalElements = page.getTotalElements(); System.out.println(&quot;获取的总条数:&quot;+totalElements); for(Item item:page)&#123; System.out.println(item); &#125; } 12345678910111213141516171819202122 ```java /** * @Description: * termQuery:功能更强大，除了匹配字符串以外，还可以匹配 * int/long/double/float/.... * @Author: https://blog.csdn.net/chen_2890 */ @Test public void testTermQuery()&#123; NativeSearchQueryBuilder builder = new NativeSearchQueryBuilder(); builder.withQuery(QueryBuilders.termQuery(&quot;price&quot;,998.0)); // 查找 Page&lt;Item&gt; page = this.itemRepository.search(builder.build()); for(Item item:page)&#123; System.out.println(item); &#125; &#125; 12345678910111213/** * @Description:布尔查询 * @author: https://blog.csdn.net/chen_2890 */ @Test public void testBooleanQuery() &#123; NativeSearchQueryBuilder builder = new NativeSearchQueryBuilder(); builder.withQuery(QueryBuilders.boolQuery().must(QueryBuilders.matchQuery(&quot;title&quot;,&quot;华为&quot;)).must(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;华为&quot;))); // 查找 Page&lt;Item&gt; page = this.itemRepository.search(builder.build()); for (Item item : page) &#123; System.out.println(item); &#125; &#125; 123456789101112131415/** * @Description:模糊查询 uthor: https://blog.csdn.net/chen_2890 */ @Test public void testFuzzyQuery()&#123; NativeSearchQueryBuilder builder = new NativeSearchQueryBuilder(); builder.withQuery(QueryBuilders.fuzzyQuery(&quot;title&quot;,&quot;faceoooo&quot;)); Page&lt;Item&gt; page = this.itemRepository.search(builder.build()); for(Item item:page)&#123; System.out.println(item); &#125; &#125; Elasticsearch－基础介绍及索引原理分析最近在参与一个基于Elasticsearch作为底层数据框架提供大数据量(亿级)的实时统计查询的方案设计工作，花了些时间学习Elasticsearch的基础理论知识，整理了一下，希望能对Elasticsearch感兴趣/想了解的同学有所帮助。 同时也希望有发现内容不正确或者有疑问的地方，望指明，一起探讨，学习，进步。 介绍Elasticsearch 是一个分布式可扩展的实时搜索和分析引擎,一个建立在全文搜索引擎 Apache Lucene(TM) 基础上的搜索引擎.当然 Elasticsearch 并不仅仅是 Lucene 那么简单，它不仅包括了全文搜索功能，还可以进行以下工作: 分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索。 实时分析的分布式搜索引擎。 可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。 基本概念先说Elasticsearch的文件存储，Elasticsearch是面向文档型数据库，一条数据在这里就是一个文档，用JSON作为文档序列化的格式，比如下面这条用户数据： 12345678&#123; &quot;name&quot; : &quot;John&quot;, &quot;sex&quot; : &quot;Male&quot;, &quot;age&quot; : 25, &quot;birthDate&quot;: &quot;1990/05/01&quot;, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 用Mysql这样的数据库存储就会容易想到建立一张User表，有balabala的字段等，在Elasticsearch里这就是一个文档，当然这个文档会属于一个User的类型，各种各样的类型存在于一个索引当中。这里有一份简易的将Elasticsearch和关系型数据术语对照表: 123关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns)Elasticsearch ⇒ 索引(Index) ⇒ 类型(type) ⇒ 文档(Docments) ⇒ 字段(Fields) 一个 Elasticsearch 集群可以包含多个索引(数据库)，也就是说其中包含了很多类型(表)。这些类型中包含了很多的文档(行)，然后每个文档中又包含了很多的字段(列)。Elasticsearch的交互，可以使用Java API，也可以直接使用HTTP的Restful API方式，比如我们打算插入一条记录，可以简单发送一个HTTP的请求： 12345678PUT /megacorp/employee/1 &#123; &quot;name&quot; : &quot;John&quot;, &quot;sex&quot; : &quot;Male&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 更新，查询也是类似这样的操作，具体操作手册可以参见Elasticsearch权威指南 索引Elasticsearch最关键的就是提供强大的索引能力了，其实InfoQ的这篇时间序列数据库的秘密(2)——索引写的非常好，我这里也是围绕这篇结合自己的理解进一步梳理下，也希望可以帮助大家更好的理解这篇文章。 Elasticsearch索引的精髓： 一切设计都是为了提高搜索的性能 另一层意思：为了提高搜索的性能，难免会牺牲某些其他方面，比如插入/更新，否则其他数据库不用混了。前面看到往Elasticsearch里插入一条记录，其实就是直接PUT一个json的对象，这个对象有多个fields，比如上面例子中的name, sex, age, about, interests，那么在插入这些数据到Elasticsearch的同时，Elasticsearch还默默1的为这些字段建立索引–倒排索引，因为Elasticsearch最核心功能是搜索。 Elasticsearch是如何做到快速索引的InfoQ那篇文章里说Elasticsearch使用的倒排索引比关系型数据库的B-Tree索引快，为什么呢？ 什么是B-Tree索引?上大学读书时老师教过我们，二叉树查找效率是logN，同时插入新的节点不必移动全部节点，所以用树型结构存储索引，能同时兼顾插入和查询的性能。因此在这个基础上，再结合磁盘的读取特性(顺序读/随机读)，传统关系型数据库采用了B-Tree/B+Tree这样的数据结构： 为了提高查询的效率，减少磁盘寻道次数，将多个值作为一个数组通过连续区间存放，一次寻道读取多个数据，同时也降低树的高度。 什么是倒排索引? 继续上面的例子，假设有这么几条数据(为了简单，去掉about, interests这两个field): 12345| ID | Name | Age | Sex || -- |:------------:| -----:| -----:| | 1 | Kate | 24 | Female| 2 | John | 24 | Male| 3 | Bill | 29 | Male ID是Elasticsearch自建的文档id，那么Elasticsearch建立的索引如下: Name: 12345| Term | Posting List || -- |:----:|| Kate | 1 || John | 2 || Bill | 3 | Age: 1234| Term | Posting List || -- |:----:|| 24 | [1,2] || 29 | 3 | Sex: 1234| Term | Posting List || -- |:----:|| Female | 1 || Male | [2,3] | Posting ListElasticsearch分别为每个field都建立了一个倒排索引，Kate, John, 24, Female这些叫term，而[1,2]就是Posting List。Posting list就是一个int的数组，存储了所有符合某个term的文档id。 看到这里，不要认为就结束了，精彩的部分才刚开始… 通过posting list这种索引方式似乎可以很快进行查找，比如要找age=24的同学，爱回答问题的小明马上就举手回答：我知道，id是1，2的同学。但是，如果这里有上千万的记录呢？如果是想通过name来查找呢？ Term DictionaryElasticsearch为了能快速找到某个term，将所有的term排个序，二分法查找term，logN的查找效率，就像通过字典查找一样，这就是Term Dictionary。现在再看起来，似乎和传统数据库通过B-Tree的方式类似啊，为什么说比B-Tree的查询快呢？ Term IndexB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，就像字典里的索引页一样，A开头的有哪些term，分别在哪页，可以理解term index是一颗树： 这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。 所以term index不需要存下所有的term，而仅仅是他们的一些前缀与Term Dictionary的block之间的映射关系，再结合FST(Finite State Transducers)的压缩技术，可以使term index缓存到内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘随机读的次数。 这时候爱提问的小明又举手了:”那个FST是神马东东啊?” 一看就知道小明是一个上大学读书的时候跟我一样不认真听课的孩子，数据结构老师一定讲过什么是FST。但没办法，我也忘了，这里再补下课： FSTs are finite-state machines that map a term (byte sequence) to an arbitrary output. 假设我们现在要将mop, moth, pop, star, stop and top(term index里的term前缀)映射到序号：0，1，2，3，4，5(term dictionary的block位置)。最简单的做法就是定义个Map&lt;string, integer=””&gt;，大家找到自己的位置对应入座就好了，但从内存占用少的角度想想，有没有更优的办法呢？答案就是：FST(理论依据在此，但我相信99%的人不会认真看完的) ⭕️表示一种状态 –&gt;表示状态的变化过程，上面的字母/数字表示状态变化和权重 将单词分成单个字母通过⭕️和–&gt;表示出来，0权重不显示。如果⭕️后面出现分支，就标记权重，最后整条路径上的权重加起来就是这个单词对应的序号。 FSTs are finite-state machines that map a term (byte sequence) to an arbitrary output. FST以字节的方式存储所有的term，这种压缩方式可以有效的缩减存储空间，使得term index足以放进内存，但这种方式也会导致查找时需要更多的CPU资源。 后面的更精彩，看累了的同学可以喝杯咖啡…… 压缩技巧Elasticsearch里除了上面说到用FST压缩term index外，对posting list也有压缩技巧。小明喝完咖啡又举手了:”posting list不是已经只存储文档id了吗？还需要压缩？” 嗯，我们再看回最开始的例子，如果Elasticsearch需要对同学的性别进行索引(这时传统关系型数据库已经哭晕在厕所……)，会怎样？如果有上千万个同学，而世界上只有男/女这样两个性别，每个posting list都会有至少百万个文档id。 Elasticsearch是如何有效的对这些文档id压缩的呢？ Frame Of Reference 增量编码压缩，将大数变小数，按字节存储 首先，Elasticsearch要求posting list是有序的(为了提高搜索的性能，再任性的要求也得满足)，这样做的一个好处是方便压缩，看下面这个图例： 如果数学不是体育老师教的话，还是比较容易看出来这种压缩技巧的。 原理就是通过增量，将原来的大数变成小数仅存储增量值，再精打细算按bit排好队，最后通过字节存储，而不是大大咧咧的尽管是2也是用int(4个字节)来存储。 Roaring bitmaps说到Roaring bitmaps，就必须先从bitmap说起。Bitmap是一种数据结构，假设有某个posting list： [1,3,4,7,10] 对应的bitmap就是： [1,0,1,1,0,0,1,0,0,1] 非常直观，用0/1表示某个值是否存在，比如10这个值就对应第10位，对应的bit值是1，这样用一个字节就可以代表8个文档id，旧版本(5.0之前)的Lucene就是用这样的方式来压缩的，但这样的压缩方式仍然不够高效，如果有1亿个文档，那么需要12.5MB的存储空间，这仅仅是对应一个索引字段(我们往往会有很多个索引字段)。于是有人想出了Roaring bitmaps这样更高效的数据结构。 Bitmap的缺点是存储空间随着文档个数线性增长，Roaring bitmaps需要打破这个魔咒就一定要用到某些指数特性： 将posting list按照65535为界限分块，比如第一块所包含的文档id范围在065535之间，第二块的id范围是65536131071，以此类推。再用&lt;商，余数&gt;的组合表示每一组id，这样每组里的id范围都在0~65535内了，剩下的就好办了，既然每组id不会变得无限大，那么我们就可以通过最有效的方式对这里的id存储。 细心的小明这时候又举手了:”为什么是以65535为界限?” 程序员的世界里除了1024外，65535也是一个经典值，因为它=2^16-1，正好是用2个字节能表示的最大数，一个short的存储单位，注意到上图里的最后一行“If a block has more than 4096 values, encode as a bit set, and otherwise as a simple array using 2 bytes per value”，如果是大块，用节省点用bitset存，小块就豪爽点，2个字节我也不计较了，用一个short[]存着方便。 那为什么用4096来区分大块还是小块呢？ 个人理解：都说程序员的世界是二进制的，4096*2bytes ＝ 8192bytes &lt; 1KB, 磁盘一次寻道可以顺序把一个小块的内容都读出来，再大一位就超过1KB了，需要两次读。 联合索引上面说了半天都是单field索引，如果多个field索引的联合查询，倒排索引如何满足快速查询的要求呢？ 利用跳表(Skip list)的数据结构快速做“与”运算，或者 利用上面提到的bitset按位“与” 先看看跳表的数据结构： 将一个有序链表level0，挑出其中几个元素到level1及level2，每个level越往上，选出来的指针元素越少，查找时依次从高level往低查找，比如55，先找到level2的31，再找到level1的47，最后找到55，一共3次查找，查找效率和2叉树的效率相当，但也是用了一定的空间冗余来换取的。 假设有下面三个posting list需要联合索引： 如果使用跳表，对最短的posting list中的每个id，逐个在另外两个posting list中查找看是否存在，最后得到交集的结果。 如果使用bitset，就很直观了，直接按位与，得到的结果就是最后的交集。 总结和思考Elasticsearch的索引思路: 将磁盘里的东西尽量搬进内存，减少磁盘随机读取次数(同时也利用磁盘顺序读特性)，结合各种奇技淫巧的压缩算法，用及其苛刻的态度使用内存。 所以，对于使用Elasticsearch进行索引时需要注意: 不需要索引的字段，一定要明确定义出来，因为默认是自动建索引的 同样的道理，对于String类型的字段，不需要analysis的也需要明确定义出来，因为默认也是会analysis的 选择有规律的ID很重要，随机性太大的ID(比如java的UUID)不利于查询 关于最后一点，个人认为有多个因素: 其中一个(也许不是最重要的)因素: 上面看到的压缩算法，都是对Posting list里的大量ID进行压缩的，那如果ID是顺序的，或者是有公共前缀等具有一定规律性的ID，压缩比会比较高； 另外一个因素: 可能是最影响查询性能的，应该是最后通过Posting list里的ID到磁盘中查找Document信息的那步，因为Elasticsearch是分Segment存储的，根据ID这个大范围的Term定位到Segment的效率直接影响了最后查询的性能，如果ID是有规律的，可以快速跳过不包含该ID的Segment，从而减少不必要的磁盘读次数，具体可以参考这篇如何选择一个高效的全局ID方案(评论也很精彩) 1.启动内存不足 12345678[es@VM_0_6_centos bin]$ ./elasticsearchOpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=NOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c5330000, 986513408, 0) failed; error=&#x27;Cannot allocate memory&#x27; (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 986513408 bytes for committing reserved memory.# An error report file with more information is saved as:# /home/tom/data/elasticsearch-6.2.2/hs_err_pid1439.log 123[es@VM_0_6_centos config]$ lselasticsearch.yml jvm.options log4j2.properties[es@VM_0_6_centos config]$ vim jvm.options #修改内容 -Xms200m -Xmx200m 12[es@VM_0_6_centos bin]$ ./elasticsearch -d[es@VM_0_6_centos bin]$ OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N 启动正常","categories":[],"tags":[{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://alloceee.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}]},{"title":"HTTPS","slug":"计算机网络/HTTPS","date":"2020-07-15T03:21:02.000Z","updated":"2020-07-16T03:30:10.000Z","comments":true,"path":"2020/07/15/计算机网络/HTTPS/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTPS/","excerpt":"","text":"下面我们来一起学习一下 HTTPS ，首先问你一个问题，为什么有了 HTTP 之后，还需要有 HTTPS ？我突然有个想法，为什么我们面试的时候需要回答标准答案呢？为什么我们不说出我们自己的想法和见解，却要记住一些所谓的标准回答呢？技术还有正确与否吗？ HTTPS 为什么会出现一个新技术的出现必定是为了解决某种问题的，那么 HTTPS 解决了 HTTP 的什么问题呢？ HTTPS 解决了什么问题一个简单的回答可能会是 HTTP 它不安全。由于 HTTP 天生明文传输的特性，在 HTTP 的传输过程中，任何人都有可能从中截获、修改或者伪造请求发送，所以可以认为 HTTP 是不安全的；在 HTTP 的传输过程中不会验证通信方的身份，因此 HTTP 信息交换的双方可能会遭到伪装，也就是没有用户验证；在 HTTP 的传输过程中，接收方和发送方并不会验证报文的完整性，综上，为了解决上述问题，HTTPS 应用而生。 什么是 HTTPS你还记得 HTTP 是怎么定义的吗？HTTP 是一种 超文本传输协议(Hypertext Transfer Protocol) 协议，它 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范，那么我们看一下 HTTPS 是如何定义的 HTTPS 的全称是 Hypertext Transfer Protocol Secure，它用来在计算机网络上的两个端系统之间进行安全的交换信息(secure communication)，它相当于在 HTTP 的基础上加了一个 Secure 安全的词眼，那么我们可以给出一个 HTTPS 的定义：HTTPS 是一个在计算机世界里专门在两点之间安全的传输文字、图片、音频、视频等超文本数据的约定和规范。HTTPS 是 HTTP 协议的一种扩展，它本身并不保传输的证安全性，那么谁来保证安全性呢？在 HTTPS 中，使用传输层安全性(TLS)或安全套接字层(SSL)对通信协议进行加密。也就是 HTTP + SSL(TLS) = HTTPS。 HTTPS 做了什么HTTPS 协议提供了三个关键的指标 加密(Encryption)， HTTPS 通过对数据加密来使其免受窃听者对数据的监听，这就意味着当用户在浏览网站时，没有人能够监听他和网站之间的信息交换，或者跟踪用户的活动，访问记录等，从而窃取用户信息。 数据一致性(Data integrity)，数据在传输的过程中不会被窃听者所修改，用户发送的数据会完整的传输到服务端，保证用户发的是什么，服务器接收的就是什么。 身份认证(Authentication)，是指确认对方的真实身份，也就是证明你是你（可以比作人脸识别），它可以防止中间人攻击并建立用户信任。 有了上面三个关键指标的保证，用户就可以和服务器进行安全的交换信息了。那么，既然你说了 HTTPS 的种种好处，那么我怎么知道网站是用 HTTPS 的还是 HTTP 的呢？给你两幅图应该就可以解释了。 HTTPS 协议其实非常简单，RFC 文档很小，只有短短的 7 页，里面规定了新的协议名，默认端口号443，至于其他的应答模式、报文结构、请求方法、URI、头字段、连接管理等等都完全沿用 HTTP，没有任何新的东西。 也就是说，除了协议名称和默认端口号外（HTTP 默认端口 80），HTTPS 协议在语法、语义上和 HTTP 一样，HTTP 有的，HTTPS 也照单全收。那么，HTTPS 如何做到 HTTP 所不能做到的安全性呢？关键在于这个 S 也就是 SSL/TLS 。 什么是 SSL/TLS认识 SSL/TLSTLS(Transport Layer Security) 是 SSL(Secure Socket Layer) 的后续版本，它们是用于在互联网两台计算机之间用于身份验证和加密的一种协议。 注意：在互联网中，很多名称都可以进行互换。 我们都知道一些在线业务（比如在线支付）最重要的一个步骤是创建一个值得信赖的交易环境，能够让客户安心的进行交易，SSL/TLS 就保证了这一点，SSL/TLS 通过将称为 X.509 证书的数字文档将网站和公司的实体信息绑定到加密密钥来进行工作。每一个密钥对(key pairs) 都有一个 私有密钥(private key) 和 公有密钥(public key)，私有密钥是独有的，一般位于服务器上，用于解密由公共密钥加密过的信息；公有密钥是公有的，与服务器进行交互的每个人都可以持有公有密钥，用公钥加密的信息只能由私有密钥来解密。 什么是 X.509：X.509 是公开密钥证书的标准格式，这个文档将加密密钥与（个人或组织）进行安全的关联。 X.509 主要应用如下 SSL/TLS 和 HTTPS 用于经过身份验证和加密的 Web 浏览 通过 S/MIME 协议签名和加密的电子邮件 代码签名：它指的是使用数字证书对软件应用程序进行签名以安全分发和安装的过程。 通过使用由知名公共证书颁发机构（例如SSL.com）颁发的证书对软件进行数字签名，开发人员可以向最终用户保证他们希望安装的软件是由已知且受信任的开发人员发布；并且签名后未被篡改或损害。 还可用于文档签名 还可用于客户端认证 政府签发的电子身份证（详见 https://www.ssl.com/article/pki-and-digital-certificates-for-government/） 我们后面还会讨论。 HTTPS 的内核是 HTTPHTTPS 并不是一项新的应用层协议，只是 HTTP 通信接口部分由 SSL 和 TLS 替代而已。通常情况下，HTTP 会先直接和 TCP 进行通信。在使用 SSL 的 HTTPS 后，则会先演变为和 SSL 进行通信，然后再由 SSL 和 TCP 进行通信。也就是说，HTTPS 就是身披了一层 SSL 的 HTTP。（我都喜欢把骚粉留在最后。。。） SSL 是一个独立的协议，不只有 HTTP 可以使用，其他应用层协议也可以使用，比如 SMTP(电子邮件协议)、Telnet(远程登录协议) 等都可以使用。 探究 HTTPS我说，你起这么牛逼的名字干嘛，还想吹牛批？你 HTTPS 不就抱上了 TLS/SSL 的大腿么，咋这么牛批哄哄的，还想探究 HTTPS，瞎胡闹，赶紧改成 TLS 是我主，赞美我主。 SSL 即安全套接字层，它在 OSI 七层网络模型中处于第五层，SSL 在 1999 年被 IETF(互联网工程组)更名为 TLS ，即传输安全层，直到现在，TLS 一共出现过三个版本，1.1、1.2 和 1.3 ，目前最广泛使用的是 1.2，所以接下来的探讨都是基于 TLS 1.2 的版本上的。 TLS 用于两个通信应用程序之间提供保密性和数据完整性。TLS 由记录协议、握手协议、警告协议、变更密码规范协议、扩展协议等几个子协议组成，综合使用了对称加密、非对称加密、身份认证等许多密码学前沿技术（如果你觉得一项技术很简单，那你只是没有学到位，任何技术都是有美感的，牛逼的人只是欣赏，并不是贬低）。 说了这么半天，我们还没有看到 TLS 的命名规范呢，下面举一个 TLS 例子来看一下 TLS 的结构（可以参考 https://www.iana.org/assignments/tls-parameters/tls-parameters.xhtml） 1ECDHE-ECDSA-AES256-GCM-SHA384 这是啥意思呢？我刚开始看也有点懵啊，但其实是有套路的，因为 TLS 的密码套件比较规范，基本格式就是 密钥交换算法 - 签名算法 - 对称加密算法 - 摘要算法 组成的一个密码串，有时候还有分组模式，我们先来看一下刚刚是什么意思 使用 ECDHE 进行密钥交换，使用 ECDSA 进行签名和认证，然后使用 AES 作为对称加密算法，密钥的长度是 256 位，使用 GCM 作为分组模式，最后使用 SHA384 作为摘要算法。 TLS 在根本上使用对称加密和 非对称加密 两种形式。 对称加密在了解对称加密前，我们先来了解一下密码学的东西，在密码学中，有几个概念：明文、密文、加密、解密 明文(Plaintext)，一般认为明文是有意义的字符或者比特集，或者是通过某种公开编码就能获得的消息。明文通常用 m 或 p 表示 密文(Ciphertext)，对明文进行某种加密后就变成了密文 加密(Encrypt)，把原始的信息（明文）转换为密文的信息变换过程 解密(Decrypt)，把已经加密的信息恢复成明文的过程。 对称加密(Symmetrical Encryption)顾名思义就是指加密和解密时使用的密钥都是同样的密钥。只要保证了密钥的安全性，那么整个通信过程也就是具有了机密性。 TLS 里面有比较多的加密算法可供使用，比如 DES、3DES、AES、ChaCha20、TDEA、Blowfish、RC2、RC4、RC5、IDEA、SKIPJACK 等。目前最常用的是 AES-128, AES-192、AES-256 和 ChaCha20。 DES 的全称是 Data Encryption Standard(数据加密标准) ，它是用于数字数据加密的对称密钥算法。尽管其 56 位的短密钥长度使它对于现代应用程序来说太不安全了，但它在加密技术的发展中具有很大的影响力。 3DES 是从原始数据加密标准（DES）衍生过来的加密算法，它在 90 年代后变得很重要，但是后面由于更加高级的算法出现，3DES 变得不再重要。 AES-128, AES-192 和 AES-256 都是属于 AES ，AES 的全称是Advanced Encryption Standard(高级加密标准)，它是 DES 算法的替代者，安全强度很高，性能也很好，是应用最广泛的对称加密算法。 ChaCha20 是 Google 设计的另一种加密算法，密钥长度固定为 256 位，纯软件运行性能要超过 AES，曾经在移动客户端上比较流行，但 ARMv8 之后也加入了 AES 硬件优化，所以现在不再具有明显的优势，但仍然算得上是一个不错算法。 （其他可自行搜索） 加密分组对称加密算法还有一个分组模式 的概念，对于 GCM 分组模式，只有和 AES，CAMELLIA 和 ARIA 搭配使用，而 AES 显然是最受欢迎和部署最广泛的选择，它可以让算法用固定长度的密钥加密任意长度的明文。 最早有 ECB、CBC、CFB、OFB 等几种分组模式，但都陆续被发现有安全漏洞，所以现在基本都不怎么用了。最新的分组模式被称为 AEAD（Authenticated Encryption with Associated Data），在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。 比如 ECDHE_ECDSA_AES128_GCM_SHA256 ，表示的是具有 128 位密钥， AES256 将表示 256 位密钥。GCM 表示具有 128 位块的分组密码的现代认证的关联数据加密（AEAD）操作模式。 我们上面谈到了对称加密，对称加密的加密方和解密方都使用同一个密钥，也就是说，加密方必须对原始数据进行加密，然后再把密钥交给解密方进行解密，然后才能解密数据，这就会造成什么问题？这就好比《小兵张嘎》去送信（信已经被加密过），但是嘎子还拿着解密的密码，那嘎子要是在途中被鬼子发现了，那这信可就是被完全的暴露了。所以，对称加密存在风险。 非对称加密非对称加密(Asymmetrical Encryption) 也被称为公钥加密，相对于对称加密来说，非对称加密是一种新的改良加密方式。密钥通过网络传输交换，它能够确保及时密钥被拦截，也不会暴露数据信息。非对称加密中有两个密钥，一个是公钥，一个是私钥，公钥进行加密，私钥进行解密。公开密钥可供任何人使用，私钥只有你自己能够知道。 使用公钥加密的文本只能使用私钥解密，同时，使用私钥加密的文本也可以使用公钥解密。公钥不需要具有安全性，因为公钥需要在网络间进行传输，非对称加密可以解决密钥交换的问题。网站保管私钥，在网上任意分发公钥，你想要登录网站只要用公钥加密就行了，密文只能由私钥持有者才能解密。而黑客因为没有私钥，所以就无法破解密文。 非对称加密算法的设计要比对称算法难得多（我们不会探讨具体的加密方式），常见的比如 DH、DSA、RSA、ECC 等。 其中 RSA 加密算法是最重要的、最出名的一个了。例如 DHE_RSA_CAMELLIA128_GCM_SHA256。它的安全性基于 整数分解，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。 ECC（Elliptic Curve Cryptography）也是非对称加密算法的一种，它基于椭圆曲线离散对数的数学难题，使用特定的曲线方程和基点生成公钥和私钥， ECDHE 用于密钥交换，ECDSA 用于数字签名。 TLS 是使用对称加密和非对称加密 的混合加密方式来实现机密性。 混合加密RSA 的运算速度非常慢，而 AES 的加密速度比较快，而 TLS 正是使用了这种混合加密方式。在通信刚开始的时候使用非对称算法，比如 RSA、ECDHE ，首先解决密钥交换的问题。然后用随机数产生对称算法使用的会话密钥（session key），再用公钥加密。对方拿到密文后用私钥解密，取出会话密钥。这样，双方就实现了对称密钥的安全交换。 现在我们使用混合加密的方式实现了机密性，是不是就能够安全的传输数据了呢？还不够，在机密性的基础上还要加上完整性、身份认证的特性，才能实现真正的安全。而实现完整性的主要手段是 摘要算法(Digest Algorithm) 摘要算法如何实现完整性呢？在 TLS 中，实现完整性的手段主要是 摘要算法(Digest Algorithm)。摘要算法你不清楚的话，MD5 你应该清楚，MD5 的全称是 Message Digest Algorithm 5，它是属于密码哈希算法(cryptographic hash algorithm)的一种，MD5 可用于从任意长度的字符串创建 128 位字符串值。尽管 MD5 存在不安全因素，但是仍然沿用至今。MD5 最常用于验证文件的完整性。但是，它还用于其他安全协议和应用程序中，例如 SSH、SSL 和 IPSec。一些应用程序通过向明文加盐值或多次应用哈希函数来增强 MD5 算法。 什么是加盐？在密码学中，盐就是一项随机数据，用作哈希数据，密码或密码的单向函数的附加输入。盐用于保护存储中的密码。例如 什么是单向？就是在说这种算法没有密钥可以进行解密，只能进行单向加密，加密后的数据无法解密，不能逆推出原文。 我们再回到摘要算法的讨论上来，其实你可以把摘要算法理解成一种特殊的压缩算法，它能够把任意长度的数据压缩成一种固定长度的字符串，这就好像是给数据加了一把锁。 除了常用的 MD5 是加密算法外，SHA-1(Secure Hash Algorithm 1) 也是一种常用的加密算法，不过 SHA-1 也是不安全的加密算法，在 TLS 里面被禁止使用。目前 TLS 推荐使用的是 SHA-1 的后继者：SHA-2。 SHA-2 的全称是Secure Hash Algorithm 2 ，它在 2001 年被推出，它在 SHA-1 的基础上做了重大的修改，SHA-2 系列包含六个哈希函数，其摘要（哈希值）分别为 224、256、384 或 512 位：SHA-224, SHA-256, SHA-384, SHA-512。分别能够生成 28 字节、32 字节、48 字节、64 字节的摘要。 有了 SHA-2 的保护，就能够实现数据的完整性，哪怕你在文件中改变一个标点符号，增加一个空格，生成的文件摘要也会完全不同，不过 SHA-2 是基于明文的加密方式，还是不够安全，那应该用什么呢？ 安全性更高的加密方式是使用 HMAC，在理解什么是 HMAC 前，你需要先知道一下什么是 MAC。 MAC 的全称是message authentication code，它通过 MAC 算法从消息和密钥生成，MAC 值允许验证者（也拥有秘密密钥）检测到消息内容的任何更改，从而保护了消息的数据完整性。 HMAC 是 MAC 更进一步的拓展，它是使用 MAC 值 + Hash 值的组合方式，HMAC 的计算中可以使用任何加密哈希函数，例如 SHA-256 等。 现在我们又解决了完整性的问题，那么就只剩下一个问题了，那就是认证，认证怎么做的呢？我们在向服务器发送数据的过程中，黑客（攻击者）有可能伪装成任何一方来窃取信息。它可以伪装成你，来向服务器发送信息，也可以伪装称为服务器，接受你发送的信息。那么怎么解决这个问题呢？ 认证如何确定你自己的唯一性呢？我们在上面的叙述过程中出现过公钥加密，私钥解密的这个概念。提到的私钥只有你一个人所有，能够辨别唯一性，所以我们可以把顺序调换一下，变成私钥加密，公钥解密。使用私钥再加上摘要算法，就能够实现数字签名，从而实现认证。 到现在，综合使用对称加密、非对称加密和摘要算法，我们已经实现了加密、数据认证、认证，那么是不是就安全了呢？非也，这里还存在一个数字签名的认证问题。因为私钥是是自己的，公钥是谁都可以发布，所以必须发布经过认证的公钥，才能解决公钥的信任问题。 所以引入了 CA，CA 的全称是 Certificate Authority，证书认证机构，你必须让 CA 颁布具有认证过的公钥，才能解决公钥的信任问题。 全世界具有认证的 CA 就几家，分别颁布了 DV、OV、EV 三种，区别在于可信程度。DV 是最低的，只是域名级别的可信，EV 是最高的，经过了法律和审计的严格核查，可以证明网站拥有者的身份（在浏览器地址栏会显示出公司的名字，例如 Apple、GitHub 的网站）。不同的信任等级的机构一起形成了层级关系。 通常情况下，数字证书的申请人将生成由私钥和公钥以及证书签名请求（CSR）组成的密钥对。CSR是一个编码的文本文件，其中包含公钥和其他将包含在证书中的信息（例如域名，组织，电子邮件地址等）。密钥对和 CSR生成通常在将要安装证书的服务器上完成，并且 CSR 中包含的信息类型取决于证书的验证级别。与公钥不同，申请人的私钥是安全的，永远不要向 CA（或其他任何人）展示。 生成 CSR 后，申请人将其发送给 CA，CA 会验证其包含的信息是否正确，如果正确，则使用颁发的私钥对证书进行数字签名，然后将其发送给申请人。 总结 本篇文章我们主要讲述了 HTTPS 为什么会出现 ，HTTPS 解决了 HTTP 的什么问题，HTTPS 和 HTTP 的关系是什么，TLS 和 SSL 是什么，TLS 和 SSL 解决了什么问题？如何实现一个真正安全的数据传输？","categories":[],"tags":[{"name":"HTTP和HTTPS","slug":"HTTP和HTTPS","permalink":"https://alloceee.github.io/tags/HTTP%E5%92%8CHTTPS/"}]},{"title":"几种消息中间件的对比","slug":"消息中间件/消息中间件-对比","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:48:58.000Z","comments":true,"path":"2020/07/15/消息中间件/消息中间件-对比/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-%E5%AF%B9%E6%AF%94/","excerpt":"","text":"本次主要对比：ActiveMQ、kafka、RocketMQ、RabbitMQ衡量消息中间件的指标：服务性能，数据存储，集群架构 1.ActiveMQActiveMQ是Apache出品的，最流行的能力，强劲的开源消息总线，并且他完全支持Java的JMs规范。丰富的API，多种集群构建模式使得它成为业界老牌的消息中间件，在中小型企业应用广泛。但是相比于kafka，rabbitmq等MQ来说，性能太弱，在如今的高并发，大数据处理的场景下显得力不从心，经常会出现一些小问题，消息延迟，堆积，堵塞等，不过其多种集群架构是优势。 如下所示主要的两种集群架构 image ActiveMQ的两种集群架构 master-slave模式:通过zk确立一个主节点，主节点对外提供服务，从节点不对外提供服务。当主节点不可用，另外一个节点就转成主节点，对外提供服务，已达到高可用的目的。 network模式就相当于两组master-slave组合在一起。 2.kafkakafka是linkedin开源的分布式发布-订阅消息系统，目前归属于Apache的顶级项目。主要特点是基于pull模式来处理消息消费，追求高吞吐量，一开始的目的是日志的收集和传输。0.8版本开始支持复制，不支持事务，对消息的丢失，重复，错误没有严格要求 适用于产生大量数据的互联网服务的数据收集业务。在廉价的服务器上都能有很高的性能，这个主要是基于操作系统底层的pagecache，不用内存胜似使用内存。 image Kafka集群 Kafka集群也是采用zk进行集群，当一个数据存放在一个节点中，会通过relicate同步到其他节点，所以我们不需要更多的关注kafka有可能丢失消息，因为其他节点会有这份数据，除非你这个地区的kafka都挂了。可靠性高的场景不适用。 3.RocketMQRocketMQ是阿里开源的，目前是也是Apache的顶级项目，纯Java开发，具有高吞吐量，高可用，适合大规模分布式系统应用的特点。其思路起源于kafka，它对消息的可靠传输以及事务性做了优化，目前在阿里被广泛应用于交易/充值/流计算/消息推送/日志流式处理/Binglog分发等场景。不过其维护是一个痛点。不过它能保证消息的顺序性，集群模式也丰富，在双十一等高并发场景承受上亿访问，三大指标都很好，但是它的商业版要收费！！！ image RocketMQ集群 它刚开始也是依赖zk做集群的，但是觉得太慢就自己开发了Name Server。 4.RabbitMQRabbitMQ是使用erlang语言开发的开源消息队列系统，基于AMQP协议来实现。AMQP的主要特征是面向消息/队列/路由(包括点对点的发布/订阅)可靠性，安全。AMQP协议更多用在企业系统内，对数据一致性/稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。rabbitMQ的可靠性很高，性能比不上kafka，但是也很高了，集群模式也有多种。 image 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，吞吐量比RabbitMQ和Kafka要低一个数量级 万级，吞吐量比RocketMQ和Kafka要低一个数量级 topic数量对吞吐量的影响 可用性 消息可靠性 时效性 功能支持 优劣总结 ActiveMQ比较成熟，文档齐全，但是目前维护较少，社区活跃，RabbitMQ性能很强，RocketMQ和ali有很大关系，可用性很高，性能不错，非常稳定成熟。Kafka在大数据领域应用较多。","categories":[],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://alloceee.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"消息中间件-常见面试题","slug":"消息中间件/消息中间件-常见面试题","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:48:10.000Z","comments":true,"path":"2020/07/15/消息中间件/消息中间件-常见面试题/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"概述 什么是消息中间件？和RPC有何区别？ 消息中间件使用场景介绍 消息中间件(ActiveMQ、RabbitMQ、RocketMQ、Kafka)简介及对比 消息中间件的编年史ActiveMq JMS规范 什么是JMS（Java Messaging Service）规范？ 包含要素 消息类型 P2P模型 Topic(PUB\\SUB)模型 ActiveMQ使用 安装和部署 原生ActiveMQ的API编程 与Spring集成 Xml配置方式 SpringBoot Request-Response模式 实战：用户注册的异步处理 ActiveMQ高级特性和用法 嵌入式MQ 消息存储的持久化机制 消息持久订阅 消息的可靠性 通配符式分层订阅 死信队列DLQ(Dead Letter Queue) 镜像队列 虚拟主题 组合Destinations 实战：限时订单 企业级高可用集群部署方案 Shared File System DB Replicated LevelDB Store Broker-ClusterRabbitMq AMQP规范和RabbitMQ基本概念 要素 生产者、消费者、消息 信道 交换器、队列、绑定、路由键 消息的确认 交换器类型 Direct Fanout Topic 虚拟主机 RabbitMQ在Windows下安装和运行 原生Java客户端使用 消息发布时的权衡 失败通知 发布者确认 事务 备用交换器 消息消费时的权衡 消息的获得方式 QoS预取模式 可靠性和性能的权衡 消息的拒绝 消息的拒绝方式 死信交换器 控制队列 临时队列 永久队列 队列级别消息过期 消息的属性 属性列表 消息的持久化 与Spring集成 Xml配置方式 SpringBoot 实战：应用解耦 安装配置 下载安装和日常管理 web监控平台 集群化与镜像队列Kafka Kafka入门 Kafka中的基本概念 为什么选择Kafka Kafka的安装和配置参数 Kafka的生产者和消费者 消息的发送和接收 生产者和消费者的配置 消费者群组和再均衡 消费者中的提交和偏移量 序列化和反序列化 深入理解Kafka 控制器和复制 请求处理流程 物理存储原理 保证Kafka的可靠数据传递 数据管道和流式处理入门 数据管道基本概念 流式处理基本概念和设计模式常见面试题 为什么使用消息队列 消息队列有什么优点和缺点? 常见消息队列的比较 消息的去重 消息的可靠性传输 消息的顺序性","categories":[],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://alloceee.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"网络架构基础","slug":"计算机网络/网络架构基础","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:47:36.000Z","comments":true,"path":"2020/07/15/计算机网络/网络架构基础/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/","excerpt":"","text":"海量数据的解决方案 缓存和页面静态化 数据库优化 表结构优化 SQL语句优化 分区 分表：mycat 分库分表中间件 索引优化 使用存储过程代替直接操作 分离活跃数据：可以根据最后一次查询时间，判断非活跃用户，将活跃数据单独分到一个表中作为默认表，不活跃数据分到另一个表中，先从默认表中查找数据，找不到再从不活跃表中查找数据，提高查询效率。 批量读取和延迟修改：原理是通过减少操作的次数来提高效率。批量读取是将多次查询合并到一次中进行，比如对同一个请求中的数据批量读取，在高并发的情况下还可以将多个请求的查询合并到一次进行，如3-5s内请求合并一起统一查询一次数据库。延迟修改主要针对高并发并且频繁修改（包括新增）的数据，如一些统计数据，这种情况可以先将需要修改的数据暂时保存到缓存中，然后定时将缓存中的数据保存到数据库找那个，程序在读取数据时可以同时读取数据中的缓存中的数据。 读写分离：读写分离的本质是对数据库进行集群，这样就可以在高并发的情况下将数据库的操作分配到多个数据库服务器去处理从而降低单台服务器的压力，不过由于数据库的特殊性——每台服务器所保存的数据都需要一致，所以数据同步就成了数据库集群中最核心的问题。一般情况是将写操作交给专门的一台服务器处理，这台专门负责写的服务器叫做主服务器。主服务器数据进行增删查改之后将数据从底层同步到别的服务器（从服务器），读数据的时候到从服务器读取，从服务器可以多台，如果从服务器过多，主服务器可以先同步一部分从服务器，然后再同步另外一部分。 分布式数据库：是将不同的表存放到不同的数据库中然后再放到不同的服务器。这样在处理请求时，如果需要调用多个表，则可以让多态服务器同时处理，从而提高处理速度。 数据库集群（读写分离）的作用是将多个请求分配到不同的服务器处理，从而减轻单台服务器的压力，而分布式数据是解决单个请求本身就非常复杂的问题，他可以将单个请求分配到多个处理器处理，使用分布式后的每个节点还可以同时使用读写分离，从而组成多个节点群。 NoSQL和Hadoop：Hadoop专门针对大数据处理，底层数据的存储思路类似于分布式加集群，不过Hadoop是将同一个表中的数据分成多块保存到多个节点（分布式），而且每一块数据都有多个节点保存（集群），这里集群除了可以并行处理相同的数据，还可以保证数据的稳定性，在其中一个节点出问题后数据不会丢失。 高并发的解决方案 应用和静态资源分离：将静态资源保存到专门的服务器中，一般会使用专门的域名去访问，通过不同的域名可以让浏览器直接访问资源服务器而不需要再访问应用服务器了 页面缓存：是将应用生成的页面缓存起来，这样就不需要每次都重新生成页面了。可以使用Nginx服务器就可以使用它自带的缓存功能，当然也可以使用专门的Squid服务器。 集群和分布式：集群是每台服务器都具有相同的功能，处理请求时调用哪台服务器都可以，主要起到分流作用，分布式是将不同的业务放到不同的服务器中，处理一个请求可能需要用到多台服务器，这样就可以提高一个请求的处理速度，而且集群和分布式也可以同时使用。集群有两个方式：一种是静态资源集群，另一种是应用程序集群。 反向代理：指的是客户端直接访问的服务器并不是真正提供服务，它从别的度武器获取资源然后将结果返回给用户的。 CDN：其实是一种特殊的集群页面缓存服务器，它和普通集群的多台页面缓冲服务器比主要是它存放的位置分布在全国各地，当接收到用户的请求后会将请求分配到最合适的CDN服务器节点获取数据。 底层优化：通过网络传输协议进行优化。 小结：网站架构的整个演变过程主要是围绕大数据和高并发这两个问题展开的。解决方案主要分为使用缓存和使用多资源两种类型。多资源主要指多存储（包括多内存）、多CPU和多网络，对于多资源来说又可以分为单个资源处理一个完整的请求和多个资源各做处理一个请求两种类型，如多存储和多CPU中的集群和分布式，多网络中的CDN和静态资源分离。","categories":[],"tags":[{"name":"架构","slug":"架构","permalink":"https://alloceee.github.io/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"HTTP协议","slug":"计算机网络/计算机网络-HTTP协议","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:41:22.000Z","comments":true,"path":"2020/07/15/计算机网络/计算机网络-HTTP协议/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-HTTP%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"HTTP协议HTTP协议是应用层协议，在TCP/IP协议接受到数据之后需要通过HTTP协议来解析才可以使用。 请求报文和响应报文： 请求报文 1234请求行：方法（请求类型） URL HTTP版本头部：键值对的属性主体 响应报文 1234请求行：HTTP版本 状态码 简短原因头部：键值对的属性主体 常见的状态码 200 OK 客户端请求成功 301 Moved Permanently （永久移除），请求的URL已移走。Response中应该包含一个Location URL,说明资源现在所处的位置 302 found 重定向 400 Bad Request 客户端请求有语法错误，不能被服务器解析 404 表示没有找到请求的资源 500 表示服务端内部错误 要方法 典型用法 典型状态码 安全？ 幂等？ GET - 获取表示- 变更时获取表示（缓存） 200（OK） - 表示已在响应中发出204（无内容） - 资源有空表示301（Moved Permanently） - 资源的 URI 已被更新303（See Other） - 其他（如，负载均衡）304（not modified）- 资源未更改（缓存）400 （bad request）- 指代坏请求（如，参数错误）404 （not found）- 资源不存在406 （not acceptable）- 服务端不支持所需表示500 （internal server error）- 通用错误响应503 （Service Unavailable）- 服务端当前无法处理请求 是 是 DELETE - 删除资源 200 （OK）- 资源已被删除301 （Moved Permanently）- 资源的 URI 已更改 303 （See Other）- 其他，如负载均衡400 （bad request）- 指代坏请求 t 404 （not found）- 资源不存在 409 （conflict）- 通用冲突500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务端当前无法处理请求 否 是 PUT - 用客户端管理的实例号创建一个资源- 通过替换的方式更新资源- 如果未被修改，则更新资源（乐观锁） 200 （OK）- 如果已存在资源被更改 201 （created）- 如果新资源被创建301（Moved Permanently）- 资源的 URI 已更改303 （See Other）- 其他（如，负载均衡）400 （bad request）- 指代坏请求404 （not found）- 资源不存在406 （not acceptable）- 服务端不支持所需表示 /p&gt;409 （conflict）- 通用冲突412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突）415 （unsupported media type）- 接受到的表示不受支持500 （internal server error）- 通用错误响应503 （Service Unavailable）- 服务当前无法处理请求 否 是 POST - 使用服务端管理的（自动产生）的实例号创建资源- 创建子资源- 部分更新资源- 如果没有被修改，则不过更新资源（乐观锁） 200（OK）- 如果现有资源已被更改 201（created）- 如果新资源被创建 202（accepted）- 已接受处理请求但尚未完成（异步处理）301（Moved Permanently）- 资源的 URI 被更新 303（See Other）- 其他（如，负载均衡）400（bad request）- 指代坏请求 404 （not found）- 资源不存在 406 （not acceptable）- 服务端不支持所需表示 409 （conflict）- 通用冲突 412 （Precondition Failed）- 前置条件失败（如执行条件更新时的冲突） 415 （unsupported media type）- 接受到的表示不受支持500 （internal server error）- 通用错误响应 503 （Service Unavailable）- 服务当前无法处理请求 否 否","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"Ping命令的使用","slug":"计算机网络/计算机网络-Ping","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:42:26.000Z","comments":true,"path":"2020/07/15/计算机网络/计算机网络-Ping/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-Ping/","excerpt":"","text":"一、概览对于 ping 命令，想必只要是程序员都知道吧？当我们检查网络情况的时候，最先使用的命令肯定是 ping 命令吧？一般我们用 ping 查看网络情况，主要是检查两个指标，第一个是看看是不是超时，第二个看看是不是延迟太高。如果超时那么肯定是网络有问题啦（禁 ping 情况除外），如果延迟太高，网络情况肯定也是很糟糕的。那么对于 ping 命令的原理，ping 是如何检查网络的？大家之前有了解吗？接下来我们来跟着 ping 命令走一圈，看看 ping 是如何工作的。 二、环境准备和抓包2.1 环境准备 1.抓包工具。我这里使用 Wireshark。 2.我准备了两台电脑，进行 ping 的操作。ip 地址分别为： A 电脑：192.168.2.135 MAC 地址：98:22:EF:E8:A8:87 B 电脑：192.168.2.179 MAC 地址：90:A4:DE:C2:DF:FE 2.2 抓包操作 打开 Wireshark，选取指定的网卡进行抓包，进行 ping 操作，在 A 电脑上 ping B 电脑的 ip。 抓包情况如下： 这里先简单的介绍下 Wireshark 的控制面板，这个面板包含 7 个字段，分别是： NO：编号 Time：包的时间戳 Source：源地址 Destination：目标地址 Protocol：协议 Length：包长度 Info：数据包附加信息 三、深入解析上图中抓包编号 54-132 显示的就是整个 ping 命令的过程，我们知道 ping 命令不是依托于 TCP 或者 UDP 这种传输层协议的，而是依托于 ICMP 协议实现的，那么什么是 ICMP 协议呢？这里简单介绍下： 3.1 ICMP 协议的产生背景 [RFC792] 中说明了 ICMP 产生的原因：由于互联网之间通讯会涉及很多网关和主机，为了能够报告数据错误，所以产生了 ICMP 协议。也就是说 ICMP 协议就是为了更高效的转发 IP 数据报和提高交付成功的机会。 3.2 ICMP 协议的数据格式 根据上图我们知道了 ICMP 协议头包含 4 个字节，头部主要用来说明类型和校验 ICMP 报文。下图是对应的类型和代码释义列表，我们后面分析抓包的时候会用到。 简单介绍完了 ICMP，那么抓包过程中出现的 ARP 协议是什么呢？我们同样来简单解释下： 3.3 ARP 协议 我们知道，在一个局域网中，计算机通信实际上是依赖于 MAC 地址进行通信的，那么 ARP（Address Resolution Protocol）的作用就是根据 IP 地址查找出对应 ip 地址的 MAC 地址。 3.4 Ping 过程解析 了解了上面的基础概念后，我们来分析下抓包的数据。图 b 的流程如下： A 电脑（192.168.2.135）发起 ping 请求，ping 192.168.2.179。 A 电脑广播发起 ARP 请求，查询 192.168.2.179 的 MAC 地址。 B 电脑应答 ARP 请求，向 A 电脑发起单向应答，告诉 A 电脑自己的 MAC 地址为 90:A4:DE:C2:DF:FE。 知道了 MAC 地址后，开始进行真正的 ping 请求，由于 B 电脑可以根据 A 电脑发送的请求知道源 MAC 地址，所有就可以根据源 MAC 地址进行响应了。 上面的请求过程我画成流程图比较直观一点： 观察仔细的朋友可能已经发现，Ping4 次请求和响应结束后，还有一次 B 电脑对 A 电脑的 ARP 请求，这是为什么呢？这里我猜测应该是有 2 个原因： 1.由于 ARP 有缓存机制，为了防止 ARP 过期，结束后重新更新下 ARP 缓存，保证下次请求能去往正确的路径，如果 ARP 过期就会导致出现一次错误，从而影响测试准确性。 2.由于 ping 命令的响应时间是根据请求包和响应包的时间戳计算出来的，所以一次 ARP 过程也是会消耗时间。这里提前缓存最新的 ARP 结果就是节省了下次 ping 的 arp 时间。 为了验证我们的猜测，我再进行一次 ping 操作，抓包看看是不是和我们猜测的一样。此时，计算机里面已经有了 ARP 的缓存，我们执行 ARP -a 看看缓存的 arp 列表： 我们看看第二次 ping 的抓包： 我们看到上图中在真正 ping 之前并没有进行一次 ARP 请求，这也就是说，直接拿了缓存中的 arp 来执行了，另外当 B 计算机进行响应之前还是进行了一次 ARP 请求，它还是要确认下之前的 ARP 缓存是否为正确的。然后结束 ping 操作之后，同样在发一次 ARP 请求，更新下自己的 ARP 缓存。这里和我们的猜想基本一致。 弄懂了 ping 的流程之后我们来解析下之前解释的 ICMP 数据结果是否和抓包的一致。我们来点击一个 ping request 看看 ICMP 协议详情： 图中红框内就行 ICMP 协议的详情了，这里的 Type=8, code=0, 校验是正确。我们对比就知道了这是一个请求报文。我们再点击 Response frame:57，这里说明响应报文在序号 57。详情如下： 上图的响应报文，Type=0, code=0。这里知道就是响应报文了，然后最后就是根据请求和响应的时间戳计算出来的响应延迟。3379.764 ms-3376.890 ms=2.874 ms。 四、总结我们分析了一次完整的 ping 请求过程，ping 命令是依托于 ICMP 协议的，ICMP 协议的存在就是为了更高效的转发 IP 数据报和提高交付成功的机会。ping 命令除了依托于 ICMP，在局域网下还要借助于 ARP 协议，ARP 协议能根据 IP 地址查出计算机 MAC 地址。ARP 是有缓存的，为了保证 ARP 的准确性，计算机会更新 ARP 缓存。","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"TCP的三次握手和四次挥手","slug":"计算机网络/计算机网络-TCP的三次握手和四次挥手","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:42:42.000Z","comments":true,"path":"2020/07/15/计算机网络/计算机网络-TCP的三次握手和四次挥手/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-TCP%E7%9A%84%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","excerpt":"","text":"TCP的三次握手和四次挥手两个序号和三个标志位 seq 表示所传数据的序号 TCP传输时每一个字节都有一个序号，发送数据时会将数据的第一个序号发送给对方，接收方会按序号检查是否接受完整了，如果没有接受完整就需要重新传送，这样就可以保证数据的完整性。 ack 表示确认号 ACK 确认位 SYN 同步位 FIN 终止位 用来在数据传输完毕后释放连接 创建TCP连接 客户端 - 发送带有SYN标志的数据包 - 一次握手 - 服务端 服务端 - 发送带有SYN/ACK标志的数据包 - 二次握手 - 客户端 客户端 - 发送带有ACK标志的数据包 - 三次握手 - 服务端 四次挥手 断开TCP连接","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"WebSocket的介绍和使用","slug":"计算机网络/计算机网络-WebSocket","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:40:24.000Z","comments":true,"path":"2020/07/15/计算机网络/计算机网络-WebSocket/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-WebSocket/","excerpt":"","text":"WebSocket介绍WebSocket协议RFC 6455提供了一种标准化方法，可通过单个TCP连接在客户端和服务器之间建立全双工双向通信通道。它来自HTTP的不同TCP协议，但设计为使用端口80和443通过HTTP工作，并允许重用现有的防火墙规则。运行在TLS之上时，默认使用443端口。 WebSocket使客户端和服务器之间的数据交换变得更加简单，允许服务器主动向客户端推送数据。在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。 WebSocket交互以HTTP请求开始，该HTTP请求使用HTTP &quot;Upgrade&quot;标头升级，或者在这种情况下切换到WebSocket协议： 12345678GET / spring-websocket-portfolio / portfolio HTTP / 1.1Host：localhost：8080Upgrade：websocket Connection：UpgradeSec-WebSocket-Key：Uc9l9TMkWGbHFD2qnFHltg ==Sec-WebSocket-Protocol：v10.stomp，v11.stompSec-WebSocket-Version：13Origin：http：// localhost：8080 Connection必须设置为Upgrade，表示客户端希望连接升级 Upgrade字段必须设施webSocket，表示连接升级到WebSocket协议 Sec-WebSocket-Key是随机字符串，服务器会用这些数据来构造出一个SHA-1的信息摘要。如此操作，避免普通HTTP请求被误以为WebScoket协议 Sec-WebSocket-Version表示支持的版本。RFC6455要求使用的版本是13，之前的版本均应当弃用 Origin字段可选，通常用来表示在浏览器中发起此WebSocket连接所在的页面，类似Referer。但是，与Referer不同的是，Origin只包含了协议和主机名称。 WebSocket和Socket的区别Socket是传输控制层协议，WebScoket是应用层协议。 客户端介绍WebSocket 对象创建1var socket = new WebSocket(url,[protocol]); 参数 描述 url 指定连接的URL protocol 参数可选，指定可接收的子协议 WebSocket 方法 方法 描述 Socket.send() 使用连接发送数据 Socket.close() 关闭连接 12345function sendMessage() &#123; if (socket.readyState === 1) &#123; socket.send(JSON.stringify(message)); &#125; &#125; 需要先打开连接再调用send方法，因此最好先做状态判断 WebSocket 事件 事件 事件处理程序 描述 open Socket.onopen 连接建立时触发 message Socket.onmessage 客户端接收服务端数据时触发 error Socket.onerror 通信发生错误时触发 close Socket.onclose 连接关闭时触发 WebSocket 属性 属性 描述 Socket.readyState 只读属性readyState表示连接状态，可以是以下值： 0 -表示连接尚未建立 1 -表示连接已建立，可以进行通信 2 -表示连接正在进行关闭 3 -表示连接已经关闭或者连接不能打开 Socket.bufferedAmount 只读属性bufferedAmount已被send()放入正在队列中等待传输，但是还没有发出的UTF-8文本字节数 12345678910111213141516171819202122232425262728293031323334353637&lt;script&gt; $(function () &#123; var socket; function openWebSocket() &#123; //判断浏览器是否支持websocket if ((typeof WebSocket) == &quot;undefined&quot;) &#123; console.log(&quot;您的浏览器不支持WebSocket&quot;); &#125; else &#123; var socketUrl = &#x27;ws://192.168.2.234:1234/socket/$&#123;user.uuid&#125;&#x27;; socket = new WebSocket(socketUrl); socket.onopen = function (ev) &#123; console.log(&quot;websocket open&quot;); &#125;; socket.onmessage = function (ev) &#123; console.log(&quot;收到的信息 : &quot;+ev.data) socket.onerror = function (ev) &#123; console.log(&quot;websocket 已经关闭&quot;) &#125;; socket.onclose = function (ev) &#123; console.log(&quot;websocket 发生了错误&quot;) &#125;; &#125; &#125;; openWebSocket(); $(&quot;#sendContent&quot;).on(&#x27;click&#x27;,function () &#123; //声明一个消息 var message = &#123; nickname: &#x27;$&#123;user.nickname&#125;&#x27;, content: $(&#x27;#message&#x27;).html() &#125;; if (socket.readyState === 1) &#123; console.log(&quot;发送的信息 : &quot;+JSON.stringify(message)); socket.send(JSON.stringify(message)); &#125; &#125;); &#125;)&lt;/script&gt; 服务端介绍方法注解 方法 作用 @ServerEndpoint() 服务端的路由映射 @OnOpen() 当客户端握手时被调用，只会被调用一次 @OnMessage() 当客户端发来消息时被调用 @OnClose() 当客户端被关闭时调用 @OnError() 当发生错误时被调用 1234567891011121314151617181920@OnOpenpublic void onOpen(Session session, @PathParam(&quot;userId&quot;)String userId)&#123; this.session = session;&#125;@OnMessagepublic void onMessage(String message,Session session) &#123; //注入的参数为前端传过来的数据 System.out.println(message); &#125;@OnClosepublic void onClose (@PathParam(&quot;userId&quot;)String userId)&#123; webSocketList.remove(userId); //在线人数减一 subOnlineCount();&#125;@OnErrorpublic void onError(Session session,Throwable error)&#123; System.out.println(&quot;发生错误&quot;); error.printStackTrace();&#125; Session代表会话，一次连接就是一个会话。意味着，N个客户端有N个session WebSocket + SpringBoot实现即时通信（群聊） 因为websocket注册的bean默认是自己管理，没有托管给spring，所以，此类是为了将websocket的bean托管给spring容器 123456789101112131415@Configuration@ConditionalOnWebApplicationpublic class WebSocketConfig &#123; //使用boot内置tomcat时需要注入此bean @Bean public ServerEndpointExporter serverEndpointExporter() &#123; return new ServerEndpointExporter(); &#125; @Bean public MySpringConfigurator mySpringConfigurator() &#123; return new MySpringConfigurator(); &#125;&#125; 123456789101112131415161718/** * 以websocketConfig.java注册的bean是由自己管理的，需要使用配置托管给spring管理 * @author AlmostLover */public class MySpringConfigurator extends ServerEndpointConfig.Configurator implements ApplicationContextAware &#123; private static volatile BeanFactory context; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; MySpringConfigurator.context = applicationContext; &#125; @Override public &lt;T&gt; T getEndpointInstance(Class&lt;T&gt; clazz) throws InstantiationException &#123; return context.getBean(clazz); &#125;&#125;0 前端页面后端控制器异常解决java.io.EOFException java.lang.IllegalStateException: The WebSocket session [1] has been closed and no method (apart from close()) may be called on a closed session","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"JS原型和原型链","slug":"大前端/JavaScript/Javascript-原型链","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:32:28.000Z","comments":true,"path":"2020/07/15/大前端/JavaScript/Javascript-原型链/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/Javascript-%E5%8E%9F%E5%9E%8B%E9%93%BE/","excerpt":"","text":"js的原型和原型链构造函数创建对象： 123456function Person() &#123;&#125;var person = new Person();person.name = &#x27;Kevin&#x27;;console.log(person.name) // Kevin Person 就是一个构造函数，我们使用 new 创建了一个实例对象 person prototype每个函数都有一个 prototype 属性 每一个JavaScript对象(null除外)在创建的时候就会与之关联另一个对象，这个对象就是我们所说的原型，每一个对象都会从原型”继承”属性。 12345678910function Person() &#123;&#125;// 虽然写在注释里，但是你要注意：// prototype是函数才会有的属性Person.prototype.name = &#x27;Kevin&#x27;;var person1 = new Person();var person2 = new Person();console.log(person1.name) // Kevinconsole.log(person2.name) // Kevin image.png proto每一个JavaScript对象(除了 null )都具有的一个属性，叫proto，这个属性会指向该对象的原型 12345function Person() &#123;&#125;var person = new Person();console.log(person.__proto__ === Person.prototype); // true image.png constructor每个原型都有一个 constructor 属性指向关联的构造函数 实例原型指向构造函数 1234function Person() &#123;&#125;console.log(Person === Person.prototype.constructor); // true image.png 12345678910function Person() &#123;&#125;var person = new Person();console.log(person.__proto__ == Person.prototype) // trueconsole.log(Person.prototype.constructor == Person) // true// 顺便学习一个ES5的方法,可以获得对象的原型console.log(Object.getPrototypeOf(person) === Person.prototype) // true 实例与原型12345678910111213function Person() &#123;&#125;Person.prototype.name = &#x27;Kevin&#x27;;var person = new Person();person.name = &#x27;Daisy&#x27;;console.log(person.name) // Daisydelete person.name;console.log(person.name) // Kevin 在这个例子中，我们给实例对象 person 添加了 name 属性，当我们打印 person.name 的时候，结果自然为 Daisy。 但是当我们删除了 person 的 name 属性时，读取 person.name，从 person 对象中找不到 name 属性就会从 person 的原型也就是 person.proto ，也就是 Person.prototype中查找，幸运的是我们找到了 name 属性，结果为 Kevin。 原型与原型123var obj = new Object();obj.name = &#x27;Kevin&#x27;console.log(obj.name) // Kevin image.png 原型链1console.log(Object.prototype.__proto__ === null) // true image.png JavaScript 默认并不会复制对象的属性，相反，JavaScript 只是在两个对象之间创建一个关联，这样，一个对象就可以通过委托访问另一个对象的属性和函数，所以与其叫继承，委托的说法反而更准确些","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://alloceee.github.io/tags/JavaScript/"}]},{"title":"JavaScript-常用功能","slug":"大前端/JavaScript/Javascript-常用功能","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:32:18.000Z","comments":true,"path":"2020/07/15/大前端/JavaScript/Javascript-常用功能/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E5%A4%A7%E5%89%8D%E7%AB%AF/JavaScript/Javascript-%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD/","excerpt":"","text":"一键复制clipboard.min.js123456789101112131、引入clipboard.min.js2、&lt;button class=&quot;copyBtn&quot; data-clipboard-text=&quot;&quot; data-clipboard-action=&quot;copy&quot;&gt;点击复制&lt;/button&gt;3、$(&quot;button&quot;).attr(&quot;data-clipboard-text&quot;, &quot;jiaomeichen.github.io&quot;); var clipboard = new Clipboard(&#x27;.copyBtn&#x27;); clipboard.on(&#x27;success&#x27;, function(e) &#123; layer.msg(&quot;已复制成功&quot; + e.text); &#125;); clipboard.on(&#x27;error&#x27;, function(e) &#123; layer.open(&#123; title: &#x27;提示&#x27;, content: &#x27;您的浏览器可能不支持，请手动复制~&#x27; &#125;); &#125;); 判断微端与PC端123456var UA = window.navigator.userAgent.toLowerCase();var isAndroid = UA.indexOf(&#x27;android&#x27;)&gt;0;var isIOS = /iphone|ipad|ipod|ios/.test(UA);if(!(isAndroid ||isIOS))&#123; // &#125; 123456789101112131415161718192021222324252627282930313233343536var browser=&#123; versions:function()&#123; var u = navigator.userAgent, app = navigator.appVersion; return &#123; trident: u.indexOf(&#x27;Trident&#x27;) &gt; -1, //IE内核 presto: u.indexOf(&#x27;Presto&#x27;) &gt; -1, //opera内核 webKit: u.indexOf(&#x27;AppleWebKit&#x27;) &gt; -1, //苹果、谷歌内核 gecko: u.indexOf(&#x27;Gecko&#x27;) &gt; -1 &amp;&amp; u.indexOf(&#x27;KHTML&#x27;) == -1,//火狐内核 mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端 ios: !!u.match(/\\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端 android: u.indexOf(&#x27;Android&#x27;) &gt; -1 || u.indexOf(&#x27;Adr&#x27;) &gt; -1, //android终端 iPhone: u.indexOf(&#x27;iPhone&#x27;) &gt; -1 , //是否为iPhone或者QQHD浏览器 iPad: u.indexOf(&#x27;iPad&#x27;) &gt; -1, //是否iPad webApp: u.indexOf(&#x27;Safari&#x27;) == -1, //是否web应该程序，没有头部与底部 weixin: u.indexOf(&#x27;MicroMessenger&#x27;) &gt; -1, //是否微信 （2015-01-22新增） qq: u.match(/\\sQQ/i) == &quot; qq&quot; //是否QQ &#125;; &#125;(), language:(navigator.browserLanguage || navigator.language).toLowerCase()&#125; //判断是否IE内核if(browser.versions.trident)&#123; alert(&quot;is IE&quot;); &#125;//判断是否webKit内核if(browser.versions.webKit)&#123; alert(&quot;is webKit&quot;); &#125;//判断是否移动端if(browser.versions.mobile||browser.versions.android||browser.versions.ios)&#123; alert(&quot;移动端&quot;); &#125;//检测浏览器语言 currentLang = navigator.language; //判断除IE外其他浏览器使用语言if(!currentLang)&#123;//判断IE浏览器使用语言 currentLang = navigator.browserLanguage;&#125;alert(currentLang); 将阿拉伯数字转化为中文1234567891011121314151617toChinesNum: function (num) &#123; let changeNum = [&#x27;零&#x27;, &#x27;一&#x27;, &#x27;二&#x27;, &#x27;三&#x27;, &#x27;四&#x27;, &#x27;五&#x27;, &#x27;六&#x27;, &#x27;七&#x27;, &#x27;八&#x27;, &#x27;九&#x27;]; //changeNum[0] = &quot;零&quot; let unit = [&quot;&quot;, &quot;十&quot;, &quot;百&quot;, &quot;千&quot;, &quot;万&quot;]; num = parseInt(num); let getWan = (temp) =&gt; &#123; let strArr = temp.toString().split(&quot;&quot;).reverse(); let newNum = &quot;&quot;; for (var i = 0; i &lt; strArr.length; i++) &#123; newNum = (i == 0 &amp;&amp; strArr[i] == 0 ? &quot;&quot; : (i &gt; 0 &amp;&amp; strArr[i] == 0 &amp;&amp; strArr[i - 1] == 0 ? &quot;&quot; : changeNum[strArr[i]] + (strArr[i] == 0 ? unit[0] : unit[i]))) + newNum; &#125; return newNum; &#125;; let overWan = Math.floor(num / 10000); let noWan = num % 10000; if (noWan.toString().length &lt; 4) noWan = &quot;0&quot; + noWan; return overWan ? getWan(overWan) + &quot;万&quot; + getWan(noWan) : getWan(num); &#125; 获取url参数1234567891011getQueryVariable: function (variable) &#123; var query = window.location.search.substring(1); var vars = query.split(&quot;&amp;&quot;); for (var i = 0; i &lt; vars.length; i++) &#123; var pair = vars[i].split(&quot;=&quot;); if (pair[0] == variable) &#123; return pair[1]; &#125; &#125; return (false); &#125; JQUERY获取html标签自定义属性值或data值//获取属性值 1 12&lt;div id=&quot;text&quot; value=&quot;黑哒哒的盟友&quot;&gt;&lt;div&gt; $(&quot;#text&quot;).attr(&quot;value&quot;); //获取自定义属性值 2 12&lt;div id=&quot;text&quot; value=&quot;123&quot; data_obj=&quot;黑哒哒的盟友&quot;&gt;&lt;div&gt; $(&quot;#text&quot;).attr(&quot;data_obj&quot;); //获取data值 3 12&lt;div id=&quot;text&quot; value=&quot;123&quot; data-name=&quot;黑哒哒的盟友&quot;&gt;&lt;div&gt; $(&quot;#text&quot;).data(&quot;name&quot;); 让select下拉click事件不能让select下来，只需要在你添加点击事件的元素上加pointer-events: none样式，代表该元素不再是点击目标直接穿过，就可以直接点击到下面的元素","categories":[],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://alloceee.github.io/tags/JavaScript/"}]},{"title":"前端小技巧：CSS雪碧图","slug":"大前端/other/前端小技巧：CSS雪碧图","date":"2020-07-15T02:39:02.000Z","updated":"2020-08-08T04:00:30.000Z","comments":true,"path":"2020/07/15/大前端/other/前端小技巧：CSS雪碧图/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E5%A4%A7%E5%89%8D%E7%AB%AF/other/%E5%89%8D%E7%AB%AF%E5%B0%8F%E6%8A%80%E5%B7%A7%EF%BC%9ACSS%E9%9B%AA%E7%A2%A7%E5%9B%BE/","excerpt":"","text":"前端小技巧：CSS雪碧图当在浏览器里输入一个URL地址的时候，你会感觉无数张图片“唰唰唰”的闪出来了。在这个过程中，浏览器会把这个网站的主资源（就是Html文件）拉取回来，然后开始分析网页中的Js，Img之类的标签，然后再去拉取这些图片和资源，这些后拉取的资源称为「子资源」。 「主资源」和「子资源」只是我们人类对资源定义的不同方式，其实对于浏览器来说，他们的请求方式都是发起一个Http请求，经历三次握手，并把文件拉取回来，一般的浏览器内核只能同时并发4，5个网络请求，所以大量的小图片特别影响性能，不但网页加载完成时间慢，还可能影响一些重要的JS逻辑，使得网页响应也变慢，卡死等等。对于浏览器来说，发起一个Http请求，来回几百毫秒的耗时，已经是相当高的资源耗费，只是人类不曾感受到，其实浏览器已经无数次喊叫：“太NMD慢了，哪个SB写的代码……”。 例如上图，一个网页的一小块区域，就三张小图标，浏览器要辛辛苦苦三次才能把这些小图标取回来，如果是50张呢，显然不可接受。面对这样的情况就只能束手就擒吗？显然优化的关键途径就是减少网络请求次数，并且还得把图片都下发下来，并能够灵活使用，那就把所有小图标拼成一张大图吧，如下图。 当前，用一次网络请求就可以下载下来三张图片了，而且文件的大小也较三张图片小一些（因为很多文件的格式信息和头信息已经共用了），大大降低了网络请求和带宽的消耗。然后呢，如何在Html或CSS中引用这些图片呢？ 这就不得不提到一个CSS属性叫做background-position，利用Ta，可以指定图片的位置，也就能把这张图片作为一个背景放在某个位置了，我们来看两句CSS代码。 利用这个属性，来标识图片相对于容器的位置（图中的坐标分别标识X，Y两个方向的偏移，这些都是示意的值哦，不是真实的值），最后再把这个类选择器应用到相应的容器就好了。 这种技术就称为CSS Sprite，中文翻译为雪碧图，Ta主要解决的是小图片过多以及耗费网络资源的问题，核心原理就是将图片合并成一张大图下发到客户程序，并利用属性来对其进行定位和切割的技术。 其实在游戏当中，很多也是利用了类似的思想，比如一个小人走路的时候，一共由8张图片组成，那么一般这8张图片会合并为一张大图，每一帧都循环播放这张大图中的不同坐标下的小图片来构建小人的行走动作。 其实在追求高性能的app中，很多也是利用了类似的思想，比如对性能极致要求的话，一个Titlebar上有5个icon，那5个icon是可以合并为一张大图的，这样只会有一次IO，减少了IO次数，会对效率提升比较大，分割图片都是内存操作，会非常迅速。","categories":[],"tags":[{"name":"前端","slug":"前端","permalink":"https://alloceee.github.io/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"数据库索引模块","slug":"数据库/MySQL/数据库索引模块","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:46:42.000Z","comments":true,"path":"2020/07/15/数据库/MySQL/数据库索引模块/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%9D%97/","excerpt":"","text":"如何设计一个关系型数据库程序实例： 存储管理 减少对数据的访问，提供缓存机制（全页缓存） SQL解析 日志管理：缓存不易过大，提供淘汰策略，在数据库修改之后及时修改缓存，进行主从同步和灾难恢复 权限划分 容灾机制 索引管理 锁管理 影响数据的运行瓶颈就是IO 存储（文件系统）索引模块为什么使用索引原始数据的查询方法就是将全表放入内存中，全表扫描轮询找到需要查找的数据，少量数据可以，大量数据严重影响性能。 什么样的信息能成为索引主键：唯一键以及普通键 索引的数据结构生成索引，建立二叉查找树进行二分查找（二叉树–平衡二叉树–红黑树）因为是二分查找，时间复杂度为O(logn) 生成索引，建立B-Tree结构进行查找因为添加结点导致二叉树的深度增加，虽然进行旋转可以使其继续维持平衡，但是增加了IO，降低了性能，因此提供了B-Tree结构（平衡多路查找树）。 每个结点最多有m个孩子，这个结点就是M阶B树 每个存储块包括关键字和指向结点的指针，最多有几个孩子取决于存储块的容量和数据库的相关配置 生成索引，建立B+-Tree结构进行查找 生成索引，建立Hash结构进行查找 密集索引和稀疏索引的区别密集索引密集索引文件中每一个搜索码值都对应一个索引值，就是叶子节点保存的不只是键值，还保存了位于同一行记录里的其他列信息，由于密集索引决定了表的物理排列顺序，一个表只有一个物理排列顺序，所以一个表只能创建一个密集索引。 稀疏索引稀疏索引文件只为索引码的某些值建立索引项，比如InnoDB的其他索引只存了键位信息和主键，MyISAM的所有索引都是稀疏索引。 MyISAM–》 主键索引，唯一键索引，还是普通索引 —都是 稀疏索引 额外知识InnoDB 若一个主键被定义，该主键则作为密集索引 若没有主键被定义，该表的第一个唯一非空索引则作为密集索引 若不满足以上条件，InnoDB内部会生成一个隐藏主键（密集索引） 非主键索引存储相关键位和其对应的主键值，包含两次查找 InnoDB聚簇表分布myisam在磁盘存储上有三个文件，每个文件名以表名开头，扩展名指出文件类型。.frm 用于存储表的定义.MYD 用于存放数据.MYI 用于存放表索引 可以看到—–》 Innodb索引与数据放在一起 慢查询/索引101.数据库索引的实现(B+树介绍、和B树、R树区别) 参考文章：数据库索引的实现原理 - 辉仔 の专栏 - 博客频道 - CSDN.NET由浅入深理解数据库中索引的底层实现 | 学步园 102.SQL性能优化 参考文章：高手详解SQL性能优化十条经验 - 51CTO.COMOracle SQL性能优化 - 一江水 - 博客园 103.数据库索引的优缺点以及什么时候数据库索引失效 参考文章：数据库索引的作用和优点缺点以及索引的11中用法 - 技术与人生 - 博客园正确高效使用数据库不可不知的索引失效问题 - simplefrog - 博客园SQL优化避免索引失效 - OPEN 开发经验库Colin Lau Oracle哪些情况下索引会失效？ - 曾是土木人 - 博客园","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"索引的使用和含义","slug":"数据库/MySQL/高级SQL","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:41:58.000Z","comments":true,"path":"2020/07/15/数据库/MySQL/高级SQL/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/%E9%AB%98%E7%BA%A7SQL/","excerpt":"","text":"索引索引（Index）是帮助MySQL高效获取数据的数据结构。我们可以简单理解为：快速查找排好序的一种数据结构。MySQL索引主要有两种结构：B+Tree索引和Hash索引。我们平常所说的索引，如果没有特殊指明，一般都是B树结构组织的索引（B+Tree索引） 索引是什么?索引是帮助MySQL高效获取数据的数据结构。 索引能干什么?提高数据查询的效率。 索引：排好序的快速查找数据结构！索引会影响where后面的查找，和order by 后面的排序。公众号：Java后端 发布过几十篇 MySQL 文章，其中包括索引的文章，可以关注后后台回复 666 获取。 一、索引的分类 从存储结构上来划分：BTree索引（B-Tree或B+Tree索引），Hash索引，full-index全文索引，R-Tree索引。 从应用层次来分：普通索引，唯一索引，复合索引。 根据中数据的物理顺序与键值的逻辑（索引）顺序关系：聚集索引，非聚集索引。 1 中所描述的是索引存储时保存的形式，2 是索引使用过程中进行的分类，两者是不同层次上的划分。不过平时讲的索引类型一般是指在应用层次的划分。 就像手机分类，安卓手机，IOS手机 与 华为手机，苹果手机，OPPO手机一样。 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引唯一索引：索引列的值必须唯一，但允许有空值复合索引：即一个索引包含多个列聚簇索引(聚集索引)：并不是一种单独的索引类型，而是一种数据存储方式。具体细节取决于不同的实现，InnoDB的聚簇索引其实就是在同一个结构中保存了B-Tree索引(技术上来说是B+Tree)和数据行。非聚簇索引：不是聚簇索引，就是非聚簇索引（认真脸）。二、索引的底层实现 mysql默认存储引擎innodb只显式支持B-Tree( 从技术上来说是B+Tree)索引，对于频繁访问的表，innodb会透明建立自适应hash索引，即在B树索引基础上建立hash索引，可以显著提高查找效率，对于客户端是透明的，不可控制的，隐式的。不谈存储引擎，只讨论实现(抽象)Hash索引 基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），并且Hash索引将所有的哈希码存储在索引中，同时在索引表中保存指向每个数据行的指针。 B-Tree能加快数据的访问速度，因为存储引擎不再需要进行全表扫描来获取数据，数据分布在各个节点之中。 是B-Tree的改进版本，同时也是数据库索引索引所采用的存储结构。数据都在叶子节点上，并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址。相比B-Tree来说，进行范围查找时只需要查找两个节点，进行遍历即可。而B-Tree需要获取所有节点，相比之下B+Tree效率更高。 案例：假设有一张学生表，id为主键 在MyISAM引擎中的实现（二级索引也是这样实现的） 在InnoDB中的实现 三、问题 问：为什么索引结构默认使用B-Tree，而不是hash，二叉树，红黑树？hash：虽然可以快速定位，但是没有顺序，IO复杂度高。二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。红黑树：树的高度随着数据量增加而增加，IO代价高。问：为什么官方建议使用自增长主键作为索引。结合B+Tree的特点，自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分。并且能减少数据的移动，每次插入都是插入到最后。总之就是减少分裂和移动的频率。插入连续的数据： 插入非连续的数据 视图触发器count(1)、count(*)和count(字段)区别https://zhuanlan.zhihu.com/p/28397595","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"缓存雪崩、缓存击穿和缓存穿透","slug":"数据库/Redis/缓存雪崩、缓存穿透和缓存击穿","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:42:10.000Z","comments":true,"path":"2020/07/15/数据库/Redis/缓存雪崩、缓存穿透和缓存击穿/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E5%92%8C%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF/","excerpt":"","text":"缓存雪崩、缓存穿透和缓存击穿的区别是什么，各自有什么解决方案缓存穿透一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。一些恶意的请求会故意查询不存在的Key，请求量很大，就会对后端系统造成很大的压力，这就叫做缓存穿透。 如何避免 对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据Insert了之后清理缓存。 对一定不存在的key进行过滤，可以把所有的可能存在的key放到一个大的bitmap中，查询时通过该bitmap过滤。 缓存雪崩当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力，导致系统崩溃。 如何避免 在缓存失效后，通过加锁或者队列来控制数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 做二级缓存，A1为原始数据，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期 不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀 缓存击穿对于设置了过期时间的key，缓存在某个时间点过期的时候，恰好这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把DB压垮。 如何避免 使用互斥锁，当缓存失效时，不立即去load DB，先使用如Redis的setnx去设置一个互斥锁，当操作成功返回时在进行Load DB的操作并回设缓存，否则重试get缓存的方法 永远不过期：物理不过期，但逻辑过期（后台异步线程去刷新） 使用了大量缓存，那么就存在缓存击穿和缓存雪崩以及缓存一致性等问题？缓存穿透指的是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。解决方案：对这些不存在的数据缓存一个空数据，对这类请求进行过滤。 缓存雪崩指的是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库。解决方案：为了防止缓存在同一时间大面积过期导致的缓存雪崩，可以通过观察用户行为，合理设置缓存过期时间来实现；为了防止缓存服务器宕机出现的缓存雪崩，可以使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。也可以进行缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。例如：首先针对不同的缓存设置不同的过期时间，比如session缓存，在userKey这个前缀中，设置是30分钟过期，并且每次用户响应的话更新缓存时间。这样每次取session,都会延长30分钟，相对来说，就减少了缓存过期的几率 缓存一致性要求数据更新的同时缓存数据也能够实时更新。 解决方案：在数据更新的同时立即去更新缓存，首先尝试从缓存读取，读到数据则直接返回；如果读不到，就读数据库，并将数据会写到缓存，并返回。在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新，需要更新数据时，先更新数据库，然后把缓存里对应的数据失效掉（删掉）。","categories":[],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://alloceee.github.io/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"TCP&UDP","slug":"计算机网络/计算机网络-TCP&UDP","date":"2020-07-15T02:39:02.000Z","updated":"2020-07-16T03:42:34.000Z","comments":true,"path":"2020/07/15/计算机网络/计算机网络-TCP&UDP/","link":"","permalink":"https://alloceee.github.io/2020/07/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-TCP&UDP/","excerpt":"","text":"OSI七层模型各层的作用 层名 作用 应用层 网络服务与最终用户的一个接口 表示层 把应用层提供的信息变换为能够共同理解的形式 会话层 建立、管理、终止会话。 传输层 定义传输数据的协议端口号，以及流控和差错校验。 网络层 路由选择和中继，在一条数据链路上复用多条网络连接 数据链路层 数据链路的建立，拆除，对数据的检错，纠错是数据链路层的基本任务。 物理层 物理层并不是物理媒体本身，它只是开放系统中利用物理媒体实现物理连接的功能描述和执行连接的规程。 TCP/IP协议Transmission Control Protocol/Internet Protocol的简写，中译名为传输控制协议/因特网互联协议，是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。协议采用了4层的层级结构。然而在很多情况下，它是利用 IP 进行通信时所必须用到的协议群的统称。 TCP 和 UDPTCP 是面向连接的、可靠的流协议，通过三次握手建立连接，通讯完成时要拆除连接。UDP是面向无连接的通讯协议，UDP通讯时不需要接收方确认，属于不可靠的传输，可能会出现丢包现象。 TCP UDP 面向连接 无连接，即发送数据前不需要建立连接 提供可靠服务，也就是说，通过TCP传输的数据无差错，不丢失，不重复，且按顺序到达 尽最大努力交付，即不保证可靠交付 面向字节流 面向报文，并且网络出现阻塞不会使得发送速率降低，因此会出现丢包 只能1对1 支持1对1，1对多 首部较大，为20字节 8字节 三次握手和四次分手 TCP/IP中的数据包 包是全能性术语； 帧用于表示数据链路层中包的单位； 片是 IP中数据的单位； 段则表示 TCP 数据流中的信息； 消息是指应用协议中数据的单位。 TCP 中通过序列号与确认应答提高可靠性 HTTP请求的传输过程 一次完整http请求的7个过程 建立 TCP 连接（之前可能还有一次DNS域名解析） 客户端向服务器发送请求命令 客户端发送请求头信息 服务服务器应答器 返回响应头信息 服务器向客户端发送数据 服务器关闭 TCP 连接 HTTP 协议报文结构 请求报文结构 响应报文结构 SocketSocket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。 Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 主机 A 的应用程序要能和主机 B 的应用程序通信，必须通过 Socket 建立连接，而建立 Socket 连接必须需要底层TCP/IP 协议来建立 TCP 连接。建立 TCP 连接需要底层 IP 协议来寻址网络中的主机。我们知道网络层使用的 IP 协议可以帮助我们根据 IP 地址来找到目标主机，但是一台主机上可能运行着多个应用程序，如何才能与指定的应用程序通信就要通过 TCP 或 UPD 的地址也就是端口号来指定。这样就可以通过一个 Socket 实例唯一代表一个主机上的一个应用程序的通信链路了。 短连接： 连接-&gt;传输数据-&gt;关闭连接 HTTP是无状态的，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。 也可以这样说：短连接是指SOCKET连接后发送后接收完数据后马上断开连接。 长连接： 连接-&gt;传输数据-&gt;保持连接 -&gt; 传输数据-&gt; 。。。 -&gt;关闭连接。 长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差。 什么时候用长连接，短连接？ ​ 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。 总之，长连接和短连接的选择要视情况而定。 Linux网络IO模型同步和异步，阻塞和非阻塞 阻塞I/O（blocking I/O） 进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。 如果数据没有准备好，一直等待….数据准备好了，从内核拷贝到用户空间,IO函数返回成功指示。 当调用recv()函数时，系统首先查是否有准备好的数据。如果数据没有准备好，那么系统就处于等待状态。当数据准备好后，将数据从系统缓冲区复制到用户空间，然后该函数返回。在套接应用程序中，当调用recv()函数时，未必用户空间就已经存在数据，那么此时recv()函数就会处于等待状态。 非阻塞IO模型 非阻塞IO通过进程反复调用IO函数（多次系统调用，并马上返回）；在数据拷贝的过程中，进程是阻塞的； 我们把一个SOCKET接口设置为非阻塞就是告诉内核，当所请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I/O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。上述模型绝不被推荐。 ​ 把SOCKET设置为非阻塞模式，即通知系统内核：在调用Windows Sockets API时，不要让线程睡眠，而应该让函数立即返回。在返回时，该函数返回一个错误代码。图所示，一个非阻塞模式套接字多次调用recv()函数的过程。前三次调用recv()函数时，内核数据还没有准备好。因此，该函数立即返回WSAEWOULDBLOCK错误代码。第四次调用recv()函数时，数据已经准备好，被复制到应用程序的缓冲区中，recv()函数返回成功指示，应用程序开始处理数据。 IO复用模型 select和epoll；对一个socket，两次调用，两次返回，比阻塞IO并没有什么优越性；关键是能实现同时对多个socket进行处理。 ​ 简介：主要是select和epoll；对一个IO端口，两次调用，两次返回，比阻塞IO并没有什么优越性；关键是能实现同时对多个IO端口进行监听； ​ I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这两个函数可以同时阻塞多个I/O操作。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时，才真正调用I/O操作函数。 当用户进程调用了select，那么整个进程会被block；而同时，kernel会“监视”所有select负责的socket；当任何一个socket中的数据准备好了，select就会返回。这个时候，用户进程再调用read操作，将数据从kernel拷贝到用户进程。 这个图和blocking IO的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select和recvfrom)，而blocking IO只调用了一个系统调用(recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 信号驱动IO 套接口进行信号驱动I/O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。 ​ 简介：两次调用，两次返回； ​ 首先我们允许套接口进行信号驱动I/O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。 异步IO模型 当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者的输入输出操作。 5个I/O模型的比较： select、poll、epoll的区别 5个I/O模型的比较select、poll、epoll的区别？： 1、支持一个进程所能打开的最大连接数 select 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是32*32，同理64位机器上FD_SETSIZE为32*64），可以对进行修改，然后重新编译内核，但是性能可能会受到影响。 poll poll本质上和select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的 epoll 连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接 2、FD剧增后带来的IO效率问题 select 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。 poll 同上 epoll 因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能会有性能问题。 3、 消息传递方式 select 内核需要将消息传递到用户空间，都需要内核拷贝动作 poll 同上 epoll epoll通过内核和用户空间共享一块内存来实现的。 补充知识点：Level_triggered(水平触发)：当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你！！！如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率！！！ Edge_triggered(边缘触发)：当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符！！ select(),poll()模型都是水平触发模式，信号驱动IO是边缘触发模式，epoll()模型即支持水平触发，也支持边缘触发，默认是水平触发。 原生JDK网络编程 原生JDK网络编程- AIO 原生JDK网络编程- NIO之Reactor模式 原生JDK网络编程- NIO 原生JDK网络编程- Buffer 114.TCP和UDP区别 参考TCP和UDP的区别（转） - bizhu - 博客园 socket通信（tcp/udp区别及JAVA的实现方式）TCP——传输控制协议，具有极高的可靠性，保证数据包按照顺序准确到达，但其也有着很高的额外负担。UDP——使用者数据元协议，并不能保证数据包会被成功的送达，也不保证数据包到达的顺序，但其传输速度很快。大多数我们会使用TCP，偶尔才会动用UDP，如声音讯号，即使少量遗失，也无关紧要。 TCP什么是TCP粘包和拆包 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包 将发送数据大于MSS(最大报文长度)，TCP在传输前将进行拆包 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包 TCP粘包和拆包的解决方法 发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了 发送端将每个数据包封装为固定长度（不够可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来 可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包分开","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"网络知识拾遗","slug":"计算机网络/网络知识拾遗","date":"2015-11-23T13:19:23.000Z","updated":"2020-02-18T09:23:42.000Z","comments":true,"path":"2015/11/23/计算机网络/网络知识拾遗/","link":"","permalink":"https://alloceee.github.io/2015/11/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%8B%BE%E9%81%97/","excerpt":"本科加上研究生大概有七年时间，一直都是学的通信，不过覆盖面不是很全，一直对一些网络相关的概念和实现有些模糊。最近补了补通信网络中的一些基础知识和盲点，有目的地看了看《云计算网络珠玑》、《图解网络硬件》等和网络相关的书和一些文章，做一下记录总结。主要包括了二层交换、三层路由、Linux网络相关的内容。","text":"本科加上研究生大概有七年时间，一直都是学的通信，不过覆盖面不是很全，一直对一些网络相关的概念和实现有些模糊。最近补了补通信网络中的一些基础知识和盲点，有目的地看了看《云计算网络珠玑》、《图解网络硬件》等和网络相关的书和一些文章，做一下记录总结。主要包括了二层交换、三层路由、Linux网络相关的内容。 1. 二层交换二层交换指的是传统的二层交换机实现的功能，主要的功能就是将以太网帧从一个端口接收，并从合适的端口发送出去。 1.1 以太网帧 以太网帧如上图所示，如此构造主要是为了表达这个帧：到哪去（目的MAC，6字节）、从哪来（源MAC，6字节）、什么帧（长度/类型，2字节）、有什么（数据，40~1500字节）、错没错（FCS，CRC，4字节）。 用户数据的长度不同，以太网帧的长度也不同，范围为64-1518字节。 1.2 存储转发与地址学习 存储转发：转发时，常用的方式为存储转发方式（store forward），即数据帧先入存储队列再根据转发表进行转发。使用存储转发，一来可以为多种速率端口的数据进行缓冲，二来也可以将残损、CRC错误等异常帧进行丢弃。小科普：除了存储转发外，还有直通转发（cut through，只读到DA后就转发，无法处理冲突帧、CRC错帧）、碎片隔离（fragment free，读一个slot共64字节，无冲突再转发，无法处理CRC帧）。 地址学习：在完成存储转发时，需要查询转发表，从而得知数据帧应该从哪个端口发出，因此，转发表存的就是目的MAC地址与端口的映射，转发表生成的过程就是地址学习。大致过程就是，来一个帧，读他的源MAC，然后把源MAC和接收的端口号存下来。这样，交换机就知道了，从X端口来过MAC地址为Y的数据，当下次Y需要转发时，就可以把他转发到X端口了。从而，完成“从哪来去回哪去”的任务 1.3 广播风暴在几个交换机构成环时，会产生广播风暴，造成广播风暴的根本原因是交换机之间不能感知到互相的存在，导致地址学习时，一个交换将某个MAC的转发端口学习成了另一个交换机的端口。可以通过STP协议进行抑制，从逻辑上“断开”环。 1.4 VLAN VLAN可以将广播域分隔为多个逻辑网段。从帧格式上来看，增加了VLAN相关的域，VLAN相关域包括0x8100标志位，然后3bit的优先级，1bit的丢帧优先级，12bit的VLANID。在做转发学习时，通过SA+VLAN来学习转发端口。值得注意的是：当某个DA+VLAN查不到表时，仅在VLAN域广播。 换一种思想来看，可以认为VLAN是一种网络的虚拟化，将一个端口虚拟化成多个端口。 2. 三层路由路由的主要功能是根据目的IP转发到相应的网络中。和二层中的转发表类似，三层路由也存在一个类似的表，叫做路由表。 也就是说如果一个LAN希望连接另一个LAN，那么需要借助路由完成。另外，在大型的LAN中，由于连接设备多，导致MAC多，导致广播负担大，因此切分子网来避免这一问题，而子网之间所属不同LAN所以也需要借助路由完成通信。 在进行路由的时候，路由首先根据最长匹配原则在路由表中查找下一跳IP地址，之后，根据ARP表，获取下一跳的MAC信息，便进入ARP流程，最后，根据下一跳MAC地址生成以太网数据帧，并将该数据帧从接口转发至网络。 路由与三层交换有类似的地方，L3交换虽具有路由功能，但其核心功能主要在于数据交换上，而路由仅具有路由转发功能。 2.1 NATNetwork Adress Translator，网络地址转换。 对于源地址NAT，主要用于内网访问外网，源地址进行转换；对于目的地址NAT，一般用于外网访问内网，目的地址进行转换。 3. Linux网络由于目前大部分云计算服务器、网络设备都是运行在Linux上的，因此，学习一些和Linux网络底层相关的实现，有助于我们理解。如Linux 上的基础网络设备详解一文中所述： Linux 用户想要使用网络功能，不能通过直接操作硬件完成，而需要直接或间接的操作一个 Linux 为我们抽象出来的设备，既通用的 Linux 网络设备来完成。一个常见的情况是，系统里装有一个硬件网卡，Linux 会在系统里为其生成一个网络设备实例，如 eth0，用户需要对 eth0 发出命令以配置或使用它了。 另外，对于Linux网络中的数据流在kernel flow中有所描述，文中更有一张神图。 3.1 网络驱动 之前，有做过网络驱动，其实回想起来，核心的实现就2个：发送函数和接收函数。 1. 发送函数（回调） 核心功能是将上层网络传来的帧，写入到网卡中； 在网络驱动初始化时，会通过注册的方式对网络驱动进行初始化， 123456static const struct net_device_ops netdev_ops = &#123; .ndo_open = driver_open, .ndo_stop = driver_close, .ndo_start_xmit = driver_xmit, .ndo_get_stats = get_stats,&#125;; 当上层有帧传来时，就会回调driver_xmit函数，因此，在driver_xmit函数中，就应当实现将帧写入到硬件，一般硬件会提供插入帧的接口，完成插入时帧会进入网卡硬件的插入队列中。 2. 接收函数（中断）核心功能是将网卡中的帧，传送到上层协议栈中。 对于接收函数，则需要依靠硬件的中断，数据帧到达网卡，硬件以中断的方式告知系统，然后，接收函数回调，回调时，需通过硬件的接口读取数据帧，然后将其上传值上层接口。一般调用netif_rx()进行传输。 3.2 Linux BridgeLinux Bridge名释其意，像一个桥梁一样把网络设备桥接起来。Linux bridge是802.1D的实现，可以参考链接。 如上图所示是eth0和eth1加入到br0后的实现，可以看出，网桥向上屏蔽了桥下的网卡设备，从上层协议上来看，仅能看到网桥设备br0。 在《Understanding Linux Network Internals》一书中，分析了Linux Bridge的实现： 在br中有个链表来链net_device，每个net_device也反向链着br。在br中有个hash结构叫做fdb_entry，存储着转发表（forward databse），若某个MAC地址在fdb中，那么就直接发到某个net_device，如果没在的话，就广播给链接到该br的所有设备。 3.3 Linux VLAN交换与隔离是VLAN的两大功能，现实世界中的802.1q交换机存在多个VLAN，每个VLAN拥有多个端口，同一VLAN的端口可数据交换，不同VLAN的端口之间隔离。而Linux VLAN实现的是隔离，需要交换的话，需要在Linux Bridge上attach一个VLAN。即Linux Bridge加VLAN device能在功能层面完整模拟现实世界里的802.1.q交换机。 在关于linux 802.1d (bridge) 和 802.1q(vlan) 实现的再思考一文中，举了个例子，觉得很不错，画了个图加深理解： 一个盒子有6个物理interface, eth0,eth1,eth2,eth3,eth4,eth5,eth6.bridge0 { eth0, eth1, eth2 }, vlan id 是2bridge1 { eth3, eth4, eth5 }, vlan id 是3eth0,eth1,eth2,eth3,eth4,eth5都在混杂模式，并且没有ip地址，它们是bridge的port.创建vlan interface, bridge0.2, bridge1.3。在bridge0.2和bridge1.3上配置ip地址。vlan 2的机器，把bridge0.2的地址设置为缺省网关；vlan 3的机器，把bridge1.3设置为缺省网关。当有包要从vlan 2发往vlan 3是，它将送到bridge0.2，然后，通过路由，找到bridge1.3，然后由bridge1.3发出去。这个过程中，packet里面的vlan id会发生改变。这个例子里面，要求从bridge port上收到的包都必须是打tag的，在bridge里面，并不能识别和处理tag，只有到三层的vlan interface才能识别并处理这些tag. 另外，Linux VLAN则是802.1Q的实现，可以参考链接 参考链接《云计算网络珠玑》《 图解网络硬件》kernel flowlinux bridge《Understanding Linux Network Internals》关于linux 802.1d (bridge) 和 802.1q(vlan) 实现的再思考802.1Q VLAN implementation for Linux","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://alloceee.github.io/tags/%E7%BD%91%E7%BB%9C/"}]}],"categories":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"数据库系统","slug":"系统架构师/数据库系统","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://alloceee.github.io/categories/ElasticSearch/"},{"name":"计算机网络","slug":"系统架构师/计算机网络","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"},{"name":"操作系统","slug":"系统架构师/操作系统","permalink":"https://alloceee.github.io/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://alloceee.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"系统架构师","slug":"系统架构师","permalink":"https://alloceee.github.io/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://alloceee.github.io/tags/ElasticSearch/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://alloceee.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"MySQL","slug":"MySQL","permalink":"https://alloceee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://alloceee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"操作系统","slug":"操作系统","permalink":"https://alloceee.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Redis","slug":"Redis","permalink":"https://alloceee.github.io/tags/Redis/"},{"name":"外键","slug":"外键","permalink":"https://alloceee.github.io/tags/%E5%A4%96%E9%94%AE/"},{"name":"redis","slug":"redis","permalink":"https://alloceee.github.io/tags/redis/"},{"name":"HTTP","slug":"HTTP","permalink":"https://alloceee.github.io/tags/HTTP/"},{"name":"HTTP和HTTPS","slug":"HTTP和HTTPS","permalink":"https://alloceee.github.io/tags/HTTP%E5%92%8CHTTPS/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://alloceee.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://alloceee.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"架构","slug":"架构","permalink":"https://alloceee.github.io/tags/%E6%9E%B6%E6%9E%84/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://alloceee.github.io/tags/JavaScript/"},{"name":"前端","slug":"前端","permalink":"https://alloceee.github.io/tags/%E5%89%8D%E7%AB%AF/"},{"name":"缓存","slug":"缓存","permalink":"https://alloceee.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"网络","slug":"网络","permalink":"https://alloceee.github.io/tags/%E7%BD%91%E7%BB%9C/"}]}